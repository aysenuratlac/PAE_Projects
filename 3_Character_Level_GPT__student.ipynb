{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Building a GPT from Scratch\n",
    "---\n",
    "\n",
    "This is an extended version of Andrej Karpathy's notebook in addition to his [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.\n",
    "\n",
    "Adapted by: \n",
    "\n",
    "Prof. Dr.-Ing. Antje Muntzinger, University of Applied Sciences Stuttgart\n",
    "\n",
    "antje.muntzinger@hft-stuttgart.de\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We'll construct a character-level **GPT (Generative Pretrained Transformer)** model from scratch. **Transformer** is the name of the underlying neural net architecture that was introduced in the 2017 groundbreaking paper \"Attention is All You Need\" (Link at the bottom).\n",
    "The model will be trained on different texts, for example Shakespeare, Goethe's \"Faust\", the \"Lord of the Rings\" or books from Jane Austen, and will be able to generate new text based on the text from the book.\n",
    "\n",
    "\n",
    "**NOTE:** You may answer in English or German.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "[1. Loading the Data](#1.-Loading-the-Data)\n",
    "\n",
    "[2. Tokenization](#2.-Tokenization)\n",
    "\n",
    "[3. Making Training Mini-Batches](#3.-Making-Training-Mini-Batches)\n",
    "\n",
    "[4. Defining the Network with PyTorch](#4.-Defining-the-Network-with-PyTorch)\n",
    "\n",
    "[5. Training](#5.-Training)\n",
    "\n",
    "[6. The Mathematical Trick in Self-Attention](#6.-The-Mathematical-Trick-in-Self-Attention)\n",
    "\n",
    "[7. Self-Attention](#7.-Self-Attention)\n",
    "\n",
    "[8. Full GPT Implementation](#9.-Full-GPT-Implementation)\n",
    "\n",
    "[9. Outlook and Next Steps](#9.-Outlook-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22bfe5b3730>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# select the right file and read it in to inspect it\n",
    "with open('faust.txt', 'r', encoding='utf-8') as f:\n",
    "#with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('austen.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('LOTR.txt', 'r') as f:\n",
    "# with open('LOTR_TVscript.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1a) Find out the length of the dataset and print the first 1000 characters! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  205807\n",
      "﻿Faust:\n",
      "Der Tragödie erster Teil\n",
      "\n",
      "by Johann Wolfgang von Goethe\n",
      "\n",
      "\n",
      "Zueignung\n",
      "\n",
      "\n",
      "Ihr naht euch wieder, schwankende Gestalten,\n",
      "Die früh sich einst dem trüben Blick gezeigt.\n",
      "Versuch ich wohl, euch diesmal festzuhalten?\n",
      "Fühl ich mein Herz noch jenem Wahn geneigt?\n",
      "Ihr drängt euch zu! nun gut, so mögt ihr walten,\n",
      "Wie ihr aus Dunst und Nebel um mich steigt;\n",
      "Mein Busen fühlt sich jugendlich erschüttert\n",
      "Vom Zauberhauch, der euren Zug umwittert.\n",
      "\n",
      "Ihr bringt mit euch die Bilder froher Tage,\n",
      "Und manche liebe Schatten steigen auf;\n",
      "Gleich einer alten, halbverklungnen Sage\n",
      "Kommt erste Lieb und Freundschaft mit herauf;\n",
      "Der Schmerz wird neu, es wiederholt die Klage\n",
      "Des Lebens labyrinthisch irren Lauf,\n",
      "Und nennt die Guten, die, um schöne Stunden\n",
      "Vom Glück getäuscht, vor mir hinweggeschwunden.\n",
      "\n",
      "Sie hören nicht die folgenden Gesänge,\n",
      "Die Seelen, denen ich die ersten sang;\n",
      "Zerstoben ist das freundliche Gedränge,\n",
      "Verklungen, ach! der erste Widerklang.\n",
      "Mein Lied ertönt der unbekannten Menge,\n",
      "Ihr Beifall selbst\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1b) Store all unique characters that occur in this text in `chars` and print them. Store the number of unique characters in `vocab_size` and print the result. **(3 points)**\n",
    "\n",
    "**Hint:** First make a set of all characters to remove duplicates, then make a list out of them to get a unique ordering, and finally sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$%()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÄÖÜßäöü—‘’“”•™﻿\n",
      "vocab_size= 92\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size=', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to **tokenize** the input. This means, we convert the raw text string to some sequence of integers according to some **vocabulary** of possible elements. A **token** can be a character like here, or a piece of a word like in ChatGPT. For a character-level language model, we just translate each character to an integer (**encoding**) and vice-versa (**decoding**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2a) Test the code above by encoding some sentence of your choice and decoding it again. Print the encoded and decoded result. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hallo Welt!\n",
      "Encoded: [32, 51, 62, 62, 65, 1, 47, 55, 62, 70, 2]\n",
      "Decoded: Hallo Welt!\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "sentence = \"Hallo Welt!\"\n",
    "encoded = encode(sentence)  # Kodieren\n",
    "decoded = decode(encoded)   # Dekodieren\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"Original:\", sentence)\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tokenization is a trade-off between vocabulary size and sequence length: Large vocabularies will lead to shorter encoding sequences and vice versa. For example, encoding each character results in a short vocabulary of 26 tokens for the standard alphabet plus some more for special characters, but each word consists of longer encodings. On the other hand, encoding on word level means each word is encoded as a single token, but the vocabulary will be much larger (up to a whole dictionary of hundreds of thousands of words for one language). In practice, for example in ChatGPT, **sub word encodings** are used, which means not encoding entire words, but also not encoding individual characters. Instead, some intermediate format is used, for example the word 'undefined' could be encoded as three tokens: 'un', 'define', 'd'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2b) Encode the entire text dataset and store it into a `torch.tensor` with `dtype=torch.long`. This will be our input data for the model, and we name it `data`. \n",
    "Print the shape and dtype of `data` and the first 1000 characters of the encoded text for comparison with the text above. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: torch.Size([205807])\n",
      "Dtype of data: torch.int64\n",
      "First 1000 characters (encoded): [91, 30, 51, 71, 69, 70, 22, 0, 28, 55, 68, 1, 44, 68, 51, 57, 82, 54, 59, 55, 1, 55, 68, 69, 70, 55, 68, 1, 44, 55, 59, 62, 0, 0, 52, 75, 1, 34, 65, 58, 51, 64, 64, 1, 47, 65, 62, 56, 57, 51, 64, 57, 1, 72, 65, 64, 1, 31, 65, 55, 70, 58, 55, 0, 0, 0, 50, 71, 55, 59, 57, 64, 71, 64, 57, 0, 0, 0, 33, 58, 68, 1, 64, 51, 58, 70, 1, 55, 71, 53, 58, 1, 73, 59, 55, 54, 55, 68, 8, 1, 69, 53, 58, 73, 51, 64, 61, 55, 64, 54, 55, 1, 31, 55, 69, 70, 51, 62, 70, 55, 64, 8, 0, 28, 59, 55, 1, 56, 68, 83, 58, 1, 69, 59, 53, 58, 1, 55, 59, 64, 69, 70, 1, 54, 55, 63, 1, 70, 68, 83, 52, 55, 64, 1, 26, 62, 59, 53, 61, 1, 57, 55, 76, 55, 59, 57, 70, 10, 0, 46, 55, 68, 69, 71, 53, 58, 1, 59, 53, 58, 1, 73, 65, 58, 62, 8, 1, 55, 71, 53, 58, 1, 54, 59, 55, 69, 63, 51, 62, 1, 56, 55, 69, 70, 76, 71, 58, 51, 62, 70, 55, 64, 24, 0, 30, 83, 58, 62, 1, 59, 53, 58, 1, 63, 55, 59, 64, 1, 32, 55, 68, 76, 1, 64, 65, 53, 58, 1, 60, 55, 64, 55, 63, 1, 47, 51, 58, 64, 1, 57, 55, 64, 55, 59, 57, 70, 24, 0, 33, 58, 68, 1, 54, 68, 81, 64, 57, 70, 1, 55, 71, 53, 58, 1, 76, 71, 2, 1, 64, 71, 64, 1, 57, 71, 70, 8, 1, 69, 65, 1, 63, 82, 57, 70, 1, 59, 58, 68, 1, 73, 51, 62, 70, 55, 64, 8, 0, 47, 59, 55, 1, 59, 58, 68, 1, 51, 71, 69, 1, 28, 71, 64, 69, 70, 1, 71, 64, 54, 1, 38, 55, 52, 55, 62, 1, 71, 63, 1, 63, 59, 53, 58, 1, 69, 70, 55, 59, 57, 70, 23, 0, 37, 55, 59, 64, 1, 26, 71, 69, 55, 64, 1, 56, 83, 58, 62, 70, 1, 69, 59, 53, 58, 1, 60, 71, 57, 55, 64, 54, 62, 59, 53, 58, 1, 55, 68, 69, 53, 58, 83, 70, 70, 55, 68, 70, 0, 46, 65, 63, 1, 50, 51, 71, 52, 55, 68, 58, 51, 71, 53, 58, 8, 1, 54, 55, 68, 1, 55, 71, 68, 55, 64, 1, 50, 71, 57, 1, 71, 63, 73, 59, 70, 70, 55, 68, 70, 10, 0, 0, 33, 58, 68, 1, 52, 68, 59, 64, 57, 70, 1, 63, 59, 70, 1, 55, 71, 53, 58, 1, 54, 59, 55, 1, 26, 59, 62, 54, 55, 68, 1, 56, 68, 65, 58, 55, 68, 1, 44, 51, 57, 55, 8, 0, 45, 64, 54, 1, 63, 51, 64, 53, 58, 55, 1, 62, 59, 55, 52, 55, 1, 43, 53, 58, 51, 70, 70, 55, 64, 1, 69, 70, 55, 59, 57, 55, 64, 1, 51, 71, 56, 23, 0, 31, 62, 55, 59, 53, 58, 1, 55, 59, 64, 55, 68, 1, 51, 62, 70, 55, 64, 8, 1, 58, 51, 62, 52, 72, 55, 68, 61, 62, 71, 64, 57, 64, 55, 64, 1, 43, 51, 57, 55, 0, 35, 65, 63, 63, 70, 1, 55, 68, 69, 70, 55, 1, 36, 59, 55, 52, 1, 71, 64, 54, 1, 30, 68, 55, 71, 64, 54, 69, 53, 58, 51, 56, 70, 1, 63, 59, 70, 1, 58, 55, 68, 51, 71, 56, 23, 0, 28, 55, 68, 1, 43, 53, 58, 63, 55, 68, 76, 1, 73, 59, 68, 54, 1, 64, 55, 71, 8, 1, 55, 69, 1, 73, 59, 55, 54, 55, 68, 58, 65, 62, 70, 1, 54, 59, 55, 1, 35, 62, 51, 57, 55, 0, 28, 55, 69, 1, 36, 55, 52, 55, 64, 69, 1, 62, 51, 52, 75, 68, 59, 64, 70, 58, 59, 69, 53, 58, 1, 59, 68, 68, 55, 64, 1, 36, 51, 71, 56, 8, 0, 45, 64, 54, 1, 64, 55, 64, 64, 70, 1, 54, 59, 55, 1, 31, 71, 70, 55, 64, 8, 1, 54, 59, 55, 8, 1, 71, 63, 1, 69, 53, 58, 82, 64, 55, 1, 43, 70, 71, 64, 54, 55, 64, 0, 46, 65, 63, 1, 31, 62, 83, 53, 61, 1, 57, 55, 70, 81, 71, 69, 53, 58, 70, 8, 1, 72, 65, 68, 1, 63, 59, 68, 1, 58, 59, 64, 73, 55, 57, 57, 55, 69, 53, 58, 73, 71, 64, 54, 55, 64, 10, 0, 0, 43, 59, 55, 1, 58, 82, 68, 55, 64, 1, 64, 59, 53, 58, 70, 1, 54, 59, 55, 1, 56, 65, 62, 57, 55, 64, 54, 55, 64, 1, 31, 55, 69, 81, 64, 57, 55, 8, 0, 28, 59, 55, 1, 43, 55, 55, 62, 55, 64, 8, 1, 54, 55, 64, 55, 64, 1, 59, 53, 58, 1, 54, 59, 55, 1, 55, 68, 69, 70, 55, 64, 1, 69, 51, 64, 57, 23, 0, 50, 55, 68, 69, 70, 65, 52, 55, 64, 1, 59, 69, 70, 1, 54, 51, 69, 1, 56, 68, 55, 71, 64, 54, 62, 59, 53, 58, 55, 1, 31, 55, 54, 68, 81, 64, 57, 55, 8, 0, 46, 55, 68, 61, 62, 71, 64, 57, 55, 64, 8, 1, 51, 53, 58, 2, 1, 54, 55, 68, 1, 55, 68, 69, 70, 55, 1, 47, 59, 54, 55, 68, 61, 62, 51, 64, 57, 10, 0, 37, 55, 59, 64, 1, 36, 59, 55, 54, 1, 55, 68, 70, 82, 64, 70, 1, 54, 55, 68, 1, 71, 64, 52, 55, 61, 51, 64, 64, 70, 55, 64, 1, 37, 55, 64, 57, 55, 8, 0, 33, 58, 68, 1, 26, 55, 59, 56, 51, 62, 62, 1, 69, 55, 62, 52, 69, 70]\n",
      "Decoded first 1000 characters: ﻿Faust:\n",
      "Der Tragödie erster Teil\n",
      "\n",
      "by Johann Wolfgang von Goethe\n",
      "\n",
      "\n",
      "Zueignung\n",
      "\n",
      "\n",
      "Ihr naht euch wieder, schwankende Gestalten,\n",
      "Die früh sich einst dem trüben Blick gezeigt.\n",
      "Versuch ich wohl, euch diesmal festzuhalten?\n",
      "Fühl ich mein Herz noch jenem Wahn geneigt?\n",
      "Ihr drängt euch zu! nun gut, so mögt ihr walten,\n",
      "Wie ihr aus Dunst und Nebel um mich steigt;\n",
      "Mein Busen fühlt sich jugendlich erschüttert\n",
      "Vom Zauberhauch, der euren Zug umwittert.\n",
      "\n",
      "Ihr bringt mit euch die Bilder froher Tage,\n",
      "Und manche liebe Schatten steigen auf;\n",
      "Gleich einer alten, halbverklungnen Sage\n",
      "Kommt erste Lieb und Freundschaft mit herauf;\n",
      "Der Schmerz wird neu, es wiederholt die Klage\n",
      "Des Lebens labyrinthisch irren Lauf,\n",
      "Und nennt die Guten, die, um schöne Stunden\n",
      "Vom Glück getäuscht, vor mir hinweggeschwunden.\n",
      "\n",
      "Sie hören nicht die folgenden Gesänge,\n",
      "Die Seelen, denen ich die ersten sang;\n",
      "Zerstoben ist das freundliche Gedränge,\n",
      "Verklungen, ach! der erste Widerklang.\n",
      "Mein Lied ertönt der unbekannten Menge,\n",
      "Ihr Beifall selbst\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "encoded_text = [stoi[c] for c in text]  # Liste von Integern\n",
    "data = torch.tensor(encoded_text, dtype=torch.long)  # Konvertieren in einen Tensor\n",
    "\n",
    "print(\"Shape of data:\", data.shape)\n",
    "print(\"Dtype of data:\", data.dtype)\n",
    "print(\"First 1000 characters (encoded):\", data.tolist()[:1000])\n",
    "print(\"Decoded first 1000 characters:\", ''.join([itos[i] for i in data.tolist()[:1000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Making Training Mini-Batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3a) Split the data into 90% training and 10% validation data and store the result in `train_data` and `val_data`, respectively. We keep the validation data to detect overfitting: We don't want just a perfect memorization of this exact input text, we want a neural network that creates new text in a similar style. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: torch.Size([185226])\n",
      "Size of validation data: torch.Size([20581])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Definiere die Aufteilung\n",
    "split_ratio = 0.9\n",
    "n = int(len(data) * split_ratio)  # Grenze für den Trainingsdatensatz\n",
    "\n",
    "# Aufteilen in Trainings- und Validierungsdaten\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Ausgabe der Größen\n",
    "print(\"Size of training data:\", train_data.shape)\n",
    "print(\"Size of validation data:\", val_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only feed in chunks of data of size 8 here: feeding in all text at once is computationally too expensive. This is called the **block size** or **context length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([91, 30, 51, 71, 69, 70, 22,  0, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1] # +1 because the target is the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this `train_data` chunk of 9 characters, 8 training examples are hidden. Let's spell it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([91]) the target is: 30\n",
      "when input is tensor([91, 30]) the target is: 51\n",
      "when input is tensor([91, 30, 51]) the target is: 71\n",
      "when input is tensor([91, 30, 51, 71]) the target is: 69\n",
      "when input is tensor([91, 30, 51, 71, 69]) the target is: 70\n",
      "when input is tensor([91, 30, 51, 71, 69, 70]) the target is: 22\n",
      "when input is tensor([91, 30, 51, 71, 69, 70, 22]) the target is: 0\n",
      "when input is tensor([91, 30, 51, 71, 69, 70, 22,  0]) the target is: 28\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # this will be the input\n",
    "y = train_data[1:block_size+1] # this will be the target\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides efficiency, a second reason to feed in chunks of size `block_size` is to make the Transformer be used to seeing contexts of different lengths, from only 1 token all the way up to `block_size` and every length in between. That is going to be useful later during inference because while we're sampling, we can start the sampling generation with as little as one character of context and the Transformer knows how to predict the next character. Then it can predict everything up to `block_size`. After `block_size`, we have to start truncating because the Transformer will never receive more than block size inputs when it's predicting the next character.\n",
    "\n",
    "Besides the **time dimension** that we have just looked at, there is also the **batch dimension**: We feed in batches of multiple chunks of text that are all stacked up in a single tensor. This is simply done for efficiency, because the GPUs can process these batches in parallel.\n",
    "\n",
    "Now let's create random **batches** of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 4 (=batch_size) random offsets into training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack 4 chunks (4x8 tensor) as rows in a minibatch\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # y is the same but one ahead (shifted 1 position to the right)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3b) Get a batch of training data and store the inputs and targets in `xb` and `yb`, respectively. Print the results and their shapes. **(2 points)** \n",
    "\n",
    "**HINT:** Apply the `get_batch()` function above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs (xb):\n",
      "tensor([[55,  1, 52, 62, 83, 58, 70, 23],\n",
      "        [73, 59, 55, 54, 55, 68, 61, 55],\n",
      "        [56, 51, 62, 70,  8,  1, 54, 51],\n",
      "        [37, 25, 42, 44, 32, 29, 10,  0]])\n",
      "Shape of xb: torch.Size([4, 8])\n",
      "\n",
      "Targets (yb):\n",
      "tensor([[ 1, 52, 62, 83, 58, 70, 23,  0],\n",
      "        [59, 55, 54, 55, 68, 61, 55, 58],\n",
      "        [51, 62, 70,  8,  1, 54, 51, 80],\n",
      "        [25, 42, 44, 32, 29, 10,  0, 32]])\n",
      "Shape of yb: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# Ausgabe der Ergebnisse und Formen\n",
    "print(\"Inputs (xb):\")\n",
    "print(xb)\n",
    "print(\"Shape of xb:\", xb.shape)\n",
    "\n",
    "print(\"\\nTargets (yb):\")\n",
    "print(yb)\n",
    "print(\"Shape of yb:\", yb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3c) How many independent training examples for the transformer does this batch contain? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**ANSWER:** The batch contains **`batch_size × block_size`** training examples because each position in the sequences (length = `block_size`) is treated as an independent example. With `batch_size = 4` and `block_size = 8`, there are \\( 4 \\times 8 = 32 \\) examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [55] the target is: 1\n",
      "when input is [55, 1] the target is: 52\n",
      "when input is [55, 1, 52] the target is: 62\n",
      "when input is [55, 1, 52, 62] the target is: 83\n",
      "when input is [55, 1, 52, 62, 83] the target is: 58\n",
      "when input is [55, 1, 52, 62, 83, 58] the target is: 70\n",
      "when input is [55, 1, 52, 62, 83, 58, 70] the target is: 23\n",
      "when input is [55, 1, 52, 62, 83, 58, 70, 23] the target is: 0\n",
      "when input is [73] the target is: 59\n",
      "when input is [73, 59] the target is: 55\n",
      "when input is [73, 59, 55] the target is: 54\n",
      "when input is [73, 59, 55, 54] the target is: 55\n",
      "when input is [73, 59, 55, 54, 55] the target is: 68\n",
      "when input is [73, 59, 55, 54, 55, 68] the target is: 61\n",
      "when input is [73, 59, 55, 54, 55, 68, 61] the target is: 55\n",
      "when input is [73, 59, 55, 54, 55, 68, 61, 55] the target is: 58\n",
      "when input is [56] the target is: 51\n",
      "when input is [56, 51] the target is: 62\n",
      "when input is [56, 51, 62] the target is: 70\n",
      "when input is [56, 51, 62, 70] the target is: 8\n",
      "when input is [56, 51, 62, 70, 8] the target is: 1\n",
      "when input is [56, 51, 62, 70, 8, 1] the target is: 54\n",
      "when input is [56, 51, 62, 70, 8, 1, 54] the target is: 51\n",
      "when input is [56, 51, 62, 70, 8, 1, 54, 51] the target is: 80\n",
      "when input is [37] the target is: 25\n",
      "when input is [37, 25] the target is: 42\n",
      "when input is [37, 25, 42] the target is: 44\n",
      "when input is [37, 25, 42, 44] the target is: 32\n",
      "when input is [37, 25, 42, 44, 32] the target is: 29\n",
      "when input is [37, 25, 42, 44, 32, 29] the target is: 10\n",
      "when input is [37, 25, 42, 44, 32, 29, 10] the target is: 0\n",
      "when input is [37, 25, 42, 44, 32, 29, 10, 0] the target is: 32\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3d) Why do the targets look like this, where does the structure come from? What do we input to the transformer? **(2 points)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The targets (`yb`) are the input (`xb`) shifted one position to the right. This structure trains the model to predict the next token in the sequence. We input the current tokens (`xb`) into the transformer, which uses embeddings and positional encodings to process them and learn dependencies for next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Defining the Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple bigram language model to start with, i.e., the model predicts the next character simply on the last character. This bigram model should look familiar from our first notebook! Only now, we implement a bigram model class inheriting from `nn.Module` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 92])\n",
      "loss= tensor(4.8776, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated text: \n",
      "\n",
      "?..KGv;äTIEzTBYy8u4.3?M•1y*e(ItThJßsk0B?!—10K%Ef6pMfq,x-p?Jg2Dr(uW;MÖL\n",
      "Z9lVrE™J;TN’Phs—?BA—?y5B?iök*\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module): # subclass of nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # e.g. if the input is token 5, the output should be the logits for all tokens at position 6 \n",
    "        # = the 5th row of the embedding table (see makemore video on bigram language model)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None): # targets are optional during inference\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # pluck out the embeddings for the tokens in the input (=the row of the embedding table corresponding to its index) and interpret them as logits=scores\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) batch size=4, time=8, channels=vocab_size because we are predicting the probability of each token (vocab_size C) at each time step (block_size T) in each batch (batch_size B)\n",
    "\n",
    "        # if we have targets, compute the CE loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # need to reshape for CE-loss in PyTorch \n",
    "            # (see https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "            targets = targets.view(B*T) # same shape as logits\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions (ignore the loss because we don't have targets)\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step = prediction for the next token\n",
    "            logits = logits[:, -1, :] # becomes (B, C) instead of (B, T, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) because we sample one token at a time for each batch\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print('loss=', loss) \n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # start with a single token = 0 (idx = current context)\n",
    "\n",
    "print(\"\\nGenerated text: \")\n",
    "# generate operates on batch level -> index into the 0th row = single batch dimension that exists -> one-dimensional array of all the indices (time steps)\n",
    "# afterwards convert to simple python list from tensor for decode function\n",
    "print(decode(model.generate(idx, max_new_tokens=100)[0].tolist())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4a) Go through the class definition above and explain what each function does! (1-2 sentences per function) **(6 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** Here’s a breakdown of the class and its functions:\n",
    "\n",
    "`__init__(self, vocab_size)`\n",
    "- Initializes the model by creating an embedding table (`self.token_embedding_table`) of size `(vocab_size, vocab_size)`.  \n",
    "- Each token directly maps to logits for predicting the next token in the sequence (bigram model logic).\n",
    "\n",
    "\n",
    "`forward(self, idx, targets=None)`\n",
    "  - Extracts logits for each token in `idx` using the embedding table.\n",
    "  - Reshapes `logits` and `targets` to compute loss using `F.cross_entropy` (PyTorch’s CrossEntropyLoss).\n",
    "\n",
    "`generate(self, idx, max_new_tokens)`\n",
    "   - Predicts logits for the next token using `forward()`.\n",
    "   - Focuses on the last time step's logits to sample the next token using `torch.multinomial`.\n",
    "   - Appends the sampled token to `idx`, extending the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4b) How do you interpret the generated text? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The generated text reflects the bigram model's ability to predict the next token based solely on the current one, creating locally coherent sequences. It mimics token patterns from the training data but lacks long-term context or grammar, often resulting in random or nonsensical output. It’s mainly used to assess how well the model has learned bigram relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4c) What loss do you expect for this model? Can you compare the actual loss with your expectation? **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The expected loss for the bigram model is around log(vocab_size) if it hasn't learned anything. A lower loss indicates the model has learned meaningful bigram relationships. Comparing the actual loss to this baseline helps assess whether the model is performing better than random chance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that up until now, the text history is not used, it is a simple bigram model (only the last character is used to predict the next one). Still, we feed in the whole sequence `xb`, `yb` up to `block_size` for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5a) Create a PyTorch Adam optimizer with a learning rate of `1e-3`, pass it the model parameters for optimization (`model.parameters()`) and store it in `optimizer`. Check the documentation if needed! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "import torch.optim as optim\n",
    "\n",
    "# Erstelle den Adam-Optimizer mit einem Lernrate von 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Überprüfen des Optimizers\n",
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the training loop now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=5.106197357177734\n",
      "step=100, loss=4.938789367675781\n",
      "step=200, loss=4.7256340980529785\n",
      "step=300, loss=4.701963424682617\n",
      "step=400, loss=4.539300918579102\n",
      "step=500, loss=4.431313514709473\n",
      "step=600, loss=4.375298976898193\n",
      "step=700, loss=4.193227291107178\n",
      "step=800, loss=4.184295654296875\n",
      "step=900, loss=4.055790424346924\n",
      "step=1000, loss=3.8715224266052246\n",
      "step=1100, loss=3.8623199462890625\n",
      "step=1200, loss=3.828604221343994\n",
      "step=1300, loss=3.743664503097534\n",
      "step=1400, loss=3.566786289215088\n",
      "step=1500, loss=3.557032585144043\n",
      "step=1600, loss=3.4300343990325928\n",
      "step=1700, loss=3.342414140701294\n",
      "step=1800, loss=3.2471885681152344\n",
      "step=1900, loss=3.251620054244995\n",
      "step=2000, loss=3.238626718521118\n",
      "step=2100, loss=3.224954605102539\n",
      "step=2200, loss=3.1596896648406982\n",
      "step=2300, loss=3.0524985790252686\n",
      "step=2400, loss=3.011270523071289\n",
      "step=2500, loss=3.080731153488159\n",
      "step=2600, loss=2.980318546295166\n",
      "step=2700, loss=2.93013072013855\n",
      "step=2800, loss=2.8149254322052\n",
      "step=2900, loss=2.9134740829467773\n",
      "step=3000, loss=2.8256847858428955\n",
      "step=3100, loss=2.762559652328491\n",
      "step=3200, loss=2.781909942626953\n",
      "step=3300, loss=2.6843528747558594\n",
      "step=3400, loss=2.7755308151245117\n",
      "step=3500, loss=2.580116033554077\n",
      "step=3600, loss=2.7096779346466064\n",
      "step=3700, loss=2.5634334087371826\n",
      "step=3800, loss=2.6413559913635254\n",
      "step=3900, loss=2.5841052532196045\n",
      "step=4000, loss=2.507134199142456\n",
      "step=4100, loss=2.5750017166137695\n",
      "step=4200, loss=2.6115541458129883\n",
      "step=4300, loss=2.4945578575134277\n",
      "step=4400, loss=2.5709176063537598\n",
      "step=4500, loss=2.6017367839813232\n",
      "step=4600, loss=2.4910426139831543\n",
      "step=4700, loss=2.5191752910614014\n",
      "step=4800, loss=2.489187240600586\n",
      "step=4900, loss=2.548396348953247\n",
      "step=5000, loss=2.341413974761963\n",
      "step=5100, loss=2.5278005599975586\n",
      "step=5200, loss=2.4247217178344727\n",
      "step=5300, loss=2.515277862548828\n",
      "step=5400, loss=2.5228331089019775\n",
      "step=5500, loss=2.307805061340332\n",
      "step=5600, loss=2.4931178092956543\n",
      "step=5700, loss=2.438764810562134\n",
      "step=5800, loss=2.4520678520202637\n",
      "step=5900, loss=2.4019439220428467\n",
      "step=6000, loss=2.311811685562134\n",
      "step=6100, loss=2.3679771423339844\n",
      "step=6200, loss=2.3922507762908936\n",
      "step=6300, loss=2.368206024169922\n",
      "step=6400, loss=2.4486188888549805\n",
      "step=6500, loss=2.2805049419403076\n",
      "step=6600, loss=2.3850011825561523\n",
      "step=6700, loss=2.4409873485565186\n",
      "step=6800, loss=2.3526854515075684\n",
      "step=6900, loss=2.296807050704956\n",
      "step=7000, loss=2.32723331451416\n",
      "step=7100, loss=2.429809093475342\n",
      "step=7200, loss=2.4069533348083496\n",
      "step=7300, loss=2.5322651863098145\n",
      "step=7400, loss=2.421046257019043\n",
      "step=7500, loss=2.3618719577789307\n",
      "step=7600, loss=2.342933177947998\n",
      "step=7700, loss=2.5208804607391357\n",
      "step=7800, loss=2.364821434020996\n",
      "step=7900, loss=2.4834039211273193\n",
      "step=8000, loss=2.3111133575439453\n",
      "step=8100, loss=2.423619508743286\n",
      "step=8200, loss=2.489002227783203\n",
      "step=8300, loss=2.2978107929229736\n",
      "step=8400, loss=2.447329044342041\n",
      "step=8500, loss=2.2938952445983887\n",
      "step=8600, loss=2.3322861194610596\n",
      "step=8700, loss=2.3712329864501953\n",
      "step=8800, loss=2.4808170795440674\n",
      "step=8900, loss=2.3278095722198486\n",
      "step=9000, loss=2.3919546604156494\n",
      "step=9100, loss=2.3977391719818115\n",
      "step=9200, loss=2.41764235496521\n",
      "step=9300, loss=2.4178342819213867\n",
      "step=9400, loss=2.3095357418060303\n",
      "step=9500, loss=2.4099204540252686\n",
      "step=9600, loss=2.381239414215088\n",
      "step=9700, loss=2.3307087421417236\n",
      "step=9800, loss=2.298687696456909\n",
      "step=9900, loss=2.429377317428589\n",
      "2.398003578186035\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increase batch size for better results\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # logits are not needed here\n",
    "    optimizer.zero_grad(set_to_none=True) # reset the gradients\n",
    "    loss.backward() # compute the gradients\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "    # print the loss every 100 steps\n",
    "    if steps % 100 == 0:\n",
    "        print(f'step={steps}, loss={loss.item()}')\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate new text based on the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "\n",
      "UStobucht in nt\n",
      "ISohahoniter un zuten MAch!\n",
      "\n",
      "Wönggunühagelt’sadier ieineralchnd wer zwaßeist wasir mat gelunk.\n",
      "AUS.\n",
      "\n",
      "Müme.\n",
      "Westh s fteu n Dit,\n",
      "\n",
      "Freb d wibinser.\n",
      "\n",
      "Sch Hertäle nullach ffü$—Weschteuchlbeur hn stendanfbirn wis inderebelt.\n",
      "\n",
      "Werottv.\n",
      "Folkel, um,\n",
      "\n",
      "Inn: n,\n",
      "Deimeich ich EP;\n",
      "\n",
      "Moer llie,\n",
      "Du u Gwisteicht!\n",
      "Uf L.\n",
      "ALaung achtrahöche st Lensedeneid u Po dabons T.\n",
      "ISTErind vochalalien beweran STüh bie LEin (Gwerbardihti2ßen Wenich Mer d min dale,\n",
      "Menkalasis, zwätwet fütztharzurtäsehen us, b’NGE\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text: \")\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5b) How do you interpret the result? What could be a reason that the output is still suboptimal? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: The output shows the bigram model has learned local token transitions but lacks coherence and meaning due to its reliance on single-token context. Suboptimal results stem from the model's simplicity, limited training data, and the randomness of token sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarized code so far (with some additions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "step 0: train loss 4.9167, val loss 4.9644\n",
      "step 300: train loss 2.8066, val loss 3.6229\n",
      "step 600: train loss 2.4820, val loss 3.5374\n",
      "step 900: train loss 2.4092, val loss 3.5775\n",
      "step 1200: train loss 2.3891, val loss 3.5933\n",
      "step 1500: train loss 2.3842, val loss 3.6093\n",
      "step 1800: train loss 2.3817, val loss 3.6303\n",
      "step 2100: train loss 2.3745, val loss 3.6583\n",
      "step 2400: train loss 2.3671, val loss 3.6622\n",
      "step 2700: train loss 2.3649, val loss 3.6897\n",
      "\n",
      "\n",
      "Man dem Numüscko Nar ichäleh;\n",
      "\n",
      "Lerst schn ir.\n",
      "Marennn GARROPHERiemien rh ßttieerr ls, u d zLEnelar)\n",
      "\n",
      "keretun, stugeben nerar d uührzun Tintze zuz arl seren! ien Wewargeberst Zwo sarteich h Ger,\n",
      "\n",
      "Ka d, der, mmangt, Zufibe de iet brt, abt anmpf!\n",
      "Un dagerosk m, sitebsin, faudeiereungten.\n",
      "Do körerohrttenchKöngöh ies donn- wit, aßt’genelsplis wenen dendes Stleisit.\n",
      "Nichribeidiel d ne Far s a.\n",
      "\n",
      "Ern m TISIchmeicht Dichrer Gla,\n",
      "ONamprsch dinäP%os.\n",
      "Feter Kieie in gautiro t ichrmin,\n",
      "\n",
      "Unflihasende än wele\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # new: check if GPU is available\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # new: we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # new: average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # new: switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # new: switch back to train mode\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "---\n",
    "## 6. The Mathematical Trick in Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now derive a more complex model that can look at all tokens at once to predict the next one, not just the last token. \n",
    "To use all previous tokens, the simplest idea is to use an average of all previous tokens. \n",
    "For example, the 5th token uses the **channels** (=feature maps, embeddings) of the 1st, 2nd, 3rd, 4th, and 5th token. \n",
    "The average of these is the **feature vector** for the 5th token and summarizes the context / history.\n",
    "Note that we have lost a lot of information, e.g. the order of the tokens, but it's a starting point. Consider the following toy example with batch size 4 , 8 tokens, 2 channels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2 # batch, time, channels. Goal: 8 tokens should talk to each other, but only from previous tokens, not from future tokens\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in each batch in the example vector `x`, we calculate the mean of the tokens that came before it in the time dimension (including itself). \n",
    "The result should be a tensor of shape (B,T,C) where the t-th row of the b-th batch contains the mean of all tokens in this batch that came before this token in the time dimension.\n",
    "We print the original tensor `x` and the resulting tensor `xbow` containing the mean values and make sure the mean values are correct. Here `bow` stands for **bag of words**, which means that each entry is an average of several words (each of the 8 tokens is considered a 'word' here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow = bag of words = simple average of all previous tokens\n",
    "for b in range(B): # iterate over batch dimension\n",
    "    for t in range(T): # iterate over time dimension\n",
    "        xprev = x[b,:t+1] # (t,C) # all previous tokens for this batch and time (slice)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # mean over time dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1546, -0.4100],\n",
       "        [-2.5211,  0.5678],\n",
       "        [-1.4687, -0.6305],\n",
       "        [ 1.3620,  1.4733],\n",
       "        [ 0.3252,  0.3394],\n",
       "        [-1.7140,  0.7753],\n",
       "        [ 1.0787,  1.0077],\n",
       "        [ 0.9735, -0.3782]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # 0th batch element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1546, -0.4100],\n",
       "        [-1.1833,  0.0789],\n",
       "        [-1.2784, -0.1576],\n",
       "        [-0.6183,  0.2502],\n",
       "        [-0.4296,  0.2680],\n",
       "        [-0.6437,  0.3526],\n",
       "        [-0.3976,  0.4462],\n",
       "        [-0.2262,  0.3431]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] # vertical average of all previous tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using several nested loops like above, we use a trick with matrix multiplication that is mathematically equivalent but more efficient. Here is a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3) \n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, c contains the sum of the column entries of b. Because we only want the \"history\", not the \"future\" tokens to influence the result, we use an upper triangular matrix `a` instead, this is called **masking**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows, \n",
    "# third row is sum of all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to normalize for averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "a = a / torch.sum(a, 1, keepdim=True) # normalize rows to sum to 1\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows + normalized, \n",
    "# third row is sum of all rows + normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6a) Now let's go back to our example above and apply the same trick. \n",
    "Define a lower triangular matrix called `wei` (previously `a`) that is normalized to sum to 1 along the rows. Matrix multiply `wei` with `x` to get a new matrix `xbow2`.\n",
    "Make sure that `xbow2` has the same shape as `x` and that it contains the correct values. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x:\n",
      "tensor([[[ 0.1546, -0.4100],\n",
      "         [-2.5211,  0.5678],\n",
      "         [-1.4687, -0.6305],\n",
      "         [ 1.3620,  1.4733],\n",
      "         [ 0.3252,  0.3394],\n",
      "         [-1.7140,  0.7753],\n",
      "         [ 1.0787,  1.0077],\n",
      "         [ 0.9735, -0.3782]],\n",
      "\n",
      "        [[ 0.0307,  0.1766],\n",
      "         [ 0.6101,  0.5782],\n",
      "         [ 0.5597, -2.0204],\n",
      "         [-0.8526, -0.1514],\n",
      "         [ 0.7733, -0.7985],\n",
      "         [-0.0772, -1.0009],\n",
      "         [-1.1819,  2.8414],\n",
      "         [-1.0992, -1.9645]],\n",
      "\n",
      "        [[ 1.0370, -1.7319],\n",
      "         [-1.5860, -0.3265],\n",
      "         [-2.0740,  0.5501],\n",
      "         [ 1.0834, -1.3492],\n",
      "         [-2.0153,  0.4432],\n",
      "         [ 0.9676,  0.2592],\n",
      "         [ 1.6551,  2.1916],\n",
      "         [ 0.0098,  1.0779]],\n",
      "\n",
      "        [[ 1.0993,  1.1367],\n",
      "         [ 1.0259, -0.3675],\n",
      "         [-0.9456,  0.2996],\n",
      "         [-1.4666,  0.9621],\n",
      "         [-0.9154,  1.3421],\n",
      "         [ 0.5196,  1.1817],\n",
      "         [-1.0856, -0.2741],\n",
      "         [-0.5417,  0.7371]]])\n",
      "Lower triangular matrix wei:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "Transformed matrix xbow2:\n",
      "tensor([[[ 0.1546, -0.4100],\n",
      "         [-1.1833,  0.0789],\n",
      "         [-1.2784, -0.1576],\n",
      "         [-0.6183,  0.2502],\n",
      "         [-0.4296,  0.2680],\n",
      "         [-0.6437,  0.3526],\n",
      "         [-0.3976,  0.4462],\n",
      "         [-0.2262,  0.3431]],\n",
      "\n",
      "        [[ 0.0307,  0.1766],\n",
      "         [ 0.3204,  0.3774],\n",
      "         [ 0.4002, -0.4219],\n",
      "         [ 0.0870, -0.3543],\n",
      "         [ 0.2242, -0.4431],\n",
      "         [ 0.1740, -0.5361],\n",
      "         [-0.0197, -0.0536],\n",
      "         [-0.1546, -0.2924]],\n",
      "\n",
      "        [[ 1.0370, -1.7319],\n",
      "         [-0.2745, -1.0292],\n",
      "         [-0.8743, -0.5028],\n",
      "         [-0.3849, -0.7144],\n",
      "         [-0.7110, -0.4828],\n",
      "         [-0.4312, -0.3592],\n",
      "         [-0.1332,  0.0052],\n",
      "         [-0.1153,  0.1393]],\n",
      "\n",
      "        [[ 1.0993,  1.1367],\n",
      "         [ 1.0626,  0.3846],\n",
      "         [ 0.3932,  0.3563],\n",
      "         [-0.0717,  0.5077],\n",
      "         [-0.2405,  0.6746],\n",
      "         [-0.1138,  0.7591],\n",
      "         [-0.2526,  0.6115],\n",
      "         [-0.2888,  0.6272]]])\n",
      "Shape of xbow2: torch.Size([4, 8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1junk\\AppData\\Local\\Temp\\ipykernel_22148\\1625008763.py:8: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3687.)\n",
      "  xbow2 = wei @ x.T  # Apply weight matrix\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Create a lower triangular matrix `wei`\n",
    "wei = torch.tril(torch.ones(8, 8))  # Lower triangular matrix (8x8)\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)  # Normalize so rows sum to 1\n",
    "\n",
    "# Matrix multiply `wei` with `x` to compute `xbow2`\n",
    "xbow2 = wei @ x.T  # Apply weight matrix\n",
    "xbow2 = xbow2.T    # Transpose back to match original shape\n",
    "\n",
    "# Verify the shape and correctness\n",
    "print(\"Original x:\")\n",
    "print(x)\n",
    "print(\"Lower triangular matrix wei:\")\n",
    "print(wei)\n",
    "print(\"Transformed matrix xbow2:\")\n",
    "print(xbow2)\n",
    "print(\"Shape of xbow2:\", xbow2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1546, -0.4100],\n",
       "         [-1.1833,  0.0789],\n",
       "         [-1.2784, -0.1576],\n",
       "         [-0.6183,  0.2502],\n",
       "         [-0.4296,  0.2680],\n",
       "         [-0.6437,  0.3526],\n",
       "         [-0.3976,  0.4462],\n",
       "         [-0.2262,  0.3431]]),\n",
       " tensor([[ 0.1546, -0.4100],\n",
       "         [-1.1833,  0.0789],\n",
       "         [-1.2784, -0.1576],\n",
       "         [-0.6183,  0.2502],\n",
       "         [-0.4296,  0.2680],\n",
       "         [-0.6437,  0.3526],\n",
       "         [-0.3976,  0.4462],\n",
       "         [-0.2262,  0.3431]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "xbow[0], xbow2[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6b) Now we use yet another mathematically equivalent way to compute the bag of words representation using **Softmax** function (this will be needed later for weighted sum instead of average of previous tokens).\n",
    "We start off with a lower triangular matrix where the lower triangle and diagonal is filled with 0, the upper with `-inf`. \n",
    "After applying the softmax function, the result will be again the `wei` matrix from before. Implement this in the following cell, calculate again the matrix multiplication of the new `wei` and `x` and check the result! **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x:\n",
      "tensor([[[ 0.1546, -0.4100],\n",
      "         [-2.5211,  0.5678],\n",
      "         [-1.4687, -0.6305],\n",
      "         [ 1.3620,  1.4733],\n",
      "         [ 0.3252,  0.3394],\n",
      "         [-1.7140,  0.7753],\n",
      "         [ 1.0787,  1.0077],\n",
      "         [ 0.9735, -0.3782]],\n",
      "\n",
      "        [[ 0.0307,  0.1766],\n",
      "         [ 0.6101,  0.5782],\n",
      "         [ 0.5597, -2.0204],\n",
      "         [-0.8526, -0.1514],\n",
      "         [ 0.7733, -0.7985],\n",
      "         [-0.0772, -1.0009],\n",
      "         [-1.1819,  2.8414],\n",
      "         [-1.0992, -1.9645]],\n",
      "\n",
      "        [[ 1.0370, -1.7319],\n",
      "         [-1.5860, -0.3265],\n",
      "         [-2.0740,  0.5501],\n",
      "         [ 1.0834, -1.3492],\n",
      "         [-2.0153,  0.4432],\n",
      "         [ 0.9676,  0.2592],\n",
      "         [ 1.6551,  2.1916],\n",
      "         [ 0.0098,  1.0779]],\n",
      "\n",
      "        [[ 1.0993,  1.1367],\n",
      "         [ 1.0259, -0.3675],\n",
      "         [-0.9456,  0.2996],\n",
      "         [-1.4666,  0.9621],\n",
      "         [-0.9154,  1.3421],\n",
      "         [ 0.5196,  1.1817],\n",
      "         [-1.0856, -0.2741],\n",
      "         [-0.5417,  0.7371]]])\n",
      "Softmax weight matrix (wei):\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan]])\n",
      "Transformed matrix xbow3:\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]])\n",
      "Shape of xbow3: torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Step 1: Create the lower triangular matrix with -inf for the upper triangle\n",
    "mask = torch.triu(torch.ones(8, 8), diagonal=1)  # Upper triangular part (including diagonal)\n",
    "logits = mask * -float('inf')  # Replace upper triangle with -inf\n",
    "\n",
    "# Step 2: Apply the softmax function row-wise\n",
    "softmax_wei = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Step 3: Matrix multiply softmax weights with x to compute the bag-of-words representation\n",
    "xbow3 = softmax_wei @ x.T  # Weighted sum\n",
    "xbow3 = xbow3.T            # Transpose back to match the input shape\n",
    "\n",
    "# Check the results\n",
    "print(\"Original x:\")\n",
    "print(x)\n",
    "print(\"Softmax weight matrix (wei):\")\n",
    "print(softmax_wei)\n",
    "print(\"Transformed matrix xbow3:\")\n",
    "print(xbow3)\n",
    "print(\"Shape of xbow3:\", xbow3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1546, -0.4100],\n",
       "         [-1.1833,  0.0789],\n",
       "         [-1.2784, -0.1576],\n",
       "         [-0.6183,  0.2502],\n",
       "         [-0.4296,  0.2680],\n",
       "         [-0.6437,  0.3526],\n",
       "         [-0.3976,  0.4462],\n",
       "         [-0.2262,  0.3431]]),\n",
       " tensor([[ 0.1546, -0.4100],\n",
       "         [-1.1833,  0.0789],\n",
       "         [-1.2784, -0.1576],\n",
       "         [-0.6183,  0.2502],\n",
       "         [-0.4296,  0.2680],\n",
       "         [-0.6437,  0.3526],\n",
       "         [-0.3976,  0.4462],\n",
       "         [-0.2262,  0.3431]]),\n",
       " tensor([[nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0], xbow3[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Self-Attention\n",
    "\n",
    "Finally we get to the most important mechanism: **Self-Attention**! This will lead to a weighted average of the tokens (some tokens are more important than others to understand the text) instead of simply using the mean. And here is the idea: Every single token will emit two vectors: A **query** (\"What am I looking for?\") and a **key** (\"What do I contain?\"). The query then dot-products with all the keys to determine the similarity = affinity (stored in `wei`). Instead of the raw input `x`, which is private, a **value** is used (\"What will I communicate?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels (increase channels for more interesting results)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16) # forward pass of x through the key layer\n",
    "q = query(x) # (B, T, 16) # forward pass of x through the query layer\n",
    "# so far, each token has a key and a query vector, no communication yet\n",
    "wei =  q @ k.transpose(-2, -1) # transpose last 2 dimensions (batch remains unchanged): (B, T, 16) @ (B, 16, T) ---> (B, T, T): for each batch, each token talks to all other tokens, so we get an affinity matrix of size (T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T)) # old version -> change to data dependent weights\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # comment to see intermediate results before normalization\n",
    "\n",
    "v = value(x) # we use the aggregated value instead of the raw x \n",
    "# x is private information to this token, v is the public information for communication\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7a) Print `wei` and compare it to the previous values. What is the most important change and why is this important here? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The updated `wei` matrix uses softmax to create a smoother, unequal weight distribution, prioritizing recent tokens over earlier ones. This is important as it better captures local context and introduces the concept of weighted importance, which is foundational for attention mechanisms in transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the first weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the final entry 0.2391 is the weight for the 8th token. The 8th token emits a query ( for example \"I am a vowel at position 8, I am looking for consonants at positions up to 4\"). All tokens then emit keys, and maybe a consonant at position 4 will emit a key with high number in this channel, meaning \"I am a consonant at position 4\". The 8th token will therefore have a high weight for the 4th token (0.2297), resulting in a high affinity (dot product) - the 4th and 8th token \"have found each other\". Through the softmax function, a lot of information from the 4th token will be passed to the 8th token (meaning the 8th token will learn a lot from the 4th)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "### Some Notes on Attention\n",
    "- Attention is a **communication mechanism**. It can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. Here we have `block_size = 8` nodes, where the first node is only pointed to by itself, the second by the first and itself, and so on. Attention can be applied to any directed graph, not only language modeling.\n",
    "- Each example across batch dimension is processed completely independently, the examples never \"talk\" to each other across different batches. The batched matrix multiplication above means applying matrix multiplication in parallel in each batch separately. For example here, you can think of 4 different graphs in parallel with 8 noded each, where the 8 nodes only communicate among each other, even though we process 32 nodes at once.\n",
    "- \"Scaled\" attention also divides `wei` by `1/sqrt(head_size)`, in the original paper:\n",
    "\\begin{equation*}\n",
    "   Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation*}\n",
    "This makes it so when input Q,K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much. Without the normalization, using Gaussian input (zero mean and variance 1), the weights will be in the order of `head_size`. Illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size) # k initialized from standard normal distribution (zero mean, unit variance)\n",
    "q = torch.randn(B,T,head_size) # q initialized from standard normal distribution (zero mean, unit variance)\n",
    "wei_unnormalized = q @ k.transpose(-2, -1) # will have variance of head_size roughly\n",
    "wei_normalized = q @ k.transpose(-2, -1)* head_size**-0.5 # normalize by sqrt of head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var() # variance of k: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var() # variance of q: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.4690)\n",
      "tensor(1.0918)\n"
     ]
    }
   ],
   "source": [
    "print(wei_unnormalized.var()) # variance of the dot product: roughly head_size=16\n",
    "print(wei_normalized.var()) # variance of the dot product: roughly 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7b) Find out why this is important: Apply softmax to a tensor with entries around 0, then to another tensor with more extreme values. What happens? Write in the answer cell why we want to avoid this. **(2 points)**\n",
    "\n",
    "**HINT:** `torch.softmax()` expects an input specifying along which dimension to calculate the normalization (=which dimension should sum to 1), so you can pass `dim=-1` as second input for a 1D tensor. (See https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** When softmax is applied to tensors with extreme values, it results in a skewed distribution, where one value dominates and others become negligible. This can lead to numerical instability, such as overflow or underflow, and cause the model to overfit to outliers. To avoid these issues, it's important to normalize inputs and control the scale of values before applying softmax, ensuring stable training and better model generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Encoding and Positional Encoding\n",
    "\n",
    "We will make one change on the token encoding: Previously, the `token_embedding_table` was of size `(vocab_size, vocab_size)`, which means we directly plucked out the logits from the embedding table. Now we want to introduce an intermediate layer (make the net bigger). Therefore, we introduce a new parameter `n_embd` for the number of embedding dimensions, for example we can choose 32 or 64 for this intermediate representation. So instead of logits, the `token_embedding_table` will give us **token embeddings**. These will be fed to a linear layer afterwards to get the logits:\n",
    "```\n",
    "self.lm_head = nn.Linear(n_embd, vocab_size) # linear layer to decode into the vocabulary   \n",
    "```\n",
    "\n",
    "In the attention mechanism derived so far, there is no notion of space. Attention simply acts over a set of vectors. Remember that we can think of attention as a directed graph, where the nodes have no idea where they are positioned in a space. But space matters in text: For example, \"people love animals\" has a significantly different meaning than \"animals love people\", so the ordering of the words is very important. This is why we need to **positionally encode** tokens:\n",
    "So far, we have only encoded each token according to its identity `idx`. But we now also encode its position in a second embedding table: Each position from `0` to `block_size-1` will get its own embedding vector. This is the code snippet from the init function that we will implement below: \n",
    "```\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # token embedding according to identity (e.g., first character in vocabulary)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # positional encoding according to position in text (e.g., first character in text)\n",
    "```\n",
    "And here is a code snippet from the forward function, showing how integers from 0 to `block_size` are positionally encoded: \n",
    "```\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - integers from 0 to T-1\n",
    "        x = tok_emb + pos_emb # (B,T,C) via broadcasting (pos_emb gets right-aligned, new dimension of 1 gets added, broadcasted across batch)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "```\n",
    "Right now, this is not useful yet, because we only use the last token in the Bigram model, so the position does not matter. But using attention, it will matter!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Single Self-Attention Head\n",
    "\n",
    "Now let's summarize the code so far and add a single self-attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "step 0: train loss 4.5645, val loss 4.5525\n",
      "step 500: train loss 2.5698, val loss 3.5501\n",
      "step 1000: train loss 2.4292, val loss 3.7169\n",
      "step 1500: train loss 2.3676, val loss 3.7612\n",
      "step 2000: train loss 2.3415, val loss 3.8056\n",
      "step 2500: train loss 2.3190, val loss 3.8577\n",
      "step 3000: train loss 2.2995, val loss 3.9520\n",
      "step 3500: train loss 2.2939, val loss 3.9182\n",
      "step 4000: train loss 2.2832, val loss 3.9271\n",
      "step 4500: train loss 2.2718, val loss 3.9604\n",
      "\n",
      "AUSTEPHEPHIERP(GRETHHCHÖHEPHELat.\n",
      "Docht! nzir.\n",
      "Marenn, GAun Mesteigten rheßt ieen Als, umf zut!\n",
      "NOpfüskendeun, stugebon nerch decher?\n",
      "Ichwer est! Jarl serin! ien Wewirgebenst Zle sarteich has selen bs, der, mmangt, Zefibende iet brt, abt anmpfugst dager sk mlusitebsin, faudellleungten.\n",
      "\n",
      "Deicher huttench deich ies donn-t vophaßt’genelsplischenen den,\n",
      "Und leisichltt arubft zel iche Frr Krand hin mangtens derwülerfer Masch,\n",
      "Von Blich diemer san dond Kieir inn Mutiro tat eum gesm Fülerder Tanänl!\n",
      "Au\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # new: increase number of iterations due to lower learning rate\n",
    "eval_interval = 500 \n",
    "learning_rate = 1e-3 # new: lower learning rate (self-attention is more complex)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # check if GPU is available\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # switch back to train mode\n",
    "    return out\n",
    "\n",
    "# new: single self-attention head\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for key. Typically no bias is used in self-attention\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for query\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for value\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # buffer = not a parameter; masking with lower triangular matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # batch, time, channels\n",
    "        k = self.key(x)   # (B,T,C) - apply the key linear layer\n",
    "        q = self.query(x) # (B,T,C) - apply the query linear layer\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) - scale by head_size**-0.5 (normalization from original paper, see above)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - mask out the future\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - apply softmax to get the weights\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C) - apply the value linear layer\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C) - weighted aggregation = self-attention\n",
    "        return out \n",
    "    \n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # new: dimensionality of embeddings changed to n_embd as intermediate layer\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # new: position embeddings\n",
    "        self.sa_head = Head(n_embd) # new: self-attention head\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # new: linear layer for prediction\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to the last block_size tokens (because we now use position embeddings, which only contain the last block_size tokens)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss decreased a bit, but the result is still not great. We will introduce some more changes following the transformer paper for further improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "---\n",
    "## 8. Full GPT Implementation\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "First, we add **multi-head attention**, which is simply several attention heads running in parallel, then concatenating the result over the channel dimension. A **projection layer** combines the concatenated outputs from all heads into a single unified representation and projects back to the original pathway. Note that \"projection\" in the context of Transformer models refers to a linear transformation that can either maintain, reduce, or even increase the dimensionality of the data. \n",
    "\n",
    "Intuitive Explanation: It helps to have multiple communication channels because these tokens have a lot to talk about - they want to find the consonants, the vowels, the vowels just from certain positions etc. and so it helps to create multiple independent channels of communication to gather lots of different types of data and then decode the output.\n",
    "\n",
    "<img src=\"img/multi-head-attention.jpg\" width=\"200\">\n",
    "\n",
    "### Transformer Block\n",
    "\n",
    "So far, we directly calculated the logits after the attention block, but this was way too fast - intuitively \"the tokens looked at each other, but didn't really have time to think on what they found from the other tokens\". Therefore, we add a feedforward single layer followed by a ReLU nonlinearity. Both layers together are called the **Transformer Block**, where we combine **communication** (self-attention) with **computation** (feedforward layer). This is on a per token level: Each token independently looks at the other tokens, and once it has gathered all the data, it thinks on that data individually. We implement this in the `Block` class below. The transformer block gets repeated over and over again.\n",
    "\n",
    "<img src=\"img/transformer.jpg\" width=\"300\">\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "Also note that the transformer architecture above contains **skip connections (residual connections)**: The network contains parallel paths (one with some computations, one with the identity as \"shortcut\") that are combined via additions. Additions are great for backpropagation because they distribute gradients equally to both branches, so there is a \"shortcut\" for the gradients to directly propagate from the output to the input of the network. This avoids the vanishing gradient problem especially in the beginning - the transformer blocks only get more influence over time.\n",
    "\n",
    "\n",
    "<img src=\"img/skip-connection.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm\n",
    "\n",
    "The transformer architecture uses **layer norm** (called \"Norm\" in the architecture image above), which is very similar to **batch norm**: Batch norm makes sure that across the batch dimension, any individual neuron has unit gaussian distribution (zero mean, unit standard deviation). In layer norm, we don't normalize the columns, but the rows, which normalizes over layers instead of over batches:\n",
    "\n",
    "\\begin{equation*}\n",
    "y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\varepsilon}}\\cdot \\gamma + \\beta,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (copied from BatchNorm1d in makemore series)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # previous batch mean -> index changed from 0 to 1\n",
    "    xvar = x.var(1, keepdim=True) # previous batch variance -> index changed from 0 to 1\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # no running mean and variance buffers needed like in batch norm\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 8a) Check if mean and standard deviation of rows and/or columns are normalized now! Write the result in the answer cell. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that layer norm is usually applied before the self-attention and linear layer nowadays (unlike the original paper) - one of the very few changes of the transformer architecture during the last years, otherwise mostly the architecture remained unchanged. This is called the **pre-norm formulation**. So here is a code snippet used below showing the two layer norms we will implement, one before the self-attention and one before the linear layer:\n",
    "\n",
    "```\n",
    "        x = x + self.sa(self.ln1(x)) # layer norm directly applied to x before self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # layer norm applied before linear layer\n",
    "```\n",
    "\n",
    "Finally, another layer norm is typically applied at the end of the Transformer and right before the final linear layer that decodes into vocabulary. \n",
    "\n",
    "The size of the layer norm is `n_embds=32` here, so this is a per token transformation, it just normalizes the features and makes them unit Gaussian at initialization. Because these layer norms contain gamma and beta as trainable parameters, the layer norm may eventually create outputs that are not unit Gaussian depending on the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up the Model\n",
    "\n",
    "We now have all components together so that we can scale up the model and make it bigger. Therefore, we add a parameter `n_layer=4` to specify that we want 4 transformer blocks.\n",
    "\n",
    "We also add **dropout** to prevent overfitting: with 4 transformer blocks, the network is getting quite large now. Therefore, we randomly deactivate some connections to prevent them from becoming too dominant. Because the mask of what's being dropped out has changed every single forward backward pass, effectively we end up training an ensemble of sub-networks. At test time, everything is fully enabled and all of those sub-networks are merged into a single ensemble, making it more robust.\n",
    "\n",
    "\n",
    "<img src=\"img/dropout.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full GPT with Multi-Head Attention and Transformer Block \n",
    "\n",
    "We finally get to the full GPT code, adding all the components explained above!\n",
    "\n",
    "**TODO:** 8b) In the summarized code below, comment each line to make sure you have understood all GPT components! You may use support from ChatGPT or GitHub Copilot, but double-check the results and be able to explain it yourself. (Yes, this is tedious, but it will help you get an in-depth understanding of the full GPT architecture) **(10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "10.809692 M parameters\n",
      "\n",
      "==============\n",
      "step 0: train loss 4.5842, val loss 4.5758\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "GOJ“iZ%%%%•oWMI’Ns‘HF0qoä6eZ88ßDYtX!(JöGP“f‘mb:“3e-2I,beCöa%A8;8DwkxYXzSÖbkXSMDdB!r1m3 wPP$9Ps—975T”,3v*!ßcfGUNcß3NQb\n",
      "möY%i(KD,%jl60R6üÖX9Üx9‘Ta”;r“l’ä\n",
      "9P)ÄCZs5faüP’bQ7cÖa6Z3MaÖp9PtIx*:(sfQWÄ6üeVUkQdo\n",
      "\n",
      "==============\n",
      "step 500: train loss 1.7782, val loss 3.8021\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "GIch mich—\n",
      "Wesaufe naber, schen.\n",
      "Bete hännetit gehr sichl gefanz nicht.\n",
      "Durch mem Nackter, ihr aucht scholt heit\n",
      "Nachön dagich weit neinen Schlibe\n",
      "Er uns der Kabterzt, nicht liete um maufenzen?\n",
      "\n",
      "MERGA\n",
      "\n",
      "==============\n",
      "step 1000: train loss 1.4102, val loss 4.1152\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Nachzen mein auf den egelbstehred g”e\n",
      "Dann kort du und langen einen!\n",
      "Meinschen Augerwirt.\n",
      "Die unser Hand die Scheite\n",
      "Du hinaustrechte Verbuß!\n",
      "Ein Frecht, fluchtier nur\n",
      "Nicht sich aus der Flästen Walb \n",
      "\n",
      "==============\n",
      "step 1500: train loss 1.1617, val loss 4.4471\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Von ist’s auch gewonni.\n",
      "\n",
      "Mir weiß zu platen! Sie abzutücknung,\n",
      "Mir viel zu gehen!\n",
      "\n",
      "FAUST.\n",
      "Du gehem Stillein neue schmütte.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Das Peine wird’s!\n",
      "\n",
      "FAUST (komm der Zeite).\n",
      "\n",
      "Wollt dich nun—h\n",
      "\n",
      "==============\n",
      "step 2000: train loss 0.8900, val loss 5.0233\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Laßt (immermehmen bohrtend).\n",
      "Grollen flähr immer ziehlingt,\n",
      "An im Grill immen sitztenglich das Balle!\n",
      "So schöne gleich und nie der!\n",
      "(Singt. FAUST (den Travend).\n",
      "Es ist Lebt und Leid, dir? Fort! ? Der \n",
      "\n",
      "==============\n",
      "step 2500: train loss 0.6132, val loss 5.7966\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Und, mein Fried erschaft, das ist die Luft ab.\n",
      "\n",
      "GEIST.\n",
      "Du noch Liebchen, die Gottes Aschuff verzegt;\n",
      "Sie hart das Schon Übelneigel geschroben!\n",
      "Betrug brauch uns dieser Täle tunche schein,\n",
      "Uns eine Gei\n",
      "\n",
      "==============\n",
      "step 3000: train loss 0.3756, val loss 6.6889\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Ihr wolche Weine, und Gesellen war zu treiben,\n",
      "Dann du dich eiles Natur verwählt,\n",
      "Um das Gute so liebernskrugen von ihm Gebärden,\n",
      "Dann mir durch meinest eile Nachbarstürzenscheit,\n",
      "Dir, um Zagemandwerk\n",
      "\n",
      "==============\n",
      "step 3500: train loss 0.2133, val loss 7.4314\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Ich glaube blächt’,\n",
      "Allein die frißen Worte treibt,\n",
      "Und mit meinem Scholken herausen!\n",
      "Gefällig die Loh gedrängt heimlich verüberstein,\n",
      "Und neu mich der Stelle bracht!\n",
      "Drum, sag ich, wenn dir zu reimer\n",
      "\n",
      "==============\n",
      "step 4000: train loss 0.1313, val loss 8.1139\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      ".\n",
      "Sppen gib Euch, nehmt mich zu sechen.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Ja, Herr! ich he!\n",
      "Als heißt ein Magd;\n",
      "Ihr seht die Gönner die Katze mehr.\n",
      "Hab ihre Nacht ihren, seht an,\n",
      "So schaue Äang die viel ungetrinkt\n",
      "Fau\n",
      "\n",
      "==============\n",
      "step 4500: train loss 0.0970, val loss 8.6510\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sololog in dem nieder,\n",
      "Dem Liebchen Geister und ab\n",
      "spiel ihr Fürwenlig fehlt sich mindertagen,\n",
      "Mit tiefer Sphären\n",
      "Möchten Götter nah einem andern knach.\n",
      "\n",
      "FROSCH.\n",
      "Wahrhaust ihr einmal wünschter Trau\n",
      "\n",
      "==============\n",
      "step 4999: train loss 0.0823, val loss 9.1966\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Der Jugend, der hat sie heilig überspiel.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Ich hat dann gelugnen Frieden,\n",
      "Und läßt mich gefohrten sein.\n",
      "\n",
      "FAUST.\n",
      "Ja, mein Kind! Der Kind ist wohl die Fächer auf gehn beschreiben?\n",
      "Du bes\n",
      "\n",
      "Final sample:\n",
      "\n",
      "Predende,\n",
      "Durch die eine, die ich genröten Geist,\n",
      "Die Meister, verbirgen sich mehr auf der Nachbarin zu deklärtchen Makrate;\n",
      "Sie nehmen durch Überreduft.\n",
      "\n",
      "FAUST.\n",
      "Hat die Königin im Fiertust, der Türen zehn\n",
      "Des Lebens ohne Lande wird,\n",
      "So freile die Seelei.\n",
      "(Er sich ihm umher.)\n",
      "\n",
      "Nach Heil und Diensteg, sein Gespenst von dem als gedrängten Wart,\n",
      "Und all ich nach dem Mutter Sinn\n",
      "In höchsten mich geschwinde Tage,\n",
      "Von Sort und Willigt liebt und bat.\n",
      "Weißt nicht die Natur dich dran Wanden.\n",
      "Ein Useh der Mensch, die Metappt—?\n",
      "\n",
      "FAUST.\n",
      "Und das ist so ein Schauerchüöfft!\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Da kann ich aus dem Hauch der wenten Tag gehölfunchte!\n",
      "\n",
      "FAUST.\n",
      "Beserk weillich Sieben gleich umwor\n",
      "Ein stilltken Glas von meinem Urinne-Gewöhne,\n",
      "Ein Gespenst sich zum führen kanr.\n",
      "Was die Menge zu befreien, das Leben,\n",
      "Nur die kam zu setzigen!\n",
      "Und eh man sich’s verschwatzt,\n",
      "Nun tuget, was das zu man und kleine Rasen,\n",
      "Dann außer Mann belpst von Gewölt Ihr\n",
      "Von einem Weibensteife,\n",
      "Und Erinnerlich das Pütze zunehmen.\n",
      "\n",
      "MARTHE.\n",
      "Es war ein Schoß zu Zeugnis Feier,\n",
      "Wohlt ich wie der rechte Leute.\n",
      "\n",
      "Verzeiheiter unter die Frau.\n",
      "\n",
      "MARTHE.\n",
      "Wenn ihr das? wo? Euch keine Furche den Schwelt.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Herr ich doch!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Warutig auf einem Punge verdauf,\n",
      "Mit armerkatzen und gehn Mephistopheles.\n",
      "\n",
      "FAUST.\n",
      "Mit von der Erstein?\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Ach weite Weh, dein selbst in dem Willen Lauf;\n",
      "Doch nennst nicht erst auch Erquickung trieben\n",
      "Und alle Stärkelsingen bestehrt;\n",
      "Es möchst das hoch erbärmlich laut,\n",
      "Mein In Nachtsgeweser darf hindet,\n",
      "Das Etwas diese trunkuit und prungend vor Messier.\n",
      "\n",
      "FAUST.\n",
      "Nun, was du ihr vergehsst? Liebsand! Er wär’s, daß nicht das Schauen gleicht,\n",
      "Das tros ist der Wangeraus und Nacht,\n",
      "Noch ihm dem allern Sinn.\n",
      "Wer fragt da armein Und himmlich an\n",
      "Sie scheinet mich auf ein Gefühle,\n",
      "Der ich meine Stunde herung, unter deine Schöpfe,\n",
      "Aus der Ferne, der Ste\n",
      "\n",
      "Schen ruhiger Geist euch mit einem Herzen,\n",
      "An Weiber und Sach ich verdrängt—\n",
      "Nun ach das! Er keinem Du etwas nicht und Verzeregt.\n",
      "(Er sch\n"
     ]
    }
   ],
   "source": [
    "# YOUR COMMENTS HERE\n",
    "batch_size = 16  # Number of samples processed per batch\n",
    "block_size = 32  # Length of each input sequence (context window)\n",
    "max_iters = 5000  # Total number of iterations to train\n",
    "eval_interval = 500  # How often to evaluate the model on validation set\n",
    "learning_rate = 1e-3  # Learning rate for optimization\n",
    "eval_iters = 200  # Number of iterations for loss estimation during evaluation\n",
    "n_embd = 64  # Dimensionality of the embedding vectors (feature size)\n",
    "n_head = 4  # Number of attention heads in the multi-head attention layer\n",
    "n_layer = 4  # Number of transformer blocks\n",
    "dropout = 0.2  # Dropout rate to prevent overfitting\n",
    "\n",
    "# hyperparameters version 2 - only uncomment when training on GPU (no comments needed here)\n",
    "# ´´´´\n",
    "batch_size = 64 \n",
    "block_size = 256 \n",
    "max_iters = 5000 \n",
    "eval_interval = 500 \n",
    "learning_rate = 3e-4 \n",
    "eval_iters = 200 \n",
    "n_embd = 384 \n",
    "n_head = 6 \n",
    "n_layer = 6 \n",
    "dropout = 0.2  \n",
    "#\n",
    "\n",
    "# Determine the device (GPU if available, otherwise CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "print('Running on device:', device)  # Print the device being used (CPU or GPU)\n",
    "\n",
    "# Function to get a batch of data for training or validation\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data  # Select data (train or validation)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # Randomly choose starting points\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # Create input sequences (x)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # Create target sequences (y)\n",
    "    x, y = x.to(device), y.to(device)  # Move data to the appropriate device (GPU or CPU)\n",
    "    return x, y  # Return the batch of inputs and targets\n",
    "\n",
    "# Function to estimate the loss on the training and validation sets\n",
    "@torch.no_grad()  # Disable gradient computation for loss estimation\n",
    "def estimate_loss():  \n",
    "    out = {}  # Dictionary to store the losses\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout)\n",
    "    for split in ['train', 'val']:  # Loop over training and validation sets\n",
    "        losses = torch.zeros(eval_iters)  # Initialize an array to store losses\n",
    "        for k in range(eval_iters):  # Loop to evaluate the model multiple times\n",
    "            X, Y = get_batch(split)  # Get a batch of data\n",
    "            logits, loss = model(X, Y)  # Forward pass to get logits and loss\n",
    "            losses[k] = loss.item()  # Store the loss for this iteration\n",
    "        out[split] = losses.mean()  # Calculate average loss for the split (train or val)\n",
    "    model.train()  # Switch the model back to training mode\n",
    "    return out  # Return the losses for both splits\n",
    "\n",
    "# Class for a single attention head\n",
    "class Head(nn.Module):  \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  # Key transformation\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Query transformation\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)  # Value transformation\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # Lower triangular mask for self-attention\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer to prevent overfitting\n",
    "\n",
    "    # Forward pass for the attention head\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Get the batch size (B), sequence length (T), and embedding size (C)\n",
    "        k = self.key(x)  # Apply key transformation to input\n",
    "        q = self.query(x)  # Apply query transformation to input\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5  # Scaled dot-product attention\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Apply mask for causal attention\n",
    "        wei = F.softmax(wei, dim=-1)  # Apply softmax to get attention weights\n",
    "        wei = self.dropout(wei)  # Apply dropout to attention weights\n",
    "        \n",
    "        v = self.value(x)  # Apply value transformation\n",
    "        out = wei @ v  # Weighted sum of values based on attention weights\n",
    "        \n",
    "        return out  # Return the output of the attention head\n",
    "\n",
    "# Multi-head attention that combines multiple attention heads\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):  \n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # List of attention heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # Linear projection after concatenating all heads\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "\n",
    "    # Forward pass for multi-head attention\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # Concatenate outputs of all attention heads\n",
    "        out = self.dropout(self.proj(out))  # Apply projection and dropout\n",
    "        return out  # Return the multi-head attention output\n",
    "\n",
    "# Feed-forward neural network for the transformer block\n",
    "class FeedFoward(nn.Module):  \n",
    "    def __init__(self, n_embd):  # Initialize with embedding size\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # First linear layer with expansion\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Second linear layer with compression\n",
    "            nn.Dropout(dropout),  # Dropout for regularization\n",
    "        )\n",
    "\n",
    "    # Forward pass for the feed-forward network\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # Return the output after passing through the network\n",
    "\n",
    "# Transformer block containing self-attention and feed-forward layers\n",
    "class Block(nn.Module):  \n",
    "    def __init__(self, n_embd, n_head):  \n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # Size of each attention head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # Self-attention layer\n",
    "        self.ffwd = FeedFoward(n_embd)  # Feed-forward network\n",
    "        self.ln1 = nn.LayerNorm(n_embd)  # Layer normalization before self-attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd)  # Layer normalization before feed-forward network\n",
    "\n",
    "    # Forward pass for the transformer block\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))  # Add residual connection after self-attention\n",
    "        x = x + self.ffwd(self.ln2(x))  # Add residual connection after feed-forward network\n",
    "        return x  # Return the output\n",
    "\n",
    "# GPT language model consisting of embeddings, transformer blocks, and output layer\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # Token embeddings\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # Positional embeddings\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])  # Stacked transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer normalization\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # Output layer for language modeling (vocab_size)\n",
    "\n",
    "    # Forward pass through the GPT model\n",
    "    def forward(self, idx, targets=None):  \n",
    "        B, T = idx.shape  # Get batch size (B) and sequence length (T)\n",
    "        tok_emb = self.token_embedding_table(idx)  # Get token embeddings\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # Get positional embeddings\n",
    "        x = tok_emb + pos_emb  # Combine token and positional embeddings\n",
    "        x = self.blocks(x)  # Pass through transformer blocks\n",
    "        x = self.ln_f(x)  # Apply final layer normalization\n",
    "        logits = self.lm_head(x)  # Get logits (predictions) for each token\n",
    "\n",
    "        if targets is None:  # If no targets are provided, return only logits\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape  # Get shape of logits\n",
    "            logits = logits.view(B * T, C)  # Flatten logits\n",
    "            targets = targets.view(B * T)  # Flatten targets\n",
    "            loss = F.cross_entropy(logits, targets)  # Calculate cross-entropy loss\n",
    "\n",
    "        return logits, loss  # Return logits and loss (if targets are provided)\n",
    "\n",
    "    # Generate new tokens given a starting context (idx)\n",
    "    def generate(self, idx, max_new_tokens):  \n",
    "        for _ in range(max_new_tokens):  # Loop to generate new tokens\n",
    "            idx_cond = idx[:, -block_size:]  # Keep only the last block_size tokens as context\n",
    "            logits, loss = self(idx_cond)  # Get logits for the next token\n",
    "            logits = logits[:, -1, :]  # Get logits for the last token\n",
    "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample next token from the distribution\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # Append the next token to the input sequence\n",
    "        return idx  # Return the generated sequence\n",
    "\n",
    "# Initialize the GPT model\n",
    "model = GPTLanguageModel()  \n",
    "model = model.to(device)  # Move model to the chosen device (CPU or GPU)\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "# Set up the optimizer (AdamW)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:  # Periodically evaluate the model\n",
    "        losses = estimate_loss()  # Get the losses for train and validation\n",
    "        print(\"\\n==============\")\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")  # Print losses\n",
    "        print(\"==============\")\n",
    "        \n",
    "        print(\"\\nSample:\")  \n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Start with an empty context\n",
    "        print(decode(model.generate(context, max_new_tokens=200)[0].tolist()))  # Generate a sample of text\n",
    "\n",
    "    # Get a batch of training data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass and loss calculation\n",
    "    logits, loss = model(xb, yb)  \n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear previous gradients\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "# Final text generation after training\n",
    "print(\"\\nFinal sample:\")  # Generate a longer sample after training\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))  # Generate a text sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have trained a more powerful GPT model using self-attention. Let's generate a longer text and see how the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "übenden Verzeihen erbärger vor.\n",
      "Der Menge heilt Schwinder\n",
      "Dich därmer Namen alten himmlischen Töge?\n",
      "Der Schlüssel und ihren auf der Schlang\n",
      "Und hießen hat ein großes Glut vergeben und gaffen,\n",
      "Und wie er, nicht sich in Stellendsges wandt.\n",
      "Da werd ihr hier Himmelslicht, ich vom Engel geben\n",
      "den sich geben aus dem Bißchen \n",
      "Göttchen mir dieser zu halten.\n",
      "Kein hoher Kupf und Schwalt,\n",
      "Der hat keinen großen Kessel gleichen Wert,\n",
      "Um  ewig erfüllem Stunden beschweren,\n",
      "Das Geflüg sich ungeben nach und führe sie.\n",
      "\n",
      "CHOR DER WEIBEL.\n",
      "Find und die vor kimmen Alten,\n",
      "Mit vollem Grassen sein,\n",
      "Wie weiß, das so ein Kessel!\n",
      "\n",
      "Das Möbkt sich so und Tage schwört!\n",
      "\n",
      "FAUST.\n",
      "Wölbst du die Ende, bin die Flamme beste\n",
      "Verholte sie sich unvorwählt der Mitte.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Ganz retchen, die Stadt wird gesteckt,\n",
      "Wird für es wohl die Menschen wogne haben.\n",
      "\n",
      "Du hast mich vor dem Herren enge spraden:\n",
      "Da, wer glühtet alle Er war.\n",
      "“Aber wardelte hält er grieben,\n",
      "Sie verengeliebten Liebenswühlen\n",
      "So sind Tag\n",
      "Fehl und die Herzen schlimmern,\n",
      "Dildet uns gefällig sich an;\n",
      "Damit das Denn, durch das Ent,\n",
      "So wird ihr Pund, und das ist und tu!\n",
      "\n",
      "Faust. Wällt! weh das Des sehr inter?\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Wir laß mich auch.\n",
      "\n",
      "DIE ER ZWEIT.\n",
      "Wir geden anders edletzusehen,\n",
      "Wir oft und Taten soll geprägt.\n",
      "Und stets das von Nachwert,\n",
      "Das ist dumpf, und hast du ein Stein übelreib,\n",
      "Die Frucht jeden auf eine Welt.\n",
      "Die Hirn, verzeihn ich die Flamme schweifen,\n",
      "Kalt auf der Nacht’ger Schluß.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Ich , gut der Frau, dem Menschen mit bequemen,\n",
      "Den Barg, vorhate sie allen,\n",
      "Und war ich mir als einen beschweren.\n",
      "\n",
      "nAber weilet sie einem Kuß ist,\n",
      "Das ist die Fässernis nimmer weicht.\n",
      "Sich hier!\n",
      "\n",
      "(Sie tringt auf dem Betracht.)\n",
      "\n",
      "GRETCHEN.\n",
      "Das ist der Lorbe,\n",
      "Beschabend Schweben.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Haltet und wir vor den\n",
      "Seines Raung aus dem Anter sittert,\n",
      "Den ihres aufzen Büchelchen sind,\n",
      "Hier selbst gar nicht, wie in Welt unter,\n",
      "Ein Schwer, ach! ich alte geseht.\n",
      "Sie recht zu hat mich vor secht flommen,\n",
      "Will Euch und verdichte stecken.\n",
      "\n",
      "CHORUS (jauchzetdier).\n",
      "Die Heiß ich gleich,\n",
      "Die luftet nicht ein Flammelbaut\n",
      "Ist meine faßt mir zu sagen.\n",
      "\n",
      "BÜRGERMÄDCHEN.\n",
      "AL.\n",
      "Meine Art, war das hat der Schmach ich berg hät,\n",
      "\n",
      "SCHÜLER.\n",
      "Ich seh wacker weit was von und raschem Beinen.\n",
      "Wie lang der Kirche sagt\n",
      "Und wir’s ein schwack, das eben und steht;\n",
      "Und läßt die Wirber ehrwormen,\n",
      "Doch ihr dem Weniger trefflich sprechen.\n",
      "Wo sie hört, Euch diese Lieben mir zerführen.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Erk ich, den Geist sind von Ding,\n",
      "Des wie ein Stimmel zu befreiten sie,\n",
      "Zu nötig Jahr doch auch herbei,\n",
      "Ich weiß woll es das mein Verbreiten.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Der Weg ist erworte nicht,\n",
      "Sich hätte viermächtig zu Leibe.\n",
      "\n",
      "FAUST.\n",
      "Auf ein Teufel weh ich wohl dir gern vehören?\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Ich kann die Sterne mag’s nicht behalten,\n",
      "Wären  es ein Geistes geben fiehlt kann!\n",
      "Mir soll das Dann verbracht,\n",
      "Der wird ich im lang Geschieben und Triebe\n",
      "Durch die nicht übergaben sonnen,\n",
      "Die Uhr mückte ganze versen,\n",
      "Wir manche Lände darauf!\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Denkte gnug, den ersteht sieht, den schönkt entbeht Geschmausen.\n",
      "\n",
      "FAUST.\n",
      "Wie rasch in Liebes Kinderspiel!\n",
      "Auf ein dem hohen Kopf vorüber,\n",
      "So ein Busen einer schnelle dem Musikrot!”\n",
      "(Er herr vor den Schmerzte weichen sie mit Kavoll aus.\n",
      "Denkt und lauschtet an Fauster, Weine freue bildet und spricht,\n",
      "Und Fels und Bäulen, zu Leider.\n",
      "\n",
      "CHOR DER Juderchen die Stadt durchs Leibe,\n",
      "Die sich mildert um Nechte beld Herzen.\n",
      "\n",
      "MARGARETE.\n",
      "Ach seh ich wie ein Feierchen,\n",
      "Das ist nicht lange nicht vor den Herdens aus!\n",
      "In Blatt vergebens dir, was in Nacht,\n",
      "Mit Euch, Hexenellig sehn?\n",
      "Soll ich vielleicht in der Flammen ersen,\n",
      "Bilde mir nicht ihn im Weise bang,\n",
      "Und vor mit Menschweit geist und zu erfinden!\n",
      "Und möcht ein letzt ich wüßte.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Wie gar denn du?\n",
      "\n",
      "FAUST.\n",
      "Betrauch nicht hin!\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "So glaubst du dir der ganze Studienstot!\n",
      "\n",
      "FAUST.\n",
      "Wollen wir diesen Augenblick verlieren,\n",
      "Wenn Er dich weit zurücke Mann sie bringt;\n",
      "Sie hat da rausche mich vergebens drüh.\n",
      "\n",
      "Welch wollte mir von dem ganzen,\n",
      "Mein Herz ist’s der Lohn,\n",
      "Und kein Schaff ich weit im Gebetrafft;\n",
      "Es war ihr’s niemals doch unterm so\n",
      "Herz ihn in meinem kühlen Magen,\n",
      "Der Alle schlägt, das qund um und kat und höcht fernährt!\n",
      "Doch gut! man sie volles schwärzen!\n",
      "Es diesen schon Geist sich verrücken!\n",
      "(Er von die He vor unterinnem nasch.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Frosch! Er gibt ihn ch nicht vergeben.\n",
      "Ihr hat mich nicht auch hier,\n",
      "Von dem, vertrauen Euch in bedingen,\n",
      "Als hätte ich wie nicht zu solchen darum\n",
      "Herunter die Lieber und durch die Welt ein\n",
      "So selbst die Erge verheiment,\n",
      "Wie der Stadt zu Streben,\n",
      "Sie sie treichen heil’gen Läufen,\n",
      "Als sehn und Böser weiche Beinet\n",
      "Gebt hin, in den Flügeln mir im Wege schweben,\n",
      "Auf heutauf ein Zaudendmalert!\n",
      "So etwas ich mit dem Kerkerlein\n",
      "Und hoff meinem Getrug schwerz,\n",
      "Ich gäb gesieg sich und übert es wert,\n",
      "Ein Ihr mein Tag gescherzt sogleich zu sei!\n",
      "Wie mir trüber fühl ich von die Menge sein\n",
      "Welche selbst durch deinen Rampen an.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Encgst zu dir ein Leben, was und wir beildet!\n",
      "\n",
      "FAUST.\n",
      "Verfluchtes Wilder!\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Herein Doktor, fasse mich! ich für ein Leben fließen,\n",
      "Bst du den Geist des Hichs von meinem Ungestandcht?\n",
      "Soll ich etwa in meiner geheil’gen Garte,\n",
      "Wenn das ein volles Kirchen schwersendeerei,\n",
      "Den Unbezuheuerhöhelter sein.\n",
      "Im diesen Himmele küchen sich.\n",
      "Zwar schon Wonner, die hörten Lassen!\n",
      "\n",
      "FAUST (welchend).\n",
      "Bestiegt mir auf uns vor dem Besen fliehn?\n",
      "Wo sagt der Zeig nicht schon leut verbindet!\n",
      "Wer mag wir’s, als nicht schon weißt,\n",
      "Sie bleibt’ euch den Gesellen,\n",
      "Den ihr mag zu neuen!\n",
      "Er hat man nicht was Recht berghangen.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Er sollt Euch er nicht in unterlich.\n",
      "Er sprich keinem Nachbarisch.\n",
      "Ich seh habt sie dich unterträglich.\n",
      "In Euch das Vergönnen Nachben seit,\n",
      "Laß mit dem Schn inbeben Langewührt,\n",
      "So ist mir ein Traum den Teges auszusen,\n",
      "\n",
      "Der eing Geschmich auf eint euch gaffen,\n",
      "Als am Fels und Scherden Baus\n",
      "UST.\n",
      "So ahn sieh die ergetangen,\n",
      "Aus dem sie Steigen eigen sfahr,\n",
      "Das ist der Streben wirNicht allzu.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Und kommt die Putze der Putze sorgen,\n",
      "Mit tausend Nacht einer seinem Steiße\n",
      "Wie wenig den Augenblick von Herzen.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Du bist am Ende kann nichts was.\n",
      "Zu Willken will ich gerne und brav,\n",
      "Begreiflich und du wirst, für deine Gunschaft!\n",
      "\n",
      "FAUST.\n",
      "Ich habe jede dank das wenig Hut und trugt,\n",
      "Mit voller Dirne schritt!\n",
      "Und blangt sie es nicht passen kann;\n",
      "Mir nicht seht um an der Stirn geschwören.\n",
      "Ich fühlt, aß dem Walz trog und bloß.\n",
      "Ich hör von je der Türeusche Leben schnauf!\n",
      "\n",
      "WAGNER.\n",
      "Ich seh it Euch, und war es zu behagen,\n",
      "Weil ich, es in dem scharfigen Natur,\n",
      "Da ich man Nachbarin, Und aufren Sötelslöhne;\n",
      "Uneine Gesellschaft bricht,\n",
      "Daß ich nack mein Werden, der eine Kanzen.\n",
      "\n",
      "MEPHISTOPHISTOPHELES.\n",
      "Ei wäret, wen der s glühen Leiden\n",
      "Soll keinen Schwestenkten sie ein.\n",
      "\n",
      "FAUST.\n",
      "Ich schreien Geschwill!\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Verlaube nur ein bißchern Zügen,\n",
      "Weil rater, Meine zieht es nüssen.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Ich hätte gern, welcher zu übertragen.\n",
      "\n",
      "FAUST.\n",
      "Ich fühl Er die Zeit, nicht auf der Bein\n",
      "Ein Diener zu deinem Pudel spelk zu streifen.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Das will Euch noch hergeben wart.\n",
      "“Reinse mit ihr mir dauern Knecht!”\n",
      "Der Die wird ihr ein Liebchen vorbeiten,\n",
      "Leide langb Kind ihr auf den seinem Inher,\n",
      "Und führ mich ehr werdetzu Augen nund häufen,\n",
      "Und mag, es steht auf.\n",
      "WAGNER.\n",
      "Aber sind so bald genugt, mein Geld für sie seh und lockt;\n",
      "Ich brauch ihn niemander sicht,\n",
      "Und was ihr weit es zu sogen.\n",
      "Den Gesetz und da ist stolzt, göttlisch Bist auf und schön!\n",
      "Neige schwind im Kosser durchs Flügel.\n",
      "Ich darf nicht aus einem edlert,\n",
      "Das eine Blutgebraucht ein törig,\n",
      "Das Dilettsum stillt,\n",
      "Und was du man nicht verstörben,\n",
      "Nach als Abgraus des Kinderscheide voll!\n",
      "Juchhe!\n",
      "Juchheisa! Heisa! Brist nicht sie gedonnt,\n",
      "Und als war denkt, die Kleine sie täuscht(ung.)\n",
      "\n",
      "Was hilft und Faust uns zu deines Sticken nah,\n",
      "Und Dreitert ein wunder Gott!\n",
      "MARTHE.\n",
      "Ach seh Sie nur, was die Augen engen Zer eiten,\n",
      "In der Welt wahrlich und wir verehr.\n",
      "\n",
      "Ich höre, so ihn Geüten, gesellschafft!\n",
      "\n",
      "FAUST.\n",
      "Ich bin alt, was zu fühl’gen, nur hat zu tragen!\n",
      "Du willst du gehst mir ein Genieben!\n",
      "(Er facht mit dem lange das Banden steht; den Kirch nicht erweicht,\n",
      "Sie heimlich auch was auch dem Stuchen bein.\n",
      "\n",
      "SCHÜLER.\n",
      "Agesk! wie hör dies! doch der Stumme zu erstinden.\n",
      "Sie hätte die Gär! ich wieder\n",
      "Den sich umsprechsten Gewiß,\n",
      "Auf hör ein Häuseln, gespeinen im Herzen,\n",
      "Wo nimm dann zu bnagen,\n",
      "Wo es nicht etwas vorzugeließen kann.\n",
      "Doch deine Menschen stehe leben,\n",
      "Durch die Wiege frei\n",
      "Ers und das Pein\n",
      "Ein Mutter legt sich die Dichter zu bezwingen.\n",
      "Ein töchten sie nur hat mich überreichen,\n",
      "Daß Ihr sogar fangt un eine Wonnerhöhe Lacht;\n",
      "Sie keinem Damiter vor ein Liebensflehn!\n",
      "Hat Er ihn hier im Feier,\n",
      "An seinen Hoschgelagt geschmachten,\n",
      "Daß ich liebte bald die Dirne gleit.\n",
      "(Er leisen beizendet).\n",
      "Mein Bruder webt sogleich Flut.\n",
      "\n",
      "FAUST.\n",
      "Verflucht eines Ruh,\n",
      "Der den Augenblick ergift und Durchssel!\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Der Puppen und diesel spalt,\n",
      "Zum Alten Hatenberat,\n",
      "Da iine liebende ja!\n",
      "Läng das Bett im Herzen und drängt.\n",
      "Bist es doch!\n",
      "In Standel werde sich überrück,\n",
      "Faßt und laufen auf dieser Schlange.\n",
      "\n",
      "Der Vater von hintern und zu Tiere.\n",
      "Flieh, die welche bewigt entfernen Seeleben,\n",
      "Er sieht die sich und und immer nah.\n",
      "Armlitzun er Tag und wachen, das alles wissen sein.\n",
      "Zwar eng, das ist, mein bildes Leben war die Verderben.\n",
      "(Faust recht, in Busikzenden und zu deicht\n",
      "merlich webt, um schon\n",
      "Mit durch die Welt herangewölbt,\n",
      "Den Schleichen hattet die Natur\n",
      "Des Des Menschwolt und Lager,\n",
      "Noch mit erstanden Gabeden.\n",
      "(Sie lauft erbiegen vor Auf.)\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Grau (aufstreuend).\n",
      "Nun frisches Verhören\n",
      "Und mit euch dieser Macht—\n",
      "(Er seht’s erbährend.)\n",
      "\n",
      "SCHÜLER (liest).\n",
      "Fort! immt so gehen das wohl der Welt!\n",
      "Das hat der Müsse schupf, aller die Welt!\n",
      "\n",
      "Aber Fackser, Scheinte, würde\n",
      "Zeit nur führen Geträglichen, zu behen,\n",
      "Und leiden bei der Knaben,\n",
      "Die ihre trübe Giebend fühl immer\n",
      "In Buser an.\n",
      "\n",
      "FAUST.\n",
      "Du Ungehst nicht und holde Bein\n",
      "Und Doch fühl ich was vor.\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Ich führe\n"
     ]
    }
   ],
   "source": [
    "# generate a longer sample\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "new_text = decode(model.generate(context, max_new_tokens=10000)[0].tolist())\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to a text file\n",
    "f =  open(\"GPT_generated_text.txt\",\"w\")\n",
    "f.write(new_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (optional):** Apply the code to a different text of your choice! What loss do you achieve? What parameters did you change and why? How do you interpret the output compared to the Shakespeare output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlook and Next Steps\n",
    "### Andrej's Suggested Further Experiments\n",
    "\n",
    "- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
    "- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
    "- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n",
    "- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder and Encoder\n",
    "\n",
    "Text generation as above only uses the **decoder** part of the transformer architecture. The **decoder attention block** implemented above has **triangular masking**, and is usually used in autoregressive settings, like language modeling. \n",
    "\n",
    "In other settings, we do want \"future\" tokens to influence the prediction, so we do not use triangular masking. For example, in sentiment analysis, we look at a whole sentence at once, then predict the sentiment \"happy\" or \"sad\" of the speaker. This can be realized using an **encoder** attention block. To implement an encoder attention block, we can simply delete the single line that does masking with `tril`, allowing all tokens to communicate. Attention does not care whether tokens from the future contribute or not, it supports arbitrary connectivity between nodes.\n",
    "\n",
    "### From Self-Attention to Cross-Attention\n",
    "\n",
    "**Self-attention** means that the keys and values are produced from the same source as queries. In **cross-attention**, the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module). For example, when translating from French to English, we condition the decoding on the past decoding *and* the fully encoded french prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From GPT to ChatGPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a long way to go from our toy GPT example to ChatGPT. \n",
    "First of all, ChatGPT's **pre-training** was done on a large chunk of internet, resulting in a decoder-only transformer for text generation. \n",
    "So the pretraining is quite similar to our toy example training, except that we used roughly 10 million parameters and the largest transformer for ChatGPT uses 175 billion (!) parameters. Also it was trained on 300 billion tokens (our training set would be 300.000 tokens roughly when not using character-level tokens, but sub-word chunks). This is about a million fold increase in number of tokens - and today, even bigger datasets are used with trillions of tokens for training on thousands of GPUs!\n",
    "\n",
    "See the following table for the number of parameters, number of layers, n_embd, number of heads, head size, batch size and learning rate in **GPT-3**:\n",
    "\n",
    "\n",
    "<img src=\"img/GPT3_params_table.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pre-training, the model will be a document completer, it will not give answers but produce more questions or result in some undefined behavior. For becoming an assistant, further **fine-tuning** is needed using **Reinforcement Learning from Human Feedback (RLHF)**. Here is an overview of manual fine-tuning with human AI trainers (see the OpenAI ChatGPT blog for details, link below):\n",
    "\n",
    "<img src=\"img/chatgpt_diagram_light.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "To sum it up, we trained a decoder only Transformer following the famous paper 'Attention is All You Need' from 2017, which is basically a GPT. We saw how using self-attention, we can calculate a weighted average of past tokens to predict the next token. We trained it on different texts (Shakespeare, Faust, Jane Austen etc.) and produced new texts in the same writing style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- Attention is All You Need paper: https://arxiv.org/abs/1706.03762\n",
    "- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 \n",
    "- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# truncate long cell output to avoid large pdf files\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtruncate_output\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m truncate_long_notebook_output\n\u001b[0;32m     12\u001b[0m truncated \u001b[38;5;241m=\u001b[39m truncate_long_notebook_output(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3_Character_Level_GPT__solution.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# convert to pdf with nbconvert\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "# This cell truncates long output to a maximum length, then converts the notebook to a PDF\n",
    "# NOTE: You may have to adapt the path or filename to match your local setup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# truncate long cell output to avoid large pdf files\n",
    "from helpers.truncate_output import truncate_long_notebook_output\n",
    "truncated = truncate_long_notebook_output('3_Character_Level_GPT__solution.ipynb')\n",
    "\n",
    "# convert to pdf with nbconvert\n",
    "if truncated:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download TRUNCATED_3_Character_Level_GPT__solution.ipynb\n",
    "else:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download 3_Character_Level_GPT__solution.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
