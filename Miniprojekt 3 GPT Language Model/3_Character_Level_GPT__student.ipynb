{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Building a GPT from Scratch\n",
    "---\n",
    "\n",
    "This is an extended version of Andrej Karpathy's notebook in addition to his [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.\n",
    "\n",
    "Adapted by: \n",
    "\n",
    "Prof. Dr.-Ing. Antje Muntzinger, University of Applied Sciences Stuttgart\n",
    "\n",
    "antje.muntzinger@hft-stuttgart.de\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We'll construct a character-level **GPT (Generative Pretrained Transformer)** model from scratch. **Transformer** is the name of the underlying neural net architecture that was introduced in the 2017 groundbreaking paper \"Attention is All You Need\" (Link at the bottom).\n",
    "The model will be trained on different texts, for example Shakespeare, Goethe's \"Faust\", the \"Lord of the Rings\" or books from Jane Austen, and will be able to generate new text based on the text from the book.\n",
    "\n",
    "\n",
    "**NOTE:** You may answer in English or German.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "[1. Loading the Data](#1.-Loading-the-Data)\n",
    "\n",
    "[2. Tokenization](#2.-Tokenization)\n",
    "\n",
    "[3. Making Training Mini-Batches](#3.-Making-Training-Mini-Batches)\n",
    "\n",
    "[4. Defining the Network with PyTorch](#4.-Defining-the-Network-with-PyTorch)\n",
    "\n",
    "[5. Training](#5.-Training)\n",
    "\n",
    "[6. The Mathematical Trick in Self-Attention](#6.-The-Mathematical-Trick-in-Self-Attention)\n",
    "\n",
    "[7. Self-Attention](#7.-Self-Attention)\n",
    "\n",
    "[8. Full GPT Implementation](#9.-Full-GPT-Implementation)\n",
    "\n",
    "[9. Outlook and Next Steps](#9.-Outlook-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x126a1f810>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# select the right file and read it in to inspect it\n",
    "with open('faust.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('austen.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('LOTR.txt', 'r') as f:\n",
    "# with open('LOTR_TVscript.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1a) Find out the length of the dataset and print the first 1000 characters! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  205807\n",
      "﻿Faust:\n",
      "Der Tragödie erster Teil\n",
      "\n",
      "by Johann Wolfgang von Goethe\n",
      "\n",
      "\n",
      "Zueignung\n",
      "\n",
      "\n",
      "Ihr naht euch wieder, schwankende Gestalten,\n",
      "Die früh sich einst dem trüben Blick gezeigt.\n",
      "Versuch ich wohl, euch diesmal festzuhalten?\n",
      "Fühl ich mein Herz noch jenem Wahn geneigt?\n",
      "Ihr drängt euch zu! nun gut, so mögt ihr walten,\n",
      "Wie ihr aus Dunst und Nebel um mich steigt;\n",
      "Mein Busen fühlt sich jugendlich erschüttert\n",
      "Vom Zauberhauch, der euren Zug umwittert.\n",
      "\n",
      "Ihr bringt mit euch die Bilder froher Tage,\n",
      "Und manche liebe Schatten steigen auf;\n",
      "Gleich einer alten, halbverklungnen Sage\n",
      "Kommt erste Lieb und Freundschaft mit herauf;\n",
      "Der Schmerz wird neu, es wiederholt die Klage\n",
      "Des Lebens labyrinthisch irren Lauf,\n",
      "Und nennt die Guten, die, um schöne Stunden\n",
      "Vom Glück getäuscht, vor mir hinweggeschwunden.\n",
      "\n",
      "Sie hören nicht die folgenden Gesänge,\n",
      "Die Seelen, denen ich die ersten sang;\n",
      "Zerstoben ist das freundliche Gedränge,\n",
      "Verklungen, ach! der erste Widerklang.\n",
      "Mein Lied ertönt der unbekannten Menge,\n",
      "Ihr Beifall selbst\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1b) Store all unique characters that occur in this text in `chars` and print them. Store the number of unique characters in `vocab_size` and print the result. **(3 points)**\n",
    "\n",
    "**Hint:** First make a set of all characters to remove duplicates, then make a list out of them to get a unique ordering, and finally sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$%()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÄÖÜßäöü—‘’“”•™﻿\n",
      "vocab_size= 92\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size=', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to **tokenize** the input. This means, we convert the raw text string to some sequence of integers according to some **vocabulary** of possible elements. A **token** can be a character like here, or a piece of a word like in ChatGPT. For a character-level language model, we just translate each character to an integer (**encoding**) and vice-versa (**decoding**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '%', 5: '(', 6: ')', 7: '*', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '?', 25: 'A', 26: 'B', 27: 'C', 28: 'D', 29: 'E', 30: 'F', 31: 'G', 32: 'H', 33: 'I', 34: 'J', 35: 'K', 36: 'L', 37: 'M', 38: 'N', 39: 'O', 40: 'P', 41: 'Q', 42: 'R', 43: 'S', 44: 'T', 45: 'U', 46: 'V', 47: 'W', 48: 'X', 49: 'Y', 50: 'Z', 51: 'a', 52: 'b', 53: 'c', 54: 'd', 55: 'e', 56: 'f', 57: 'g', 58: 'h', 59: 'i', 60: 'j', 61: 'k', 62: 'l', 63: 'm', 64: 'n', 65: 'o', 66: 'p', 67: 'q', 68: 'r', 69: 's', 70: 't', 71: 'u', 72: 'v', 73: 'w', 74: 'x', 75: 'y', 76: 'z', 77: 'Ä', 78: 'Ö', 79: 'Ü', 80: 'ß', 81: 'ä', 82: 'ö', 83: 'ü', 84: '—', 85: '‘', 86: '’', 87: '“', 88: '”', 89: '•', 90: '™', 91: '\\ufeff'}\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2a) Test the code above by encoding some sentence of your choice and decoding it again. Print the encoded and decoded result. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [32, 55, 62, 62, 65, 8, 1, 30, 51, 71, 69, 70, 2]\n",
      "Decoded: Hello, Faust!\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "sentence = \"Hello, Faust!\"\n",
    "\n",
    "encoded_test_sentence = encode(sentence)\n",
    "print(\"Encoded:\", encoded_test_sentence)\n",
    "\n",
    "decoded_test_sentence = decode(encoded_test_sentence)\n",
    "print(\"Decoded:\", decoded_test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tokenization is a trade-off between vocabulary size and sequence length: Large vocabularies will lead to shorter encoding sequences and vice versa. For example, encoding each character results in a short vocabulary of 26 tokens for the standard alphabet plus some more for special characters, but each word consists of longer encodings. On the other hand, encoding on word level means each word is encoded as a single token, but the vocabulary will be much larger (up to a whole dictionary of hundreds of thousands of words for one language). In practice, for example in ChatGPT, **sub word encodings** are used, which means not encoding entire words, but also not encoding individual characters. Instead, some intermediate format is used, for example the word 'undefined' could be encoded as three tokens: 'un', 'define', 'd'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2b) Encode the entire text dataset and store it into a `torch.tensor` with `dtype=torch.long`. This will be our input data for the model, and we name it `data`. \n",
    "Print the shape and dtype of `data` and the first 1000 characters of the encoded text for comparison with the text above. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: torch.Size([205807])\n",
      "Dtype of data: torch.int64\n",
      "First 1000 encoded characters: tensor([91, 30, 51, 71, 69, 70, 22,  0, 28, 55, 68,  1, 44, 68, 51, 57, 82, 54,\n",
      "        59, 55,  1, 55, 68, 69, 70, 55, 68,  1, 44, 55, 59, 62,  0,  0, 52, 75,\n",
      "         1, 34, 65, 58, 51, 64, 64,  1, 47, 65, 62, 56, 57, 51, 64, 57,  1, 72,\n",
      "        65, 64,  1, 31, 65, 55, 70, 58, 55,  0,  0,  0, 50, 71, 55, 59, 57, 64,\n",
      "        71, 64, 57,  0,  0,  0, 33, 58, 68,  1, 64, 51, 58, 70,  1, 55, 71, 53,\n",
      "        58,  1, 73, 59, 55, 54, 55, 68,  8,  1, 69, 53, 58, 73, 51, 64, 61, 55,\n",
      "        64, 54, 55,  1, 31, 55, 69, 70, 51, 62, 70, 55, 64,  8,  0, 28, 59, 55,\n",
      "         1, 56, 68, 83, 58,  1, 69, 59, 53, 58,  1, 55, 59, 64, 69, 70,  1, 54,\n",
      "        55, 63,  1, 70, 68, 83, 52, 55, 64,  1, 26, 62, 59, 53, 61,  1, 57, 55,\n",
      "        76, 55, 59, 57, 70, 10,  0, 46, 55, 68, 69, 71, 53, 58,  1, 59, 53, 58,\n",
      "         1, 73, 65, 58, 62,  8,  1, 55, 71, 53, 58,  1, 54, 59, 55, 69, 63, 51,\n",
      "        62,  1, 56, 55, 69, 70, 76, 71, 58, 51, 62, 70, 55, 64, 24,  0, 30, 83,\n",
      "        58, 62,  1, 59, 53, 58,  1, 63, 55, 59, 64,  1, 32, 55, 68, 76,  1, 64,\n",
      "        65, 53, 58,  1, 60, 55, 64, 55, 63,  1, 47, 51, 58, 64,  1, 57, 55, 64,\n",
      "        55, 59, 57, 70, 24,  0, 33, 58, 68,  1, 54, 68, 81, 64, 57, 70,  1, 55,\n",
      "        71, 53, 58,  1, 76, 71,  2,  1, 64, 71, 64,  1, 57, 71, 70,  8,  1, 69,\n",
      "        65,  1, 63, 82, 57, 70,  1, 59, 58, 68,  1, 73, 51, 62, 70, 55, 64,  8,\n",
      "         0, 47, 59, 55,  1, 59, 58, 68,  1, 51, 71, 69,  1, 28, 71, 64, 69, 70,\n",
      "         1, 71, 64, 54,  1, 38, 55, 52, 55, 62,  1, 71, 63,  1, 63, 59, 53, 58,\n",
      "         1, 69, 70, 55, 59, 57, 70, 23,  0, 37, 55, 59, 64,  1, 26, 71, 69, 55,\n",
      "        64,  1, 56, 83, 58, 62, 70,  1, 69, 59, 53, 58,  1, 60, 71, 57, 55, 64,\n",
      "        54, 62, 59, 53, 58,  1, 55, 68, 69, 53, 58, 83, 70, 70, 55, 68, 70,  0,\n",
      "        46, 65, 63,  1, 50, 51, 71, 52, 55, 68, 58, 51, 71, 53, 58,  8,  1, 54,\n",
      "        55, 68,  1, 55, 71, 68, 55, 64,  1, 50, 71, 57,  1, 71, 63, 73, 59, 70,\n",
      "        70, 55, 68, 70, 10,  0,  0, 33, 58, 68,  1, 52, 68, 59, 64, 57, 70,  1,\n",
      "        63, 59, 70,  1, 55, 71, 53, 58,  1, 54, 59, 55,  1, 26, 59, 62, 54, 55,\n",
      "        68,  1, 56, 68, 65, 58, 55, 68,  1, 44, 51, 57, 55,  8,  0, 45, 64, 54,\n",
      "         1, 63, 51, 64, 53, 58, 55,  1, 62, 59, 55, 52, 55,  1, 43, 53, 58, 51,\n",
      "        70, 70, 55, 64,  1, 69, 70, 55, 59, 57, 55, 64,  1, 51, 71, 56, 23,  0,\n",
      "        31, 62, 55, 59, 53, 58,  1, 55, 59, 64, 55, 68,  1, 51, 62, 70, 55, 64,\n",
      "         8,  1, 58, 51, 62, 52, 72, 55, 68, 61, 62, 71, 64, 57, 64, 55, 64,  1,\n",
      "        43, 51, 57, 55,  0, 35, 65, 63, 63, 70,  1, 55, 68, 69, 70, 55,  1, 36,\n",
      "        59, 55, 52,  1, 71, 64, 54,  1, 30, 68, 55, 71, 64, 54, 69, 53, 58, 51,\n",
      "        56, 70,  1, 63, 59, 70,  1, 58, 55, 68, 51, 71, 56, 23,  0, 28, 55, 68,\n",
      "         1, 43, 53, 58, 63, 55, 68, 76,  1, 73, 59, 68, 54,  1, 64, 55, 71,  8,\n",
      "         1, 55, 69,  1, 73, 59, 55, 54, 55, 68, 58, 65, 62, 70,  1, 54, 59, 55,\n",
      "         1, 35, 62, 51, 57, 55,  0, 28, 55, 69,  1, 36, 55, 52, 55, 64, 69,  1,\n",
      "        62, 51, 52, 75, 68, 59, 64, 70, 58, 59, 69, 53, 58,  1, 59, 68, 68, 55,\n",
      "        64,  1, 36, 51, 71, 56,  8,  0, 45, 64, 54,  1, 64, 55, 64, 64, 70,  1,\n",
      "        54, 59, 55,  1, 31, 71, 70, 55, 64,  8,  1, 54, 59, 55,  8,  1, 71, 63,\n",
      "         1, 69, 53, 58, 82, 64, 55,  1, 43, 70, 71, 64, 54, 55, 64,  0, 46, 65,\n",
      "        63,  1, 31, 62, 83, 53, 61,  1, 57, 55, 70, 81, 71, 69, 53, 58, 70,  8,\n",
      "         1, 72, 65, 68,  1, 63, 59, 68,  1, 58, 59, 64, 73, 55, 57, 57, 55, 69,\n",
      "        53, 58, 73, 71, 64, 54, 55, 64, 10,  0,  0, 43, 59, 55,  1, 58, 82, 68,\n",
      "        55, 64,  1, 64, 59, 53, 58, 70,  1, 54, 59, 55,  1, 56, 65, 62, 57, 55,\n",
      "        64, 54, 55, 64,  1, 31, 55, 69, 81, 64, 57, 55,  8,  0, 28, 59, 55,  1,\n",
      "        43, 55, 55, 62, 55, 64,  8,  1, 54, 55, 64, 55, 64,  1, 59, 53, 58,  1,\n",
      "        54, 59, 55,  1, 55, 68, 69, 70, 55, 64,  1, 69, 51, 64, 57, 23,  0, 50,\n",
      "        55, 68, 69, 70, 65, 52, 55, 64,  1, 59, 69, 70,  1, 54, 51, 69,  1, 56,\n",
      "        68, 55, 71, 64, 54, 62, 59, 53, 58, 55,  1, 31, 55, 54, 68, 81, 64, 57,\n",
      "        55,  8,  0, 46, 55, 68, 61, 62, 71, 64, 57, 55, 64,  8,  1, 51, 53, 58,\n",
      "         2,  1, 54, 55, 68,  1, 55, 68, 69, 70, 55,  1, 47, 59, 54, 55, 68, 61,\n",
      "        62, 51, 64, 57, 10,  0, 37, 55, 59, 64,  1, 36, 59, 55, 54,  1, 55, 68,\n",
      "        70, 82, 64, 70,  1, 54, 55, 68,  1, 71, 64, 52, 55, 61, 51, 64, 64, 70,\n",
      "        55, 64,  1, 37, 55, 64, 57, 55,  8,  0, 33, 58, 68,  1, 26, 55, 59, 56,\n",
      "        51, 62, 62,  1, 69, 55, 62, 52, 69, 70])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Store the encoded text in a torch.tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"Shape of data:\", data.shape)\n",
    "print(\"Dtype of data:\", data.dtype)\n",
    "\n",
    "print(\"First 1000 encoded characters:\", data[:1000])\n",
    "\n",
    "#print(\"First 1000 decoded characters\", decode(data.tolist())[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Making Training Mini-Batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3a) Split the data into 90% training and 10% validation data and store the result in `train_data` and `val_data`, respectively. We keep the validation data to detect overfitting: We don't want just a perfect memorization of this exact input text, we want a neural network that creates new text in a similar style. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training data: 185226\n",
      "Length of the validation data: 20581\n",
      "Check against total length; must be equal 0: (len(data) - len(train_data)-len(val_data)): 0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "train_data = data[:int(len(data)*0.9)]\n",
    "print (\"Length of the training data:\",len(train_data))\n",
    "\n",
    "val_data = data[int(len(data)*0.9):]\n",
    "print (\"Length of the validation data:\",len(val_data))\n",
    "\n",
    "print(\"Check against total length; must be equal 0: (len(data) - len(train_data)-len(val_data)):\", len(data)-len(train_data)-len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only feed in chunks of data of size 8 here: feeding in all text at once is computationally too expensive. This is called the **block size** or **context length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([91, 30, 51, 71, 69, 70, 22,  0, 28])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1] # +1 because the target is the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this `train_data` chunk of 9 characters, 8 training examples are hidden. Let's spell it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([91]) the target is: 30\n",
      "when input is tensor([91, 30]) the target is: 51\n",
      "when input is tensor([91, 30, 51]) the target is: 71\n",
      "when input is tensor([91, 30, 51, 71]) the target is: 69\n",
      "when input is tensor([91, 30, 51, 71, 69]) the target is: 70\n",
      "when input is tensor([91, 30, 51, 71, 69, 70]) the target is: 22\n",
      "when input is tensor([91, 30, 51, 71, 69, 70, 22]) the target is: 0\n",
      "when input is tensor([91, 30, 51, 71, 69, 70, 22,  0]) the target is: 28\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # this will be the input\n",
    "y = train_data[1:block_size+1] # this will be the target\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides efficiency, a second reason to feed in chunks of size `block_size` is to make the Transformer be used to seeing contexts of different lengths, from only 1 token all the way up to `block_size` and every length in between. That is going to be useful later during inference because while we're sampling, we can start the sampling generation with as little as one character of context and the Transformer knows how to predict the next character. Then it can predict everything up to `block_size`. After `block_size`, we have to start truncating because the Transformer will never receive more than block size inputs when it's predicting the next character.\n",
    "\n",
    "Besides the **time dimension** that we have just looked at, there is also the **batch dimension**: We feed in batches of multiple chunks of text that are all stacked up in a single tensor. This is simply done for efficiency, because the GPUs can process these batches in parallel.\n",
    "\n",
    "Now let's create random **batches** of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 4 (=batch_size) random offsets into training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack 4 chunks (4x8 tensor) as rows in a minibatch\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # y is the same but one ahead (shifted 1 position to the right)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3b) Get a batch of training data and store the inputs and targets in `xb` and `yb`, respectively. Print the results and their shapes. **(2 points)** \n",
    "\n",
    "**HINT:** Apply the `get_batch()` function above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (xb):\n",
      "tensor([[55,  1, 52, 62, 83, 58, 70, 23],\n",
      "        [73, 59, 55, 54, 55, 68, 61, 55],\n",
      "        [56, 51, 62, 70,  8,  1, 54, 51],\n",
      "        [37, 25, 42, 44, 32, 29, 10,  0]])\n",
      "Shape of xb: torch.Size([4, 8])\n",
      "\n",
      "Target (yb):\n",
      "tensor([[ 1, 52, 62, 83, 58, 70, 23,  0],\n",
      "        [59, 55, 54, 55, 68, 61, 55, 58],\n",
      "        [51, 62, 70,  8,  1, 54, 51, 80],\n",
      "        [25, 42, 44, 32, 29, 10,  0, 32]])\n",
      "Shape of yb: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Get a batch of training data\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(\"Input (xb):\")\n",
    "print(xb)\n",
    "print(\"Shape of xb:\", xb.shape)\n",
    "\n",
    "print(\"\\nTarget (yb):\")\n",
    "print(yb)\n",
    "print(\"Shape of yb:\", yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3c) How many independent training examples for the transformer does this batch contain? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**ANSWER:** \n",
    "\n",
    "Each batch contains `batch_size` independent sequences, and each sequence has `block_size` positions. Therefore, the total number of independent training examples in the batch is `batch_size * block_size` which is `4 * 8 = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [55] the target is: 1\n",
      "when input is [55, 1] the target is: 52\n",
      "when input is [55, 1, 52] the target is: 62\n",
      "when input is [55, 1, 52, 62] the target is: 83\n",
      "when input is [55, 1, 52, 62, 83] the target is: 58\n",
      "when input is [55, 1, 52, 62, 83, 58] the target is: 70\n",
      "when input is [55, 1, 52, 62, 83, 58, 70] the target is: 23\n",
      "when input is [55, 1, 52, 62, 83, 58, 70, 23] the target is: 0\n",
      "when input is [73] the target is: 59\n",
      "when input is [73, 59] the target is: 55\n",
      "when input is [73, 59, 55] the target is: 54\n",
      "when input is [73, 59, 55, 54] the target is: 55\n",
      "when input is [73, 59, 55, 54, 55] the target is: 68\n",
      "when input is [73, 59, 55, 54, 55, 68] the target is: 61\n",
      "when input is [73, 59, 55, 54, 55, 68, 61] the target is: 55\n",
      "when input is [73, 59, 55, 54, 55, 68, 61, 55] the target is: 58\n",
      "when input is [56] the target is: 51\n",
      "when input is [56, 51] the target is: 62\n",
      "when input is [56, 51, 62] the target is: 70\n",
      "when input is [56, 51, 62, 70] the target is: 8\n",
      "when input is [56, 51, 62, 70, 8] the target is: 1\n",
      "when input is [56, 51, 62, 70, 8, 1] the target is: 54\n",
      "when input is [56, 51, 62, 70, 8, 1, 54] the target is: 51\n",
      "when input is [56, 51, 62, 70, 8, 1, 54, 51] the target is: 80\n",
      "when input is [37] the target is: 25\n",
      "when input is [37, 25] the target is: 42\n",
      "when input is [37, 25, 42] the target is: 44\n",
      "when input is [37, 25, 42, 44] the target is: 32\n",
      "when input is [37, 25, 42, 44, 32] the target is: 29\n",
      "when input is [37, 25, 42, 44, 32, 29] the target is: 10\n",
      "when input is [37, 25, 42, 44, 32, 29, 10] the target is: 0\n",
      "when input is [37, 25, 42, 44, 32, 29, 10, 0] the target is: 32\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3d) Why do the targets look like this, where does the structure come from? What do we input to the transformer? **(2 points)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "The input to the transformer is the input context `xb`. The transformer uses this context to predict the next character in the sequence, which is the target `yb`. During training, the model learns to predict the next character based on the given context `xb`.\n",
    "\n",
    "For each position in the sequence, the input context is a slice of the sequence up to that position. For example, if the sequence is `[64, 1, 69, 59, 55, 52, 55, 64]`, the input contexts for each position is:\n",
    "\n",
    "`[64]`<br>\n",
    "`[64, 1]`<br>\n",
    "`[64, 1, 69]`<br>\n",
    "`[64, 1, 69, 59]`<br>\n",
    "`[64, 1, 69, 59, 55]`<br>\n",
    "`[64, 1, 69, 59, 55, 52]`<br>\n",
    "`[64, 1, 69, 59, 55, 52, 55]`<br>\n",
    "`[64, 1, 69, 59, 55, 52, 55, 64]`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Defining the Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple bigram language model to start with, i.e., the model predicts the next character simply on the last character. This bigram model should look familiar from our first notebook! Only now, we implement a bigram model class inheriting from `nn.Module` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 92])\n",
      "loss= tensor(4.8776, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated text: \n",
      "\n",
      "Y:1ufb,”:\n",
      "CtjE8‘ÄGm5rOwB.öq%—d.INKvläöSu’$—Ö﻿eEXo1A’M*QPEp” xNSHF!b“)“q—tf™FÄÖ3TmA-RmM—Eß6pßaL﻿wÖ‘pa\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module): # subclass of nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # e.g. if the input is token 5, the output should be the logits for all tokens at position 6 \n",
    "        # = the 5th row of the embedding table (see makemore video on bigram language model)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None): # targets are optional during inference\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # pluck out the embeddings for the tokens in the input (=the row of the embedding table corresponding to its index) and interpret them as logits=scores\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) batch size=4, time=8, channels=vocab_size because we are predicting the probability of each token (vocab_size C) at each time step (block_size T) in each batch (batch_size B)\n",
    "\n",
    "        # if we have targets, compute the CE loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # need to reshape for CE-loss in PyTorch \n",
    "            # (see https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "            targets = targets.view(B*T) # same shape as logits\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions (ignore the loss because we don't have targets)\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step = prediction for the next token\n",
    "            logits = logits[:, -1, :] # becomes (B, C) instead of (B, T, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) because we sample one token at a time for each batch\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print('loss=', loss) \n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # start with a single token = 0 (idx = current context)\n",
    "\n",
    "print(\"\\nGenerated text: \")\n",
    "# generate operates on batch level -> index into the 0th row = single batch dimension that exists -> one-dimensional array of all the indices (time steps)\n",
    "# afterwards convert to simple python list from tensor for decode function\n",
    "print(decode(model.generate(idx, max_new_tokens=100)[0].tolist())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4a) Go through the class definition above and explain what each function does! (1-2 sentences per function) **(6 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "__init__:\n",
    "Initializes the BigramLanguageModel by creating an embedding table of size (`vocab_size`, `vocab_size`) that directly maps each input token to a set of logits predicting the next token’s probabilities.\n",
    "\n",
    "forward:\n",
    "Takes a batch of token indices `idx` and, optionally, their corresponding `targets`. It uses the embedding table to produce logits (scores) for the next token at each position and, if `targets` are provided, computes and returns the cross-entropy loss between the predicted logits and the targets.\n",
    "\n",
    "generate:\n",
    "Given a starting sequence `idx`, repeatedly predicts the next token’s probabilities using the model, samples one token from that distribution, and appends it to the sequence. This process continues `max_new_tokens` times, effectively generating new text autoregressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4b) How do you interpret the generated text? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "The generated text appears random. This is due to the following reasons:\n",
    "- it's a simple BLM\n",
    "- During generation, the model uses a probabilistic sampling method `(torch.multinomial)`, which introduces randomness. Without strong contextual or linguistic constraints, this randomness leads to nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4c) What loss do you expect for this model? Can you compare the actual loss with your expectation? **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "With a `vocab_size` of `92` we expect the following cross-entropy loss for an untrained BLM:\n",
    "\n",
    "`expected_loss = -log(1/vocab_size) = log(vocab_size) = log(92) = 4,522`\n",
    "\n",
    "The reported loss is slightly higher than the theoretical `4,522`. This might be due to the random initialization of the embedding table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that up until now, the text history is not used, it is a simple bigram model (only the last character is used to predict the next one). Still, we feed in the whole sequence `xb`, `yb` up to `block_size` for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5a) Create a PyTorch Adam optimizer with a learning rate of `1e-3`, pass it the model parameters for optimization (`model.parameters()`) and store it in `optimizer`. Check the documentation if needed! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the training loop now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=5.116175174713135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=100, loss=4.827666759490967\n",
      "step=200, loss=4.703289985656738\n",
      "step=300, loss=4.792712211608887\n",
      "step=400, loss=4.5202717781066895\n",
      "step=500, loss=4.437468528747559\n",
      "step=600, loss=4.442217826843262\n",
      "step=700, loss=4.289590358734131\n",
      "step=800, loss=4.111300468444824\n",
      "step=900, loss=4.063968658447266\n",
      "step=1000, loss=3.8623154163360596\n",
      "step=1100, loss=3.8420138359069824\n",
      "step=1200, loss=3.837150812149048\n",
      "step=1300, loss=3.6671175956726074\n",
      "step=1400, loss=3.575347661972046\n",
      "step=1500, loss=3.5930984020233154\n",
      "step=1600, loss=3.56148099899292\n",
      "step=1700, loss=3.355199098587036\n",
      "step=1800, loss=3.290208101272583\n",
      "step=1900, loss=3.3211188316345215\n",
      "step=2000, loss=3.161468267440796\n",
      "step=2100, loss=3.273216724395752\n",
      "step=2200, loss=3.1431286334991455\n",
      "step=2300, loss=3.0248074531555176\n",
      "step=2400, loss=2.9094643592834473\n",
      "step=2500, loss=2.9856903553009033\n",
      "step=2600, loss=3.014256715774536\n",
      "step=2700, loss=3.0027623176574707\n",
      "step=2800, loss=2.9097800254821777\n",
      "step=2900, loss=2.8159520626068115\n",
      "step=3000, loss=2.7627511024475098\n",
      "step=3100, loss=2.7356598377227783\n",
      "step=3200, loss=2.886899709701538\n",
      "step=3300, loss=2.796566963195801\n",
      "step=3400, loss=2.719935655593872\n",
      "step=3500, loss=2.767118453979492\n",
      "step=3600, loss=2.782073974609375\n",
      "step=3700, loss=2.607400417327881\n",
      "step=3800, loss=2.6644721031188965\n",
      "step=3900, loss=2.7180097103118896\n",
      "step=4000, loss=2.670649766921997\n",
      "step=4100, loss=2.4364335536956787\n",
      "step=4200, loss=2.482208490371704\n",
      "step=4300, loss=2.504244089126587\n",
      "step=4400, loss=2.6091794967651367\n",
      "step=4500, loss=2.548475742340088\n",
      "step=4600, loss=2.5073165893554688\n",
      "step=4700, loss=2.410452127456665\n",
      "step=4800, loss=2.5167486667633057\n",
      "step=4900, loss=2.466028928756714\n",
      "step=5000, loss=2.4987387657165527\n",
      "step=5100, loss=2.5332841873168945\n",
      "step=5200, loss=2.4900317192077637\n",
      "step=5300, loss=2.469388008117676\n",
      "step=5400, loss=2.43495774269104\n",
      "step=5500, loss=2.556631565093994\n",
      "step=5600, loss=2.456815242767334\n",
      "step=5700, loss=2.445634126663208\n",
      "step=5800, loss=2.347567081451416\n",
      "step=5900, loss=2.389230966567993\n",
      "step=6000, loss=2.494581699371338\n",
      "step=6100, loss=2.4332807064056396\n",
      "step=6200, loss=2.4178733825683594\n",
      "step=6300, loss=2.4255805015563965\n",
      "step=6400, loss=2.4397945404052734\n",
      "step=6500, loss=2.5028975009918213\n",
      "step=6600, loss=2.3273189067840576\n",
      "step=6700, loss=2.5676119327545166\n",
      "step=6800, loss=2.4157774448394775\n",
      "step=6900, loss=2.458317995071411\n",
      "step=7000, loss=2.4722306728363037\n",
      "step=7100, loss=2.36042857170105\n",
      "step=7200, loss=2.3780198097229004\n",
      "step=7300, loss=2.414492130279541\n",
      "step=7400, loss=2.4381208419799805\n",
      "step=7500, loss=2.4004933834075928\n",
      "step=7600, loss=2.389051914215088\n",
      "step=7700, loss=2.384294271469116\n",
      "step=7800, loss=2.334012985229492\n",
      "step=7900, loss=2.345813274383545\n",
      "step=8000, loss=2.328324556350708\n",
      "step=8100, loss=2.3709969520568848\n",
      "step=8200, loss=2.5139517784118652\n",
      "step=8300, loss=2.584735631942749\n",
      "step=8400, loss=2.328157901763916\n",
      "step=8500, loss=2.339715003967285\n",
      "step=8600, loss=2.4137516021728516\n",
      "step=8700, loss=2.284053325653076\n",
      "step=8800, loss=2.371702194213867\n",
      "step=8900, loss=2.441977024078369\n",
      "step=9000, loss=2.303317070007324\n",
      "step=9100, loss=2.408406972885132\n",
      "step=9200, loss=2.409428119659424\n",
      "step=9300, loss=2.420987606048584\n",
      "step=9400, loss=2.355881452560425\n",
      "step=9500, loss=2.495178461074829\n",
      "step=9600, loss=2.256054401397705\n",
      "step=9700, loss=2.3387274742126465\n",
      "step=9800, loss=2.4053375720977783\n",
      "step=9900, loss=2.5225958824157715\n",
      "2.410107135772705\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increase batch size for better results\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # logits are not needed here\n",
    "    optimizer.zero_grad(set_to_none=True) # reset the gradients\n",
    "    loss.backward() # compute the gradients\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "    # print the loss every 100 steps\n",
    "    if steps % 100 == 0:\n",
    "        print(f'step={steps}, loss={loss.item()}')\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate new text based on the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "uspf I, ber deis lnt\n",
      "Am FAlast Erdeinitend det Übie.\n",
      "Man Glt\n",
      "Zanabebeht hr besaunq- NEr hleit wo﻿%xchwan eurid’sttt,\n",
      "ME.\n",
      "MAchrdr.\n",
      "(IEsckolügech GAUSanerkundiei, Pht n.\n",
      "Far wen FAUn Kergen vellem MAlor’g ur n, S.\n",
      "(xGleich Oy,\n",
      "Wichoheinufft ven Deihl omölel Sie PHer Trtf! STjüchand GEPHöngeheickll h m zur et ht aun d e t n, gelt!\n",
      "Dicht.\n",
      "(STOPHEScken wer hresen dun gauch ü0f STOPHend,\n",
      "De d he Soger hörseldese STOPHÜ9rfandenz binihrberdeinitchen womm g? Aben bink,\n",
      "Ih dumin.\n",
      "Geht?\n",
      "Sorer debelanier, d\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text: \")\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5b) How do you interpret the result? What could be a reason that the output is still suboptimal? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: \n",
    "\n",
    "The output is mostly nonsensical, with random characters, words, and symbols jumbled together. While it might contain some recognizable characters and occasionally form something that looks like a word, it doesn't produce coherent sentences or meaningful text. This behavior is expected from a simple bigram model.\n",
    "\n",
    "Reason for Suboptimal Output:\n",
    "\n",
    "1. **Bigram Model Limitation:**\n",
    "The model uses only the previous single token to predict the next. This extremely limited context makes it impossible for the model to capture longer-range linguistic structure such as grammar, semantics, or even consistent word formation beyond two-character sequences.\n",
    "\n",
    "2. **Insufficient Model Complexity and Training:**\n",
    "Even if trained extensively, a bigram model cannot learn complex language patterns. More sophisticated models—e.g., using multi-layer transformers that consider a longer sequence context—are needed to produce coherent and meaningful text.\n",
    "In summary, the suboptimal output is not a sign of poor training but a fundamental limitation of the simple bigram modeling approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarized code so far (with some additions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n",
      "step 0: train loss 4.9232, val loss 5.0076\n",
      "step 300: train loss 2.8302, val loss 3.6603\n",
      "step 600: train loss 2.4870, val loss 3.5628\n",
      "step 900: train loss 2.4054, val loss 3.5805\n",
      "step 1200: train loss 2.3905, val loss 3.6182\n",
      "step 1500: train loss 2.3761, val loss 3.6328\n",
      "step 1800: train loss 2.3728, val loss 3.6460\n",
      "step 2100: train loss 2.3749, val loss 3.6329\n",
      "step 2400: train loss 2.3661, val loss 3.6580\n",
      "step 2700: train loss 2.3631, val loss 3.6954\n",
      "\n",
      "Ichät, d MArerndendige St\n",
      "Flin;\n",
      "Kör Blt,\n",
      "\n",
      "MEYEnzerumicht,\n",
      "\n",
      "USorst der Schaf sich juechmeunfr’st win dit?\n",
      "Wein drn, san.\n",
      "E er ol Hurn, Dar igesoleust zeVör werisen.\n",
      "Den Geich mar dan Zurgegen!\n",
      "\n",
      "(Minnuaß™, d\n",
      "FAtongichärerer sieif en,\n",
      "ESTr ige?\n",
      "Fldet ST.\n",
      "Un mito ser vichrnbzechlt weim deit!\n",
      "Zustveien\n",
      "Mecht ffübe\n",
      "\n",
      "Distwi\n",
      "\n",
      "STOPHau!\n",
      "Deichtendes uf.\n",
      "NDagascher n Juen,\n",
      "ARETOPHat ETer Weieses, STOPHISTOPHoch lidend!\n",
      "\n",
      "Mun, uffllas LLeheichört Tenid Eg lerinen;\n",
      "MERGSoch NEr mieneinen m;\n",
      "Das (fen,\n",
      "Dir woche\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'mps'\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # new: we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # new: average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # new: switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # new: switch back to train mode\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "---\n",
    "## 6. The Mathematical Trick in Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now derive a more complex model that can look at all tokens at once to predict the next one, not just the last token. \n",
    "To use all previous tokens, the simplest idea is to use an average of all previous tokens. \n",
    "For example, the 5th token uses the **channels** (=feature maps, embeddings) of the 1st, 2nd, 3rd, 4th, and 5th token. \n",
    "The average of these is the **feature vector** for the 5th token and summarizes the context / history.\n",
    "Note that we have lost a lot of information, e.g. the order of the tokens, but it's a starting point. Consider the following toy example with batch size 4 , 8 tokens, 2 channels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2 # batch, time, channels. Goal: 8 tokens should talk to each other, but only from previous tokens, not from future tokens\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in each batch in the example vector `x`, we calculate the mean of the tokens that came before it in the time dimension (including itself). \n",
    "The result should be a tensor of shape (B,T,C) where the t-th row of the b-th batch contains the mean of all tokens in this batch that came before this token in the time dimension.\n",
    "We print the original tensor `x` and the resulting tensor `xbow` containing the mean values and make sure the mean values are correct. Here `bow` stands for **bag of words**, which means that each entry is an average of several words (each of the 8 tokens is considered a 'word' here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow = bag of words = simple average of all previous tokens\n",
    "for b in range(B): # iterate over batch dimension\n",
    "    for t in range(T): # iterate over time dimension\n",
    "        xprev = x[b,:t+1] # (t,C) # all previous tokens for this batch and time (slice)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # mean over time dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0430,  0.7628],\n",
       "        [-1.0536,  0.1370],\n",
       "        [-0.2743,  1.5676],\n",
       "        [ 0.3086, -0.4727],\n",
       "        [ 0.2641,  0.5549],\n",
       "        [ 1.3514,  0.2295],\n",
       "        [-0.2290,  0.0087],\n",
       "        [-0.1615,  0.5542]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # 0th batch element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0430,  0.7628],\n",
       "        [-0.5483,  0.4499],\n",
       "        [-0.4570,  0.8225],\n",
       "        [-0.2656,  0.4987],\n",
       "        [-0.1596,  0.5099],\n",
       "        [ 0.0922,  0.4632],\n",
       "        [ 0.0463,  0.3983],\n",
       "        [ 0.0203,  0.4178]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] # vertical average of all previous tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using several nested loops like above, we use a trick with matrix multiplication that is mathematically equivalent but more efficient. Here is a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3) \n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, c contains the sum of the column entries of b. Because we only want the \"history\", not the \"future\" tokens to influence the result, we use an upper triangular matrix `a` instead, this is called **masking**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows, \n",
    "# third row is sum of all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to normalize for averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "a = a / torch.sum(a, 1, keepdim=True) # normalize rows to sum to 1\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows + normalized, \n",
    "# third row is sum of all rows + normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6a) Now let's go back to our example above and apply the same trick. \n",
    "Define a lower triangular matrix called `wei` (previously `a`) that is normalized to sum to 1 along the rows. Matrix multiply `wei` with `x` to get a new matrix `xbow2`.\n",
    "Make sure that `xbow2` has the same shape as `x` and that it contains the correct values. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix x:\n",
      "tensor([[[-0.0430,  0.7628],\n",
      "         [-1.0536,  0.1370],\n",
      "         [-0.2743,  1.5676],\n",
      "         [ 0.3086, -0.4727],\n",
      "         [ 0.2641,  0.5549],\n",
      "         [ 1.3514,  0.2295],\n",
      "         [-0.2290,  0.0087],\n",
      "         [-0.1615,  0.5542]],\n",
      "\n",
      "        [[ 0.6630, -0.1377],\n",
      "         [-0.5458, -0.7306],\n",
      "         [-0.4159,  1.1934],\n",
      "         [ 0.1754,  0.1962],\n",
      "         [-0.5227,  0.9560],\n",
      "         [ 0.6879,  0.0692],\n",
      "         [-0.1940,  0.9279],\n",
      "         [-0.4659,  0.2719]],\n",
      "\n",
      "        [[ 0.6551, -0.4154],\n",
      "         [ 0.6817, -0.4051],\n",
      "         [-0.4009,  0.9840],\n",
      "         [-0.0280, -0.0881],\n",
      "         [ 0.6077, -1.0126],\n",
      "         [ 0.2341,  1.1571],\n",
      "         [ 1.1633,  1.0771],\n",
      "         [-0.2268, -0.9550]],\n",
      "\n",
      "        [[ 0.3904, -0.0266],\n",
      "         [ 0.8186,  0.3478],\n",
      "         [ 2.1756, -0.1433],\n",
      "         [-0.7308,  1.1156],\n",
      "         [-0.8629, -0.0287],\n",
      "         [-0.8543, -0.5502],\n",
      "         [ 0.4755, -0.4117],\n",
      "         [ 0.5877, -0.5599]]])\n",
      "\n",
      "Lower triangular weight matrix wei:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n",
      "Computed xbow2 (Bag of Words):\n",
      "tensor([[[-0.0430,  0.7628],\n",
      "         [-0.5483,  0.4499],\n",
      "         [-0.4570,  0.8225],\n",
      "         [-0.2656,  0.4987],\n",
      "         [-0.1596,  0.5099],\n",
      "         [ 0.0922,  0.4632],\n",
      "         [ 0.0463,  0.3983],\n",
      "         [ 0.0203,  0.4178]],\n",
      "\n",
      "        [[ 0.6630, -0.1377],\n",
      "         [ 0.0586, -0.4341],\n",
      "         [-0.0995,  0.1084],\n",
      "         [-0.0308,  0.1303],\n",
      "         [-0.1292,  0.2955],\n",
      "         [ 0.0070,  0.2578],\n",
      "         [-0.0217,  0.3535],\n",
      "         [-0.0772,  0.3433]],\n",
      "\n",
      "        [[ 0.6551, -0.4154],\n",
      "         [ 0.6684, -0.4102],\n",
      "         [ 0.3120,  0.0545],\n",
      "         [ 0.2270,  0.0188],\n",
      "         [ 0.3031, -0.1874],\n",
      "         [ 0.2916,  0.0367],\n",
      "         [ 0.4161,  0.1853],\n",
      "         [ 0.3358,  0.0428]],\n",
      "\n",
      "        [[ 0.3904, -0.0266],\n",
      "         [ 0.6045,  0.1606],\n",
      "         [ 1.1282,  0.0593],\n",
      "         [ 0.6635,  0.3234],\n",
      "         [ 0.3582,  0.2530],\n",
      "         [ 0.1561,  0.1191],\n",
      "         [ 0.2017,  0.0433],\n",
      "         [ 0.2500, -0.0321]]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Define the lower triangular weight matrix\n",
    "wei = torch.tril(torch.ones(T, T))  # Lower triangular matrix\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)  # Normalize rows to sum to 1  # what is the diff to: wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "\n",
    "# Matrix multiply wei with x\n",
    "xbow2 = wei @ x  # Shape will be (B, T, C) after broadcasting\n",
    "\n",
    "print(\"Original matrix x:\")\n",
    "print(x)\n",
    "print(\"\\nLower triangular weight matrix wei:\")\n",
    "print(wei)\n",
    "print(\"\\nComputed xbow2 (Bag of Words):\")\n",
    "print(xbow2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0430,  0.7628],\n",
       "         [-0.5483,  0.4499],\n",
       "         [-0.4570,  0.8225],\n",
       "         [-0.2656,  0.4987],\n",
       "         [-0.1596,  0.5099],\n",
       "         [ 0.0922,  0.4632],\n",
       "         [ 0.0463,  0.3983],\n",
       "         [ 0.0203,  0.4178]]),\n",
       " tensor([[-0.0430,  0.7628],\n",
       "         [-0.5483,  0.4499],\n",
       "         [-0.4570,  0.8225],\n",
       "         [-0.2656,  0.4987],\n",
       "         [-0.1596,  0.5099],\n",
       "         [ 0.0922,  0.4632],\n",
       "         [ 0.0463,  0.3983],\n",
       "         [ 0.0203,  0.4178]]))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "xbow[0], xbow2[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6b) Now we use yet another mathematically equivalent way to compute the bag of words representation using **Softmax** function (this will be needed later for weighted sum instead of average of previous tokens).\n",
    "We start off with a lower triangular matrix where the lower triangle and diagonal is filled with 0, the upper with `-inf`. \n",
    "After applying the softmax function, the result will be again the `wei` matrix from before. Implement this in the following cell, calculate again the matrix multiplication of the new `wei` and `x` and check the result! **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask M:\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Weighing matrix after softmax (wei2):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n",
      "Resulting bag-of-words (xbow3):\n",
      "tensor([[[-0.0430,  0.7628],\n",
      "         [-0.5483,  0.4499],\n",
      "         [-0.4570,  0.8225],\n",
      "         [-0.2656,  0.4987],\n",
      "         [-0.1596,  0.5099],\n",
      "         [ 0.0922,  0.4632],\n",
      "         [ 0.0463,  0.3983],\n",
      "         [ 0.0203,  0.4178]],\n",
      "\n",
      "        [[ 0.6630, -0.1377],\n",
      "         [ 0.0586, -0.4341],\n",
      "         [-0.0995,  0.1084],\n",
      "         [-0.0308,  0.1303],\n",
      "         [-0.1292,  0.2955],\n",
      "         [ 0.0070,  0.2578],\n",
      "         [-0.0217,  0.3535],\n",
      "         [-0.0772,  0.3433]],\n",
      "\n",
      "        [[ 0.6551, -0.4154],\n",
      "         [ 0.6684, -0.4102],\n",
      "         [ 0.3120,  0.0545],\n",
      "         [ 0.2270,  0.0188],\n",
      "         [ 0.3031, -0.1874],\n",
      "         [ 0.2916,  0.0367],\n",
      "         [ 0.4161,  0.1853],\n",
      "         [ 0.3358,  0.0428]],\n",
      "\n",
      "        [[ 0.3904, -0.0266],\n",
      "         [ 0.6045,  0.1606],\n",
      "         [ 1.1282,  0.0593],\n",
      "         [ 0.6635,  0.3234],\n",
      "         [ 0.3582,  0.2530],\n",
      "         [ 0.1561,  0.1191],\n",
      "         [ 0.2017,  0.0433],\n",
      "         [ 0.2500, -0.0321]]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# We start with a mask that has zeros for positions in the lower triangle (including diagonal)\n",
    "# and -inf in the upper triangle\n",
    "M = torch.zeros((T, T))\n",
    "M[torch.tril(torch.ones(T, T)) == 0] = float('-inf')  # upper triangle to -inf\n",
    "\n",
    "# Apply softmax along the time dimension (dim=1)\n",
    "wei = torch.softmax(M, dim=1)\n",
    "\n",
    "# Multiply wei2 by x to get the bag of words representation\n",
    "xbow3 = wei @ x\n",
    "\n",
    "# Print matrices to verify\n",
    "print(\"Mask M:\")\n",
    "print(M)\n",
    "print(\"\\nWeighing matrix after softmax (wei2):\")\n",
    "print(wei)\n",
    "print(\"\\nResulting bag-of-words (xbow3):\")\n",
    "print(xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0430,  0.7628],\n",
       "         [-0.5483,  0.4499],\n",
       "         [-0.4570,  0.8225],\n",
       "         [-0.2656,  0.4987],\n",
       "         [-0.1596,  0.5099],\n",
       "         [ 0.0922,  0.4632],\n",
       "         [ 0.0463,  0.3983],\n",
       "         [ 0.0203,  0.4178]]),\n",
       " tensor([[-0.0430,  0.7628],\n",
       "         [-0.5483,  0.4499],\n",
       "         [-0.4570,  0.8225],\n",
       "         [-0.2656,  0.4987],\n",
       "         [-0.1596,  0.5099],\n",
       "         [ 0.0922,  0.4632],\n",
       "         [ 0.0463,  0.3983],\n",
       "         [ 0.0203,  0.4178]]),\n",
       " tensor([[-0.0430,  0.7628],\n",
       "         [-0.5483,  0.4499],\n",
       "         [-0.4570,  0.8225],\n",
       "         [-0.2656,  0.4987],\n",
       "         [-0.1596,  0.5099],\n",
       "         [ 0.0922,  0.4632],\n",
       "         [ 0.0463,  0.3983],\n",
       "         [ 0.0203,  0.4178]]))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0], xbow3[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Self-Attention\n",
    "\n",
    "Finally we get to the most important mechanism: **Self-Attention**! This will lead to a weighted average of the tokens (some tokens are more important than others to understand the text) instead of simply using the mean. And here is the idea: Every single token will emit two vectors: A **query** (\"What am I looking for?\") and a **key** (\"What do I contain?\"). The query then dot-products with all the keys to determine the similarity = affinity (stored in `wei`). Instead of the raw input `x`, which is private, a **value** is used (\"What will I communicate?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels (increase channels for more interesting results)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16) # forward pass of x through the key layer\n",
    "q = query(x) # (B, T, 16) # forward pass of x through the query layer\n",
    "# so far, each token has a key and a query vector, no communication yet\n",
    "wei =  q @ k.transpose(-2, -1) # transpose last 2 dimensions (batch remains unchanged): (B, T, 16) @ (B, 16, T) ---> (B, T, T): for each batch, each token talks to all other tokens, so we get an affinity matrix of size (T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T)) # old version -> change to data dependent weights\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # comment to see intermediate results before normalization\n",
    "\n",
    "v = value(x) # we use the aggregated value instead of the raw x \n",
    "# x is private information to this token, v is the public information for communication\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7a) Print `wei` and compare it to the previous values. What is the most important change and why is this important here? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "Previously, the weight matrices (like the lower-triangular normalized matrix) were fixed and position-based. They only reflected the sequence order and provided uniform or position-only weighting to past tokens.\n",
    "\n",
    "Now, wei is data-dependent. It is computed from the dot products of queries and keys derived from the input x. This means that different tokens can have different attention weights based on their content rather than just their position. This dynamic weighting is the core idea behind self-attention: it lets the model learn which past tokens are most relevant for predicting the next token, leading to more context-aware and meaningful aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the first weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the final entry 0.2391 is the weight for the 8th token. The 8th token emits a query ( for example \"I am a vowel at position 8, I am looking for consonants at positions up to 4\"). All tokens then emit keys, and maybe a consonant at position 4 will emit a key with high number in this channel, meaning \"I am a consonant at position 4\". The 8th token will therefore have a high weight for the 4th token (0.2297), resulting in a high affinity (dot product) - the 4th and 8th token \"have found each other\". Through the softmax function, a lot of information from the 4th token will be passed to the 8th token (meaning the 8th token will learn a lot from the 4th)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "### Some Notes on Attention\n",
    "- Attention is a **communication mechanism**. It can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. Here we have `block_size = 8` nodes, where the first node is only pointed to by itself, the second by the first and itself, and so on. Attention can be applied to any directed graph, not only language modeling.\n",
    "- Each example across batch dimension is processed completely independently, the examples never \"talk\" to each other across different batches. The batched matrix multiplication above means applying matrix multiplication in parallel in each batch separately. For example here, you can think of 4 different graphs in parallel with 8 noded each, where the 8 nodes only communicate among each other, even though we process 32 nodes at once.\n",
    "- \"Scaled\" attention also divides `wei` by `1/sqrt(head_size)`, in the original paper:\n",
    "\\begin{equation*}\n",
    "   Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation*}\n",
    "This makes it so when input Q,K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much. Without the normalization, using Gaussian input (zero mean and variance 1), the weights will be in the order of `head_size`. Illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size) # k initialized from standard normal distribution (zero mean, unit variance)\n",
    "q = torch.randn(B,T,head_size) # q initialized from standard normal distribution (zero mean, unit variance)\n",
    "wei_unnormalized = q @ k.transpose(-2, -1) # will have variance of head_size roughly\n",
    "wei_normalized = q @ k.transpose(-2, -1)* head_size**-0.5 # normalize by sqrt of head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var() # variance of k: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var() # variance of q: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.4690)\n",
      "tensor(1.0918)\n"
     ]
    }
   ],
   "source": [
    "print(wei_unnormalized.var()) # variance of the dot product: roughly head_size=16\n",
    "print(wei_normalized.var()) # variance of the dot product: roughly 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7b) Find out why this is important: Apply softmax to a tensor with entries around 0, then to another tensor with more extreme values. What happens? Write in the answer cell why we want to avoid this. **(2 points)**\n",
    "\n",
    "**HINT:** `torch.softmax()` expects an input specifying along which dimension to calculate the normalization (=which dimension should sum to 1), so you can pass `dim=-1` as second input for a 1D tensor. (See https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values around zero: tensor([ 0.1000, -0.0500,  0.0000])\n",
      "Softmax result: tensor([0.3616, 0.3112, 0.3272])\n",
      "Sum of probabilities: 1.0 \n",
      "\n",
      "Large values: tensor([50., 60., 55.])\n",
      "Softmax result: tensor([4.5094e-05, 9.9326e-01, 6.6925e-03])\n",
      "Sum of probabilities: 1.0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Example 1: Values around zero\n",
    "values_small = torch.tensor([0.1, -0.05, 0.0], dtype=torch.float32)\n",
    "prob_small = F.softmax(values_small, dim=-1)\n",
    "print(\"Values around zero:\", values_small)\n",
    "print(\"Softmax result:\", prob_small)\n",
    "print(\"Sum of probabilities:\", prob_small.sum().item(), \"\\n\")\n",
    "\n",
    "# Example 2: Larger values\n",
    "values_large = torch.tensor([50.0, 60.0, 55.0], dtype=torch.float32)\n",
    "prob_large = F.softmax(values_large, dim=-1)\n",
    "print(\"Large values:\", values_large)\n",
    "print(\"Softmax result:\", prob_large)\n",
    "print(\"Sum of probabilities:\", prob_large.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "If we apply `softmax()` to values around zero (e.g., `[0.1, -0.05, 0.0]`), we get a relatively balanced distribution of probabilities. However, if we apply `softmax()` to very large values (e.g., `[50, 60, 55]`), the exponentiation inside `softmax()` makes the largest value dominate completely, leading to a probability distribution that is almost one-hot. This extreme skew can cause numerical instability, exploding gradients, and poor training dynamics.\n",
    "\n",
    "By normalizing the dot-products by `1/√head_size`, we keep the attention scores around a more moderate scale, so that the `softmax()` function doesn't collapse to near one-hot distributions prematurely. This helps stabilize training and ensures that the model can properly learn to distribute attention among multiple relevant tokens, rather than always focusing on the single largest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Encoding and Positional Encoding\n",
    "\n",
    "We will make one change on the token encoding: Previously, the `token_embedding_table` was of size `(vocab_size, vocab_size)`, which means we directly plucked out the logits from the embedding table. Now we want to introduce an intermediate layer (make the net bigger). Therefore, we introduce a new parameter `n_embd` for the number of embedding dimensions, for example we can choose 32 or 64 for this intermediate representation. So instead of logits, the `token_embedding_table` will give us **token embeddings**. These will be fed to a linear layer afterwards to get the logits:\n",
    "```\n",
    "self.lm_head = nn.Linear(n_embd, vocab_size) # linear layer to decode into the vocabulary   \n",
    "```\n",
    "\n",
    "In the attention mechanism derived so far, there is no notion of space. Attention simply acts over a set of vectors. Remember that we can think of attention as a directed graph, where the nodes have no idea where they are positioned in a space. But space matters in text: For example, \"people love animals\" has a significantly different meaning than \"animals love people\", so the ordering of the words is very important. This is why we need to **positionally encode** tokens:\n",
    "So far, we have only encoded each token according to its identity `idx`. But we now also encode its position in a second embedding table: Each position from `0` to `block_size-1` will get its own embedding vector. This is the code snippet from the init function that we will implement below: \n",
    "```\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # token embedding according to identity (e.g., first character in vocabulary)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # positional encoding according to position in text (e.g., first character in text)\n",
    "```\n",
    "And here is a code snippet from the forward function, showing how integers from 0 to `block_size` are positionally encoded: \n",
    "```\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - integers from 0 to T-1\n",
    "        x = tok_emb + pos_emb # (B,T,C) via broadcasting (pos_emb gets right-aligned, new dimension of 1 gets added, broadcasted across batch)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "```\n",
    "Right now, this is not useful yet, because we only use the last token in the Bigram model, so the position does not matter. But using attention, it will matter!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Single Self-Attention Head\n",
    "\n",
    "Now let's summarize the code so far and add a single self-attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5645, val loss 4.5525\n",
      "step 500: train loss 2.5698, val loss 3.5501\n",
      "step 1000: train loss 2.4292, val loss 3.7169\n",
      "step 1500: train loss 2.3676, val loss 3.7612\n",
      "step 2000: train loss 2.3415, val loss 3.8056\n",
      "step 2500: train loss 2.3190, val loss 3.8577\n",
      "step 3000: train loss 2.2995, val loss 3.9520\n",
      "step 3500: train loss 2.2939, val loss 3.9182\n",
      "step 4000: train loss 2.2832, val loss 3.9271\n",
      "step 4500: train loss 2.2718, val loss 3.9604\n",
      "\n",
      "Ichät, dustrerndenndie St\n",
      "CHin;\n",
      "KWrlem.\n",
      "Eimend zerum den,\n",
      "Sichr ffen Scht.\n",
      "Leiseberechmeunfr’st win diem rch adr mist iel ochol Hurn, Dlseigescheust zuerr werisen.\n",
      "Den Gei Baufr dan Zurgegen!\n",
      "\n",
      "(Mingum faß sostong Esich est Is isen,\n",
      "ELiran,\n",
      "Zs as benz ss mmito ser vier nbzuchle ochm deit!\n",
      "Zus veienngecht fülun\n",
      "\n",
      "Disman\n",
      "\n",
      "STOPEPHig! ichtenn sen Maßte, macht nwiest,\n",
      "Auntt get ETer Weinen men grkermes nloplidend wind Naun Glas Lundeind Hiles ideng lerinen;\n",
      "MERRSoch NEr and.\n",
      "\n",
      "FAlle;\n",
      "Das (ren,\n",
      "Vür doche\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # new: increase number of iterations due to lower learning rate\n",
    "eval_interval = 500 \n",
    "learning_rate = 1e-3 # new: lower learning rate (self-attention is more complex)\n",
    "device = 'mps'\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # switch back to train mode\n",
    "    return out\n",
    "\n",
    "# new: single self-attention head\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for key. Typically no bias is used in self-attention\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for query\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for value\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # buffer = not a parameter; masking with lower triangular matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # batch, time, channels\n",
    "        k = self.key(x)   # (B,T,C) - apply the key linear layer\n",
    "        q = self.query(x) # (B,T,C) - apply the query linear layer\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) - scale by head_size**-0.5 (normalization from original paper, see above)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - mask out the future\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - apply softmax to get the weights\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C) - apply the value linear layer\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C) - weighted aggregation = self-attention\n",
    "        return out \n",
    "    \n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # new: dimensionality of embeddings changed to n_embd as intermediate layer\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # new: position embeddings\n",
    "        self.sa_head = Head(n_embd) # new: self-attention head\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # new: linear layer for prediction\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to the last block_size tokens (because we now use position embeddings, which only contain the last block_size tokens)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss decreased a bit, but the result is still not great. We will introduce some more changes following the transformer paper for further improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "---\n",
    "## 8. Full GPT Implementation\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "First, we add **multi-head attention**, which is simply several attention heads running in parallel, then concatenating the result over the channel dimension. A **projection layer** combines the concatenated outputs from all heads into a single unified representation and projects back to the original pathway. Note that \"projection\" in the context of Transformer models refers to a linear transformation that can either maintain, reduce, or even increase the dimensionality of the data. \n",
    "\n",
    "Intuitive Explanation: It helps to have multiple communication channels because these tokens have a lot to talk about - they want to find the consonants, the vowels, the vowels just from certain positions etc. and so it helps to create multiple independent channels of communication to gather lots of different types of data and then decode the output.\n",
    "\n",
    "<img src=\"img/multi-head-attention.jpg\" width=\"200\">\n",
    "\n",
    "### Transformer Block\n",
    "\n",
    "So far, we directly calculated the logits after the attention block, but this was way too fast - intuitively \"the tokens looked at each other, but didn't really have time to think on what they found from the other tokens\". Therefore, we add a feedforward single layer followed by a ReLU nonlinearity. Both layers together are called the **Transformer Block**, where we combine **communication** (self-attention) with **computation** (feedforward layer). This is on a per token level: Each token independently looks at the other tokens, and once it has gathered all the data, it thinks on that data individually. We implement this in the `Block` class below. The transformer block gets repeated over and over again.\n",
    "\n",
    "<img src=\"img/transformer.jpg\" width=\"300\">\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "Also note that the transformer architecture above contains **skip connections (residual connections)**: The network contains parallel paths (one with some computations, one with the identity as \"shortcut\") that are combined via additions. Additions are great for backpropagation because they distribute gradients equally to both branches, so there is a \"shortcut\" for the gradients to directly propagate from the output to the input of the network. This avoids the vanishing gradient problem especially in the beginning - the transformer blocks only get more influence over time.\n",
    "\n",
    "\n",
    "<img src=\"img/skip-connection.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm\n",
    "\n",
    "The transformer architecture uses **layer norm** (called \"Norm\" in the architecture image above), which is very similar to **batch norm**: Batch norm makes sure that across the batch dimension, any individual neuron has unit gaussian distribution (zero mean, unit standard deviation). In layer norm, we don't normalize the columns, but the rows, which normalizes over layers instead of over batches:\n",
    "\n",
    "\\begin{equation*}\n",
    "y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\varepsilon}}\\cdot \\gamma + \\beta,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (copied from BatchNorm1d in makemore series)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # previous batch mean -> index changed from 0 to 1\n",
    "    xvar = x.var(1, keepdim=True) # previous batch variance -> index changed from 0 to 1\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # no running mean and variance buffers needed like in batch norm\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 8a) Check if mean and standard deviation of rows and/or columns are normalized now! Write the result in the answer cell. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row means: tensor([ 2.3842e-09,  2.1458e-08,  7.1526e-09,  8.9407e-09,  0.0000e+00,\n",
      "        -7.1526e-09,  4.7684e-09,  1.7881e-08,  4.7684e-09, -1.4305e-08,\n",
      "        -4.7684e-09,  9.5367e-09, -4.7684e-09, -4.7684e-09, -1.9073e-08,\n",
      "         9.2387e-09, -1.4305e-08,  4.7684e-09, -4.7684e-09, -1.4305e-08,\n",
      "         2.9802e-09,  1.6689e-08, -1.1921e-08, -1.1921e-08,  3.0994e-08,\n",
      "         1.1921e-08,  2.8610e-08,  1.1921e-08, -1.4305e-08,  1.7881e-09,\n",
      "         2.1458e-08, -9.5367e-09])\n",
      "Row stds: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "Column means: tensor([ 0.1469, -0.5910, -0.3974,  0.0468, -0.1431,  0.0138, -0.2664,  0.4181,\n",
      "         0.1426,  0.2191,  0.2554, -0.2625, -0.0543, -0.1050,  0.1541,  0.2492,\n",
      "         0.2498,  0.1354, -0.2027, -0.3772,  0.2920,  0.1959, -0.2249, -0.0574,\n",
      "         0.1293, -0.1413,  0.1445, -0.2509,  0.1434,  0.0128,  0.0631, -0.2482,\n",
      "        -0.0977,  0.0945,  0.1880,  0.0951,  0.0047,  0.2833,  0.1154, -0.3063,\n",
      "         0.0510,  0.1602,  0.0598,  0.1157,  0.0083, -0.2541, -0.0447, -0.0921,\n",
      "         0.1891, -0.0150, -0.1857, -0.4513, -0.1106,  0.0320,  0.0417,  0.1272,\n",
      "        -0.3022, -0.2864,  0.2507, -0.1101,  0.0402,  0.2277,  0.2753,  0.2577,\n",
      "        -0.1698,  0.2775, -0.1854,  0.0767, -0.2023,  0.2106,  0.1443,  0.1391,\n",
      "         0.1628,  0.1442, -0.0223, -0.0108,  0.0173,  0.0508, -0.0126,  0.1257,\n",
      "         0.0073,  0.0803, -0.0412,  0.0335, -0.2576, -0.0642, -0.1877, -0.1026,\n",
      "        -0.0888,  0.1562,  0.1422,  0.0295,  0.1925, -0.2169, -0.1416,  0.1693,\n",
      "        -0.1704, -0.2708,  0.1440, -0.2105])\n",
      "Column stds: tensor([0.8803, 0.8950, 1.0106, 0.8120, 0.8378, 0.8778, 0.9006, 1.0241, 0.8884,\n",
      "        0.9859, 1.0292, 1.0922, 0.8939, 0.8852, 1.2166, 0.9655, 1.0318, 0.9787,\n",
      "        1.0585, 1.0982, 1.1355, 1.1005, 1.1575, 1.1192, 0.7503, 0.9659, 0.9294,\n",
      "        0.9057, 0.8608, 0.9525, 0.8335, 0.9605, 1.0106, 0.9159, 1.0465, 1.0527,\n",
      "        0.9906, 1.2211, 0.8427, 0.7639, 1.1859, 0.9081, 1.0370, 1.0241, 1.1700,\n",
      "        0.8488, 0.7953, 0.8892, 0.8306, 1.1103, 1.1911, 1.0175, 1.0594, 0.9435,\n",
      "        1.1854, 1.1084, 0.9090, 1.1427, 0.8859, 1.0218, 0.9299, 0.9352, 0.9851,\n",
      "        0.8771, 1.0274, 1.0281, 1.1688, 0.9663, 0.9362, 0.9669, 0.9714, 1.0670,\n",
      "        0.8951, 0.9651, 0.9589, 0.8693, 1.1023, 0.7872, 0.8986, 1.1589, 1.0867,\n",
      "        1.1410, 1.0204, 1.4143, 0.8721, 0.9212, 1.0595, 1.0101, 0.8902, 0.9748,\n",
      "        1.0133, 0.8148, 1.0270, 0.9148, 0.7725, 1.0107, 0.9155, 1.1451, 0.8832,\n",
      "        0.9467])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Means along dimension 1 (per sample)\n",
    "print(\"Row means:\", x.mean(dim=1))  # should be close to 0\n",
    "print(\"Row stds:\", x.std(dim=1))    # should be close to 1\n",
    "\n",
    "# Means along dimension 0 (per feature across samples)\n",
    "print(\"Column means:\", x.mean(dim=0)) # no guarantee to be 0\n",
    "print(\"Column stds:\", x.std(dim=0))   # no guarantee to be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "row_means will be very close to zero.\n",
    "\n",
    "row_stds will be very close to one.\n",
    "\n",
    "col_means and col_stds will not necessarily be normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that layer norm is usually applied before the self-attention and linear layer nowadays (unlike the original paper) - one of the very few changes of the transformer architecture during the last years, otherwise mostly the architecture remained unchanged. This is called the **pre-norm formulation**. So here is a code snippet used below showing the two layer norms we will implement, one before the self-attention and one before the linear layer:\n",
    "\n",
    "```\n",
    "        x = x + self.sa(self.ln1(x)) # layer norm directly applied to x before self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # layer norm applied before linear layer\n",
    "```\n",
    "\n",
    "Finally, another layer norm is typically applied at the end of the Transformer and right before the final linear layer that decodes into vocabulary. \n",
    "\n",
    "The size of the layer norm is `n_embds=32` here, so this is a per token transformation, it just normalizes the features and makes them unit Gaussian at initialization. Because these layer norms contain gamma and beta as trainable parameters, the layer norm may eventually create outputs that are not unit Gaussian depending on the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up the Model\n",
    "\n",
    "We now have all components together so that we can scale up the model and make it bigger. Therefore, we add a parameter `n_layer=4` to specify that we want 4 transformer blocks.\n",
    "\n",
    "We also add **dropout** to prevent overfitting: with 4 transformer blocks, the network is getting quite large now. Therefore, we randomly deactivate some connections to prevent them from becoming too dominant. Because the mask of what's being dropped out has changed every single forward backward pass, effectively we end up training an ensemble of sub-networks. At test time, everything is fully enabled and all of those sub-networks are merged into a single ensemble, making it more robust.\n",
    "\n",
    "\n",
    "<img src=\"img/dropout.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full GPT with Multi-Head Attention and Transformer Block\n",
    "\n",
    "We finally get to the full GPT code, adding all the components explained above!\n",
    "\n",
    "**TODO:** 8b) In the summarized code below, comment each line to make sure you have understood all GPT components! You may use support from ChatGPT or GitHub Copilot, but double-check the results and be able to explain it yourself. (Yes, this is tedious, but it will help you get an in-depth understanding of the full GPT architecture) **(10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n",
      "10.809692 M parameters\n",
      "\n",
      "==============\n",
      "step 0: train loss 4.5842, val loss 4.5758\n",
      "==============\n",
      "\n",
      "Sample:\n"
     ]
    }
   ],
   "source": [
    "# YOUR COMMENTS HERE\n",
    "batch_size = 16 # \n",
    "block_size = 32 # \n",
    "max_iters = 5000 # \n",
    "eval_interval = 500 # \n",
    "learning_rate = 1e-3 # \n",
    "eval_iters = 200 # \n",
    "n_embd = 64 # \n",
    "n_head = 4 # \n",
    "n_layer = 4 # \n",
    "dropout = 0.2 # \n",
    "\n",
    "# hyperparameters version 2 - only uncomment when training on GPU (no comments needed here)\n",
    "#\"\"\"\n",
    "batch_size = 64 \n",
    "block_size = 256 \n",
    "max_iters = 5000 \n",
    "eval_interval = 500 \n",
    "learning_rate = 3e-4 \n",
    "eval_iters = 200 \n",
    "n_embd = 384 \n",
    "n_head = 6 \n",
    "n_layer = 6 \n",
    "dropout = 0.2  \n",
    "#\"\"\"\n",
    "\n",
    "# ------------\n",
    "\n",
    "device = 'mps'\n",
    "print('Running on device:',device) # \n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data # \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # \n",
    "    x, y = x.to(device), y.to(device) # \n",
    "    return x, y #\n",
    "\n",
    "@torch.no_grad() # \n",
    "def estimate_loss(): # \n",
    "    out = {} # \n",
    "    model.eval() # \n",
    "    for split in ['train', 'val']: # \n",
    "        losses = torch.zeros(eval_iters) # \n",
    "        for k in range(eval_iters): # \n",
    "            X, Y = get_batch(split) # \n",
    "            logits, loss = model(X, Y) #  \n",
    "            losses[k] = loss.item() # \n",
    "        out[split] = losses.mean() # \n",
    "    model.train() # \n",
    "    return out # \n",
    "\n",
    "class Head(nn.Module): ### nochmal ckecken\n",
    "\n",
    "    def __init__(self, head_size): # \n",
    "        super().__init__() # \n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # \n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # \n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # \n",
    "\n",
    "    def forward(self, x): # \n",
    "        B,T,C = x.shape # \n",
    "        k = self.key(x)   # \n",
    "        q = self.query(x) # \n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # \n",
    "        wei = F.softmax(wei, dim=-1) # \n",
    "        wei = self.dropout(wei) # \n",
    "        \n",
    "        v = self.value(x) # \n",
    "        out = wei @ v # \n",
    "        \n",
    "        return out # \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size): \n",
    "        super().__init__() # \n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # \n",
    "        self.proj = nn.Linear(n_embd, n_embd) #\n",
    "        self.dropout = nn.Dropout(dropout) # \n",
    "        \n",
    "    def forward(self, x): # \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # \n",
    "        out = self.dropout(self.proj(out)) # \n",
    "        return out # \n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd): #\n",
    "        super().__init__() # \n",
    "        self.net = nn.Sequential( # \n",
    "            nn.Linear(n_embd, 4 * n_embd), # \n",
    "            nn.ReLU(), # \n",
    "            nn.Linear(4 * n_embd, n_embd), # \n",
    "            nn.Dropout(dropout), # \n",
    "        )\n",
    "\n",
    "    def forward(self, x): # \n",
    "        return self.net(x) # \n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head): # \n",
    "        super().__init__() # \n",
    "        head_size = n_embd // n_head # \n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # \n",
    "        self.ffwd = FeedFoward(n_embd) # \n",
    "        self.ln1 = nn.LayerNorm(n_embd) # \n",
    "        self.ln2 = nn.LayerNorm(n_embd) # \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # \n",
    "        x = x + self.ffwd(self.ln2(x)) #\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() # \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # \n",
    "        self.ln_f = nn.LayerNorm(n_embd) # \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # \n",
    "\n",
    "    def forward(self, idx, targets=None): # \n",
    "        B, T = idx.shape # \n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # \n",
    "        x = tok_emb + pos_emb # \n",
    "        x = self.blocks(x) # \n",
    "        x = self.ln_f(x) # \n",
    "        logits = self.lm_head(x) # \n",
    "\n",
    "        if targets is None: # \n",
    "            loss = None # \n",
    "        else:\n",
    "            B, T, C = logits.shape # \n",
    "            logits = logits.view(B*T, C) # \n",
    "            targets = targets.view(B*T) # \n",
    "            loss = F.cross_entropy(logits, targets) # \n",
    "\n",
    "        return logits, loss # \n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # \n",
    "        for _ in range(max_new_tokens): # \n",
    "            idx_cond = idx[:, -block_size:] # \n",
    "            logits, loss = self(idx_cond)  # \n",
    "            logits = logits[:, -1, :] # \n",
    "            probs = F.softmax(logits, dim=-1) # \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # \n",
    "        return idx\n",
    "\n",
    "\n",
    "model = GPTLanguageModel() # \n",
    "model = model.to(device) # \n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') # \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # \n",
    "\n",
    "for iter in range(max_iters): # \n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1: #\n",
    "        losses = estimate_loss() # \n",
    "        print(\"\\n==============\")\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # \n",
    "        print(\"==============\")\n",
    "        \n",
    "        print(\"\\nSample:\")\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=device) # \n",
    "        print(decode(model.generate(context, max_new_tokens=200)[0].tolist())) # \n",
    "\n",
    "    xb, yb = get_batch('train') #\n",
    "\n",
    "    logits, loss = model(xb, yb) # \n",
    "    optimizer.zero_grad(set_to_none=True) # \n",
    "    loss.backward() # \n",
    "    optimizer.step() # \n",
    "    \n",
    "print(\"\\nFinal sample:\") # \n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) #\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist())) # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have trained a more powerful GPT model using self-attention. Let's generate a longer text and see how the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ichät, d MArerndendige St\n",
      "Flin;\n",
      "Kör Blt,\n",
      "\n",
      "MEYEnzerumicht,\n",
      "\n",
      "USorst der Schaf sich juechmeunfr’st win dit?\n",
      "Wein drn, san.\n",
      "E er ol Hurn, Dar igesoleust zeVör werisen.\n",
      "Den Geich mar dan Zurgegen!\n",
      "\n",
      "(Minnuaß™, d\n",
      "FAtongichärerer sieif en,\n",
      "ESTr ige?\n",
      "Fldet ST.\n",
      "Un mito ser vichrnbzechlt weim deit!\n",
      "Zustveien\n",
      "Mecht ffübe\n",
      "\n",
      "Distwi\n",
      "\n",
      "STOPHau!\n",
      "Deichtendes uf.\n",
      "NDagascher n Juen,\n",
      "ARETOPHat ETer Weieses, STOPHISTOPHoch lidend!\n",
      "\n",
      "Mun, uffllas LLeheichört Tenid Eg lerinen;\n",
      "MERGSoch NEr mieneinen m;\n",
      "Das (fen,\n",
      "Dir wochen,\n",
      "Weieis zenels!\n",
      "Dan deinhns wichrtfchür War spf weirt,\n",
      "(mprt,\n",
      "Sin,\n",
      "Menn dirichöfsch ichöt icherzwäh ELageiterk!\n",
      "(Ihero TOPHinffr serd\n",
      "\n",
      "Mäueickn, Gen i,\n",
      "Den llollle: s STürs n Damar derög s vo iftexellen,\n",
      "Deich KÜLEPht d auGenins, afllim, dernn spfllobs ir).\n",
      "Nunn zußereneichrar m dichinau nerd ie inschaund\n",
      "Zend Jabaf we ve wen enn.\n",
      "Felt He m eiser gt wär sten.\n",
      "Jan?\n",
      "Dant s dunn eichelllktühech mminder d,\n",
      "FAchon wet iesone westefes mien gerns inigener Kndechlppr ernindast ner vosigeen, er mm!\n",
      "FAUSTHEr fach.\n",
      "\n",
      "Mäß en.\n",
      "Zalldich Wihtr ht nanduerichen Zu st He ierän,\n",
      "\n",
      "\n",
      "ST.\n",
      "Ncham So Michlomant bettendieden sin e risordienenüch f Qufüh jagll,\n",
      "\n",
      "Vobllaft Gest, ir!\n",
      "West stenn kTMESchltr al d dehn ndirb Senichmöh?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Füche schlind! Genagegr ie Fr den waben Kres von, hübet öhl fruskehne is äfüfte,\n",
      "\n",
      "Schanders ichtellaln, w’sinadint Danzun f so (wit selendahei wenntendeihr LToßt dero dereicht kerfr Tasch TEs d ut Ger, esinaß ve üble, nndeich HISch Gitzun l bege?\n",
      "Din! maben be*Si,\n",
      "Dechäß ihinn, as dein in.\n",
      "\n",
      "gasiesen wildir Demmpin st wübicherlen it das bein de sGld\n",
      "Dend asch.\n",
      "S.\n",
      "HISTan! d FAun e Man ipfälldu)\n",
      "Dereblemanditoch wendundan;\n",
      "\n",
      "SCHEREuß T.\n",
      "HELäß gll?\n",
      "SCHEichimu d a nzierieni\n",
      "\n",
      "\n",
      "Went.\n",
      "\n",
      "Dor Kodigerch ufte s Ge walst,\n",
      "Deanu iere.\n",
      "CHE.\n",
      "Wenderur.\n",
      "ARebelied MENammunginu voh hebed h de Bigedet.\n",
      "OPHauneiner ur s mira eie ffeohwan ESTHeß de err uchrät HÜbs Bu dage.)\n",
      "Nau HEISpf STast MELEPHEulzeindunil s Kalt Köh horubsckenen zuffüt Naumindem Meht sch fr zuich ist.\n",
      "\n",
      "Wie, STE.)\n",
      "Ven\n",
      "Dich iein ent, TEPHön dede ach Bom Zatse seichtunhellichehtendeub r ieich heih Flgerrakauf ie,\n",
      "Wutiedu as mpfr HISchehnen, s aßes LFAunn,\n",
      "STE e’s Hanend usthrelalen nd waunnder:\n",
      "Sohe.\n",
      "Hörochindirr NZust!\n",
      "\n",
      "\n",
      "Ichror EPHISTe abebteick s zungs?\n",
      "Nach seht den Wehren1YEL.\n",
      "\n",
      "Wischlch hiemelt!\n",
      "Gen;\n",
      "In n in s acheis nimen ick wegichercken, is uferkah r seste g tzest dellon Simin!\n",
      "\n",
      "Unen Gmu be Genktenn weler,\n",
      "DISederispach ein icher Bl?\n",
      "TErier.\n",
      "\n",
      "(Geinden Gelt\n",
      "\n",
      "V wabrs n’st eranbes eh him MAUndeier angere,\n",
      "Ungeiche urrzer d iche l us Ichtsoch h Jauß’s sisenor emmerzgtexen s NESirli, Hen fasor.\n",
      "IEht frunu t is Hinstes d wbrdue de micheieich\n",
      "Jus sandocherprndasteig, anst wener m un NDibächehen alisckt—\n",
      "Er deme veu Helte’stsckuschmme b indefelstufren ksgriemin geht)\n",
      "STe, dorllamenenichas mme\n",
      "Dalleb st Kin damerar fn, om STOPHeunen salül d us ich!\n",
      "\n",
      "\n",
      "\n",
      "Deuleruschaß.\n",
      "\n",
      "Fel enorärdell\n",
      "Barssprun Narm Drbates li—\n",
      "OPHon;\n",
      "CHIhrerqu s Waguf win.\n",
      "Fl er mfür Kache Glämüben?\n",
      "Unqu Scheh ves der din?\n",
      "ISchert, d’s Gote umeimeieindeimern;\n",
      "Un,\n",
      "STEirb’senb Kaummlenu din st winer dad win ein d dich n ud mir Sts im alärd egieich ich s z!\n",
      "Deh ed eh er m sespab FALE drmind.\n",
      "Zenk dast Somit a züheheundaumeschten, se.\n",
      "\n",
      "Der hte.\n",
      "\n",
      "Mer Wen Miend: Len f wemigan aseien gellbt Ga eier,\n",
      "Mer frn un,\n",
      "Ichend aß, lt Micha eneu ich.\n",
      "Aus diche ztichäh nzinnzar ngund daungotzüsienewindirat seieill an HIh EL•woh en\n",
      "\n",
      "Wersfs men\n",
      "Mehertt agllien (Zu sehechinngest s wichich s AGer!\n",
      "Enürt,\n",
      "Wend ndeiterak\n",
      "(z zun vogleieralten, ger farmm füFreu, Taröngl wer al zunirin.\n",
      "Da derben,\n",
      "Un.\n",
      "S.\n",
      "Wint erder Fl ier s f saschim Kertser dt d mu dicht wesehwührsiche d Kerbilern grsteib, zussichinich au ei ielll Ker zun s n ERTOPen terch de wane s ETOPHEPHELE n, gem. TIh AUn st.\n",
      "\n",
      "FAR seh s ust dem sach drek mdarod GESTERAbaleges, zend dier min bt k edinübteief, em it Bein funisigstechire mibt Mendreh.\n",
      "Un, deif FAlksk wög;\n",
      "FAUmöräfndieich Danom d ichklt dan wezinir n Wisar Zeirran r GAuch.\n",
      "S.\n",
      "\n",
      "DIhas tt ES.).\n",
      "Icheh von STOPude wots en, dinnen dihech!\n",
      "Denken—d l.\n",
      "\n",
      "\n",
      "ST.\n",
      "Dor?\n",
      "CHuhocht z Ge an, frn ST.\n",
      "Waustrübern Übzut f m seiendam irt d der zut!\n",
      "AUnnder KApelaleie st,\n",
      "Doche dellieder Wih muft enenglohopfürch dr.\n",
      "Quchmpabed teiehn d sit,\n",
      "\n",
      "Dalin.\n",
      "Vit m anereberun.\n",
      "Din Keieragesch derän jERGerwien7Ätt?\n",
      "Zauft,\n",
      "’\n",
      "Dall Schllst.\n",
      "Wen dinn ken s in,\n",
      "ISo inb, d, LVAuch indureich?\n",
      "Dar mm ELe7Welst icht Geigen elolsenachlaß Blthw:Wer STOPHITOrent Wätickr dam dre wübtsiene wär z iWemienkS.\n",
      "Sch uftemmwrewieich s t,\n",
      "Inen st n\n",
      "Grirwer s mar Kn s s fah Zuf flluft mauchich wielürimwernich mimit, vo n d u, in mmmiersendeweisch dal ichzt STL.)\n",
      "Daumasaune.\n",
      "Wahalennumen s winiberen dind as iendemiench h e ent igachal uer füh absieckewinein uf sch LEPHichr nno m ger s.\n",
      "Flin,\n",
      "USTOPHerdir as n!\n",
      "Mand bs ewohnufftzurodiein\n",
      "ISchr elsin Sten merar inimmtt d ageinagehichet at wer d in—\n",
      "EPHEben eh Henumeht hmünlichehrnich lt in wan ben urb deng, liche e Webechrllenagauch lschtzut elen mm d Schl ichleisch Auch mmabe, aßtt HEPHISehtrje st Linnteu hwes is il)\n",
      "MELerzun ar,\n",
      "Iniseier ichageiß ien, Gengr ahrn wo ie Sxert, Ven Waur ENat’st. hrst ien\n",
      "\n",
      "DELEr!\n",
      "\n",
      "Meg TObebs sein\n",
      "Bleingert esit!\n",
      "Hatt!\n",
      "Jufieicht schitebsts feiespfüße, duß,\n",
      "Ge,\n",
      "Daußt dert? u gstt, ü$2Quminu sa’g, wieur derer?\n",
      "Pe inie rufülst FAun ind gen\n",
      "Jaut soh ken drb!\n",
      "D’s﻿ icke zwesf ten äber woch Waresint u kun er mieh NTOPHE.\n",
      "DCHag icht, sich rön, gewagaßen Wau ETr du manweirch,\n",
      "FARErin zundichterorur ih d ncko vochn si,\n",
      "Jachttebichton dr Michrs S.\n",
      "Märu Eichtichenicke n;\n",
      "ISaß Schtendeninicho afob R.\n",
      "DEPHEiß ez!\n",
      "Geie menso st Jumichern Hecht se R weinwureier igalidat g veierlditt.\n",
      "Dopfalll, dusicht beiene beröneunestt\n",
      "USi™Qufe bar.\n",
      "E.\n",
      "EPft n;\n",
      "Gun lbslerst)\n",
      "Un,Vendue Tebent,\n",
      "F“Fünktefle ders ichr Fürspun! TEPHEPHIcheien Bin ju dieichn.\n",
      "\n",
      "AUN.\n",
      "Br n!\n",
      "\n",
      "öfütattederür nachaut iefühastende malt Wei, ichiebeite it sgaufüh Wäldiertenaal, ir\n",
      "Wage,\n",
      "Wisch S“Ach dien STOPusumen FAltert gnongl!Hachlalichter s s venduch Geizulichaß vodendebör TOPHiglttern, falet rb wen MEien (m uinzu.\n",
      "Wenscharistase Tandirebiert hnen)\n",
      "Naut, Is, ge Trd ienndutät sprtengst inte Kil mie GAYE g.\n",
      "(Zertebilt beich flürkon, st HEKebest mmieneran.\n",
      "Imien memeigt, s Das;\n",
      "\n",
      "\n",
      "FAur, nie n he.\n",
      "\n",
      "Sch mön.).\n",
      "US.\n",
      "Wechlun mzutrond mer ver s wedueh sägleieirbollüß.\n",
      "Duf FAnkerd e Muger Pfen Glzu,\n",
      "Me vogen? kudo get Zebein, uneiht,$Waut m Tüg glühteu kerenteimeht s, ut Thtgsanst,\n",
      "USo en.\n",
      "\n",
      "Daorach decht.\n",
      "Im Gasünanin s (ENan hochores d den\n",
      "\n",
      "Dien,\n",
      "Närerühn,\n",
      "Wiß d o8MANasch t mieichru s wan s afz blu o Tast Bresirn hnkigalacht ited GEn Zeint Gergten e!\n",
      "Nansa, atüytar t, S.\n",
      "MELEle, FANagen STr,\n",
      "ISTr gun;\n",
      "NDIh.\n",
      "\n",
      "Maßt n?\n",
      "De d t iet3n Dit GARe lebas Krdaun götenstzu\n",
      "ST.\n",
      "DEST.\n",
      "Wiener an Ber be micker sch Kuübein d ft s BÜLEst,\n",
      "\n",
      "\n",
      "ÄRähenselend INtachwenero atrangll,\n",
      "\n",
      "Aunstrt fteihwon biTon deller Gehloleh rsirengrsam e EISiglasche r trn, berauch Nagtauste, b d et d)\n",
      "Stor G3?\n",
      "\n",
      "Wo4ön, be Kür b,\n",
      "\n",
      "EPHEnt schl’st,\n",
      "Ilüllllent dengogehich n Dr;\n",
      "Mas windiliß it! HESt Zigen d sVese sochr r ELartich;\n",
      "Ferte wolecht HIch d Heglderit, henengeben at TOGnsten?\n",
      "Euden.)\n",
      "Mäminer ha E d Wiche mun dus GEPUn!—LERLeh t,\n",
      "\n",
      "Diet MELageh venhrit ed Tr litin, wie men nsich, Schrch h!\n",
      "FAben, fs st MERN GE TOPHörrt ilau minder dasprer deneleichr Leiniest.\n",
      "STOrbeietzen,\n",
      "Wenn nitendnd krt,\n",
      "\n",
      "Molschr s e je vagexen, s wachm\n",
      "Maschiebvon d Ge.\n",
      "\n",
      "Nagebahochr inlich lteie, Fewene Spierschin bt stwilstr ieie h Kr Gr en d deient iterb mero LEBeigen stzuh bie iersenerscht atater ut;\n",
      "R.\n",
      "\n",
      "Drun, un dehllel Gasen.\n",
      "NDLERSirst fübench vochen denfim RR.).\n",
      "Acht,\n",
      "\n",
      "Doders.\n",
      "Zebr masige Zwi Linst HE CHielerafr ht, R asch Weit Gnsen dufichenar s s dts neren n.\n",
      "Ku, irllbengstzwörichoch,\n",
      "\n",
      "Daut sin m hrien\n",
      "FAUSylequr PWed\n",
      "AGel Zich, Ichagt dichran sagewerfen, ELat, vochodewiele ield DIST.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "öhnir m Schl Im Küchr bemin don wuhe ichärin vomieläbsst?\n",
      "\n",
      "Vorste\n",
      "\n",
      "SCHIht bererk!\n",
      "\n",
      "Den en iß HERTänereuchk!\n",
      "Zu K.\n",
      "Frkeis mngeseun ne de, Kr liesch n,\n",
      "DarT.\n",
      "\n",
      "ISpi!\n",
      "—\n",
      "Sen sechtzul\n",
      "Fre Lumah dl ahrlzut!\n",
      "Eierltercheir,\n",
      "Bin dfas mehndenmelusinn, h ufr meieint,\n",
      "Demil! Hußtaller mer\n",
      "\n",
      "ME de a Fas!\n",
      "(Sch SBordn,\n",
      "(ELELE.\n",
      "Mube R.\n",
      "Malldehägolt mert Au d dichn d k,\n",
      "E.\n",
      "Blantsichlaldrherteiego ildicherengaßemerle nndadichobrstenolumin ile ierdunk ven.\n",
      "Dis Venemehldu m! llenst FAUn wähndeich Molaß! an dallieies Mrch inn achauf5R ss agen de solldih klbehtin de at,\n",
      "MAl s d kur ÜLEPas ickerie dt.\n",
      "Gen win m enedat?\n",
      "MAchtachonnngewir üh wiehr wite)\n",
      "Win diennd icherkler, dihienengäu,\n",
      "Wit?\n",
      "Etuchr widen waundu z sche Men m s in medr st, wien.\n",
      "Gau i,\n",
      "Sohte Ar’s n, he din USchaun inder micheno iterldichumin zur TMieger mest ustumindelt, sit en it” waga DErunet fendid!\n",
      "Brarufür ir woß aun wesischh, d ducht un (t ktein Erh ar Ich schle dagenl\n",
      "MAurggeräran meich duchn Zwinen s ke wisInde was ETHISt neneiteiroghen\n",
      "Walt weflen ma,\n",
      "Nu E Kaß,\n",
      "US sirn s Nein, de ss Je AUCHähen dochsie, s dit gtun hi!\n",
      "Ichs?\n",
      "Sonn daus Eu iet g, m,\n",
      "Fühtaht,\n",
      "Deinderobeint;\n",
      "MAuhlluten er—“Welerun s Bon TO gelt,\n",
      "ISer dereicht uferen\n",
      "Far!\n",
      "De\n",
      "\n",
      "\n",
      "FAR.\n",
      "Lald jaläfolte manich Ger rlch nebten f m urt.\n",
      "Gleinbesereracht ltzwicht, iegeir 4len min bt s”, ere, LENam ER.\n",
      "Err us waron salbrerintalkat fsinenut s mugelen ets h,\n",
      "Zeräm mm PHienge d Ihenso dumim dutenit wübt imprisiebe h d Gr zuch met z er,\n",
      "Mut.\n",
      "Wen em ihe Gostu Komab au,\n",
      "Malicho arsenien Schabrdan,\n",
      "ISTru mie!\n",
      "Dut LE eien, achönne s mas wömen z d,\n",
      "Docht Ko deeumgerst Gofezumein Zelandan! de en delier.\n",
      "Krallichr ien saue\n",
      "Wich h e (lur Wit diedent densen ufoche.)\n",
      "Um TOKöhtesetzt vondeg ieradeitäräuch unerem!\n",
      "FAne Wißt hachref.\n",
      "HISind ser de r!\n",
      "Un!\n",
      "ST, as acht, Fich ist.\n",
      "Nulertr Wist Vosehan n it lardir inerast’s•pfeberdie ziche bol micht’getent’sitlät.)\n",
      "Un;”\n",
      "Me be di gllen,\n",
      "Meicht, Degt,\n",
      "MAbidabeichtener Nusichadundus PHernin alch de FAUnar Gerürt Tr THELierasundens dam in\n",
      "\n",
      "STOh gecheb tt es ies Wienandunge\n",
      "AUnr s s dirt,\n",
      "Dum s de itehätrs,\n",
      "Frweft Icht ün Sichr wäfentr st uderas ichtienrnrert zwiemich d denn erebebes leichich u ST.\n",
      "\n",
      "STaterseir eh s!\n",
      "\n",
      "Semg geb geingren Jaspfsich woler el SCHELit\n",
      "ISCh STOPHEIROPHISTOPHERe GrtüSin med geldin unin\n",
      "Gasochn woh RO\n"
     ]
    }
   ],
   "source": [
    "# generate a longer sample\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "new_text = decode(model.generate(context, max_new_tokens=10000)[0].tolist())\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to a text file\n",
    "f =  open(\"GPT_generated_text.txt\",\"w\")\n",
    "f.write(new_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (optional):** Apply the code to a different text of your choice! What loss do you achieve? What parameters did you change and why? How do you interpret the output compared to the Shakespeare output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlook and Next Steps\n",
    "### Andrej's Suggested Further Experiments\n",
    "\n",
    "- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
    "- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
    "- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n",
    "- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder and Encoder\n",
    "\n",
    "Text generation as above only uses the **decoder** part of the transformer architecture. The **decoder attention block** implemented above has **triangular masking**, and is usually used in autoregressive settings, like language modeling. \n",
    "\n",
    "In other settings, we do want \"future\" tokens to influence the prediction, so we do not use triangular masking. For example, in sentiment analysis, we look at a whole sentence at once, then predict the sentiment \"happy\" or \"sad\" of the speaker. This can be realized using an **encoder** attention block. To implement an encoder attention block, we can simply delete the single line that does masking with `tril`, allowing all tokens to communicate. Attention does not care whether tokens from the future contribute or not, it supports arbitrary connectivity between nodes.\n",
    "\n",
    "### From Self-Attention to Cross-Attention\n",
    "\n",
    "**Self-attention** means that the keys and values are produced from the same source as queries. In **cross-attention**, the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module). For example, when translating from French to English, we condition the decoding on the past decoding *and* the fully encoded french prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From GPT to ChatGPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a long way to go from our toy GPT example to ChatGPT. \n",
    "First of all, ChatGPT's **pre-training** was done on a large chunk of internet, resulting in a decoder-only transformer for text generation. \n",
    "So the pretraining is quite similar to our toy example training, except that we used roughly 10 million parameters and the largest transformer for ChatGPT uses 175 billion (!) parameters. Also it was trained on 300 billion tokens (our training set would be 300.000 tokens roughly when not using character-level tokens, but sub-word chunks). This is about a million fold increase in number of tokens - and today, even bigger datasets are used with trillions of tokens for training on thousands of GPUs!\n",
    "\n",
    "See the following table for the number of parameters, number of layers, n_embd, number of heads, head size, batch size and learning rate in **GPT-3**:\n",
    "\n",
    "\n",
    "<img src=\"img/GPT3_params_table.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pre-training, the model will be a document completer, it will not give answers but produce more questions or result in some undefined behavior. For becoming an assistant, further **fine-tuning** is needed using **Reinforcement Learning from Human Feedback (RLHF)**. Here is an overview of manual fine-tuning with human AI trainers (see the OpenAI ChatGPT blog for details, link below):\n",
    "\n",
    "<img src=\"img/chatgpt_diagram_light.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "To sum it up, we trained a decoder only Transformer following the famous paper 'Attention is All You Need' from 2017, which is basically a GPT. We saw how using self-attention, we can calculate a weighted average of past tokens to predict the next token. We trained it on different texts (Shakespeare, Faust, Jane Austen etc.) and produced new texts in the same writing style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- Attention is All You Need paper: https://arxiv.org/abs/1706.03762\n",
    "- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 \n",
    "- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output in 3_Character_Level_GPT__student.ipynb above threshold seen and so a NEW version has been made: `TRUNCATED_3_Character_Level_GPT__student.ipynb`.\n",
      "[NbConvertApp] Converting notebook TRUNCATED_3_Character_Level_GPT__student.ipynb to webpdf\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 6 image(s).\n",
      "[NbConvertApp] Building PDF\n",
      "Removing unused browser at /Users/HAI5RT/Library/Caches/ms-playwright/chromium-1140\n",
      "Downloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-mac-arm64.zip\u001b[22m\n",
      "\u001b[1G121.6 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G121.6 MiB [                    ] 0% 33.3s\u001b[0K\u001b[1G121.6 MiB [                    ] 0% 19.8s\u001b[0K\u001b[1G121.6 MiB [                    ] 0% 13.9s\u001b[0K\u001b[1G121.6 MiB [                    ] 0% 10.6s\u001b[0K\u001b[1G121.6 MiB [                    ] 0% 9.1s\u001b[0K\u001b[1G121.6 MiB [                    ] 1% 8.1s\u001b[0K\u001b[1G121.6 MiB [                    ] 1% 7.7s\u001b[0K\u001b[1G121.6 MiB [                    ] 1% 7.0s\u001b[0K\u001b[1G121.6 MiB [                    ] 2% 7.0s\u001b[0K\u001b[1G121.6 MiB [                    ] 2% 6.7s\u001b[0K\u001b[1G121.6 MiB [=                   ] 2% 6.5s\u001b[0K\u001b[1G121.6 MiB [=                   ] 3% 6.4s\u001b[0K\u001b[1G121.6 MiB [=                   ] 3% 6.3s\u001b[0K\u001b[1G121.6 MiB [=                   ] 3% 6.0s\u001b[0K\u001b[1G121.6 MiB [=                   ] 4% 5.9s\u001b[0K\u001b[1G121.6 MiB [=                   ] 4% 7.9s\u001b[0K\u001b[1G121.6 MiB [=                   ] 4% 8.0s\u001b[0K\u001b[1G121.6 MiB [=                   ] 4% 7.9s\u001b[0K\u001b[1G121.6 MiB [=                   ] 4% 7.8s\u001b[0K\u001b[1G121.6 MiB [=                   ] 5% 7.8s\u001b[0K\u001b[1G121.6 MiB [=                   ] 5% 7.7s\u001b[0K\u001b[1G121.6 MiB [=                   ] 5% 7.6s\u001b[0K\u001b[1G121.6 MiB [=                   ] 5% 7.5s\u001b[0K\u001b[1G121.6 MiB [=                   ] 6% 7.6s\u001b[0K\u001b[1G121.6 MiB [=                   ] 6% 7.5s\u001b[0K\u001b[1G121.6 MiB [=                   ] 6% 7.4s\u001b[0K\u001b[1G121.6 MiB [=                   ] 7% 7.3s\u001b[0K\u001b[1G121.6 MiB [=                   ] 7% 7.8s\u001b[0K\u001b[1G121.6 MiB [=                   ] 7% 8.2s\u001b[0K\u001b[1G121.6 MiB [=                   ] 7% 8.4s\u001b[0K\u001b[1G121.6 MiB [=                   ] 7% 8.6s\u001b[0K\u001b[1G121.6 MiB [=                   ] 7% 8.8s\u001b[0K\u001b[1G121.6 MiB [=                   ] 7% 9.0s\u001b[0K\u001b[1G121.6 MiB [=                   ] 7% 9.1s\u001b[0K\u001b[1G121.6 MiB [==                  ] 7% 9.1s\u001b[0K\u001b[1G121.6 MiB [==                  ] 7% 9.0s\u001b[0K\u001b[1G121.6 MiB [==                  ] 8% 9.9s\u001b[0K\u001b[1G121.6 MiB [==                  ] 8% 9.7s\u001b[0K\u001b[1G121.6 MiB [==                  ] 8% 9.6s\u001b[0K\u001b[1G121.6 MiB [==                  ] 8% 9.4s\u001b[0K\u001b[1G121.6 MiB [==                  ] 9% 9.2s\u001b[0K\u001b[1G121.6 MiB [==                  ] 9% 9.0s\u001b[0K\u001b[1G121.6 MiB [==                  ] 9% 8.9s\u001b[0K\u001b[1G121.6 MiB [==                  ] 10% 8.8s\u001b[0K\u001b[1G121.6 MiB [==                  ] 10% 8.7s\u001b[0K\u001b[1G121.6 MiB [==                  ] 10% 8.5s\u001b[0K\u001b[1G121.6 MiB [==                  ] 11% 8.5s\u001b[0K\u001b[1G121.6 MiB [==                  ] 11% 8.6s\u001b[0K\u001b[1G121.6 MiB [==                  ] 11% 8.4s\u001b[0K\u001b[1G121.6 MiB [==                  ] 11% 8.3s\u001b[0K\u001b[1G121.6 MiB [==                  ] 12% 8.2s\u001b[0K\u001b[1G121.6 MiB [==                  ] 12% 8.1s\u001b[0K\u001b[1G121.6 MiB [===                 ] 12% 8.0s\u001b[0K\u001b[1G121.6 MiB [===                 ] 13% 7.9s\u001b[0K\u001b[1G121.6 MiB [===                 ] 13% 7.7s\u001b[0K\u001b[1G121.6 MiB [===                 ] 14% 7.6s\u001b[0K\u001b[1G121.6 MiB [===                 ] 14% 7.4s\u001b[0K\u001b[1G121.6 MiB [===                 ] 15% 7.8s\u001b[0K\u001b[1G121.6 MiB [===                 ] 15% 7.7s\u001b[0K\u001b[1G121.6 MiB [===                 ] 16% 7.6s\u001b[0K\u001b[1G121.6 MiB [===                 ] 16% 7.5s\u001b[0K\u001b[1G121.6 MiB [===                 ] 16% 7.8s\u001b[0K\u001b[1G121.6 MiB [===                 ] 16% 7.7s\u001b[0K\u001b[1G121.6 MiB [===                 ] 17% 7.7s\u001b[0K\u001b[1G121.6 MiB [===                 ] 17% 7.6s\u001b[0K\u001b[1G121.6 MiB [====                ] 17% 7.6s\u001b[0K\u001b[1G121.6 MiB [====                ] 17% 7.5s\u001b[0K\u001b[1G121.6 MiB [====                ] 18% 7.5s\u001b[0K\u001b[1G121.6 MiB [====                ] 18% 7.4s\u001b[0K\u001b[1G121.6 MiB [====                ] 19% 7.4s\u001b[0K\u001b[1G121.6 MiB [====                ] 19% 7.5s\u001b[0K\u001b[1G121.6 MiB [====                ] 19% 7.8s\u001b[0K\u001b[1G121.6 MiB [====                ] 20% 7.7s\u001b[0K\u001b[1G121.6 MiB [====                ] 20% 7.6s\u001b[0K\u001b[1G121.6 MiB [====                ] 21% 7.5s\u001b[0K\u001b[1G121.6 MiB [====                ] 21% 7.4s\u001b[0K\u001b[1G121.6 MiB [====                ] 21% 7.3s\u001b[0K\u001b[1G121.6 MiB [====                ] 22% 7.2s\u001b[0K\u001b[1G121.6 MiB [=====               ] 22% 7.2s\u001b[0K\u001b[1G121.6 MiB [=====               ] 23% 7.1s\u001b[0K\u001b[1G121.6 MiB [=====               ] 23% 7.0s\u001b[0K\u001b[1G121.6 MiB [=====               ] 24% 6.9s\u001b[0K\u001b[1G121.6 MiB [=====               ] 24% 6.8s\u001b[0K\u001b[1G121.6 MiB [=====               ] 25% 6.7s\u001b[0K\u001b[1G121.6 MiB [=====               ] 25% 6.6s\u001b[0K\u001b[1G121.6 MiB [=====               ] 26% 6.6s\u001b[0K\u001b[1G121.6 MiB [=====               ] 26% 6.5s\u001b[0K\u001b[1G121.6 MiB [=====               ] 26% 6.7s\u001b[0K\u001b[1G121.6 MiB [=====               ] 27% 6.6s\u001b[0K\u001b[1G121.6 MiB [======              ] 27% 6.7s\u001b[0K\u001b[1G121.6 MiB [======              ] 27% 6.8s\u001b[0K\u001b[1G121.6 MiB [======              ] 27% 6.9s\u001b[0K\u001b[1G121.6 MiB [======              ] 27% 7.0s\u001b[0K\u001b[1G121.6 MiB [======              ] 27% 7.1s\u001b[0K\u001b[1G121.6 MiB [======              ] 27% 7.2s\u001b[0K\u001b[1G121.6 MiB [======              ] 28% 7.1s\u001b[0K\u001b[1G121.6 MiB [======              ] 28% 7.0s\u001b[0K\u001b[1G121.6 MiB [======              ] 29% 7.0s\u001b[0K\u001b[1G121.6 MiB [======              ] 29% 6.9s\u001b[0K\u001b[1G121.6 MiB [======              ] 29% 7.1s\u001b[0K\u001b[1G121.6 MiB [======              ] 29% 7.0s\u001b[0K\u001b[1G121.6 MiB [======              ] 30% 7.0s\u001b[0K\u001b[1G121.6 MiB [======              ] 30% 6.9s\u001b[0K\u001b[1G121.6 MiB [======              ] 31% 6.8s\u001b[0K\u001b[1G121.6 MiB [======              ] 31% 6.7s\u001b[0K\u001b[1G121.6 MiB [======              ] 32% 6.6s\u001b[0K\u001b[1G121.6 MiB [=======             ] 32% 6.6s\u001b[0K\u001b[1G121.6 MiB [=======             ] 32% 6.5s\u001b[0K\u001b[1G121.6 MiB [=======             ] 33% 6.5s\u001b[0K\u001b[1G121.6 MiB [=======             ] 33% 6.4s\u001b[0K\u001b[1G121.6 MiB [=======             ] 33% 6.3s\u001b[0K\u001b[1G121.6 MiB [=======             ] 34% 6.3s\u001b[0K\u001b[1G121.6 MiB [=======             ] 34% 6.2s\u001b[0K\u001b[1G121.6 MiB [=======             ] 35% 6.2s\u001b[0K\u001b[1G121.6 MiB [=======             ] 35% 6.1s\u001b[0K\u001b[1G121.6 MiB [=======             ] 36% 6.0s\u001b[0K\u001b[1G121.6 MiB [=======             ] 36% 6.1s\u001b[0K\u001b[1G121.6 MiB [=======             ] 36% 6.0s\u001b[0K\u001b[1G121.6 MiB [=======             ] 37% 6.0s\u001b[0K\u001b[1G121.6 MiB [========            ] 37% 5.9s\u001b[0K\u001b[1G121.6 MiB [========            ] 38% 5.9s\u001b[0K\u001b[1G121.6 MiB [========            ] 38% 5.8s\u001b[0K\u001b[1G121.6 MiB [========            ] 38% 5.7s\u001b[0K\u001b[1G121.6 MiB [========            ] 39% 5.7s\u001b[0K\u001b[1G121.6 MiB [========            ] 39% 5.6s\u001b[0K\u001b[1G121.6 MiB [========            ] 40% 5.6s\u001b[0K\u001b[1G121.6 MiB [========            ] 40% 5.5s\u001b[0K\u001b[1G121.6 MiB [========            ] 41% 5.4s\u001b[0K\u001b[1G121.6 MiB [========            ] 41% 5.3s\u001b[0K\u001b[1G121.6 MiB [========            ] 42% 5.3s\u001b[0K\u001b[1G121.6 MiB [=========           ] 42% 5.3s\u001b[0K\u001b[1G121.6 MiB [=========           ] 43% 5.3s\u001b[0K\u001b[1G121.6 MiB [=========           ] 43% 5.2s\u001b[0K\u001b[1G121.6 MiB [=========           ] 44% 5.2s\u001b[0K\u001b[1G121.6 MiB [=========           ] 44% 5.1s\u001b[0K\u001b[1G121.6 MiB [=========           ] 45% 5.1s\u001b[0K\u001b[1G121.6 MiB [=========           ] 45% 5.0s\u001b[0K\u001b[1G121.6 MiB [=========           ] 46% 4.9s\u001b[0K\u001b[1G121.6 MiB [=========           ] 46% 4.8s\u001b[0K\u001b[1G121.6 MiB [=========           ] 47% 4.8s\u001b[0K\u001b[1G121.6 MiB [==========          ] 47% 4.7s\u001b[0K\u001b[1G121.6 MiB [==========          ] 48% 4.7s\u001b[0K\u001b[1G121.6 MiB [==========          ] 48% 4.6s\u001b[0K\u001b[1G121.6 MiB [==========          ] 49% 4.6s\u001b[0K\u001b[1G121.6 MiB [==========          ] 49% 4.5s\u001b[0K\u001b[1G121.6 MiB [==========          ] 50% 4.5s\u001b[0K\u001b[1G121.6 MiB [==========          ] 51% 4.4s\u001b[0K\u001b[1G121.6 MiB [==========          ] 51% 4.3s\u001b[0K\u001b[1G121.6 MiB [==========          ] 52% 4.3s\u001b[0K\u001b[1G121.6 MiB [===========         ] 52% 4.2s\u001b[0K\u001b[1G121.6 MiB [===========         ] 53% 4.2s\u001b[0K\u001b[1G121.6 MiB [===========         ] 53% 4.1s\u001b[0K\u001b[1G121.6 MiB [===========         ] 54% 4.0s\u001b[0K\u001b[1G121.6 MiB [===========         ] 55% 3.9s\u001b[0K\u001b[1G121.6 MiB [===========         ] 56% 3.8s\u001b[0K\u001b[1G121.6 MiB [===========         ] 56% 3.7s\u001b[0K\u001b[1G121.6 MiB [===========         ] 57% 3.7s\u001b[0K\u001b[1G121.6 MiB [===========         ] 57% 3.8s\u001b[0K\u001b[1G121.6 MiB [============        ] 57% 3.8s\u001b[0K\u001b[1G121.6 MiB [============        ] 57% 3.7s\u001b[0K\u001b[1G121.6 MiB [============        ] 58% 3.7s\u001b[0K\u001b[1G121.6 MiB [============        ] 58% 3.6s\u001b[0K\u001b[1G121.6 MiB [============        ] 59% 3.6s\u001b[0K\u001b[1G121.6 MiB [============        ] 59% 3.5s\u001b[0K\u001b[1G121.6 MiB [============        ] 60% 3.5s\u001b[0K\u001b[1G121.6 MiB [============        ] 60% 3.4s\u001b[0K\u001b[1G121.6 MiB [============        ] 61% 3.3s\u001b[0K\u001b[1G121.6 MiB [============        ] 62% 3.3s\u001b[0K\u001b[1G121.6 MiB [============        ] 62% 3.2s\u001b[0K\u001b[1G121.6 MiB [=============       ] 62% 3.2s\u001b[0K\u001b[1G121.6 MiB [=============       ] 63% 3.1s\u001b[0K\u001b[1G121.6 MiB [=============       ] 64% 3.1s\u001b[0K\u001b[1G121.6 MiB [=============       ] 64% 3.0s\u001b[0K\u001b[1G121.6 MiB [=============       ] 64% 3.1s\u001b[0K\u001b[1G121.6 MiB [=============       ] 64% 3.0s\u001b[0K\u001b[1G121.6 MiB [=============       ] 65% 3.0s\u001b[0K\u001b[1G121.6 MiB [=============       ] 65% 2.9s\u001b[0K\u001b[1G121.6 MiB [=============       ] 66% 2.9s\u001b[0K\u001b[1G121.6 MiB [=============       ] 67% 2.8s\u001b[0K\u001b[1G121.6 MiB [==============      ] 67% 2.8s\u001b[0K\u001b[1G121.6 MiB [==============      ] 67% 2.7s\u001b[0K\u001b[1G121.6 MiB [==============      ] 68% 2.7s\u001b[0K\u001b[1G121.6 MiB [==============      ] 69% 2.6s\u001b[0K\u001b[1G121.6 MiB [==============      ] 69% 2.5s\u001b[0K\u001b[1G121.6 MiB [==============      ] 70% 2.5s\u001b[0K\u001b[1G121.6 MiB [==============      ] 70% 2.4s\u001b[0K\u001b[1G121.6 MiB [==============      ] 71% 2.4s\u001b[0K\u001b[1G121.6 MiB [==============      ] 72% 2.4s\u001b[0K\u001b[1G121.6 MiB [===============     ] 72% 2.3s\u001b[0K\u001b[1G121.6 MiB [===============     ] 73% 2.3s\u001b[0K\u001b[1G121.6 MiB [===============     ] 73% 2.2s\u001b[0K\u001b[1G121.6 MiB [===============     ] 74% 2.2s\u001b[0K\u001b[1G121.6 MiB [===============     ] 74% 2.1s\u001b[0K\u001b[1G121.6 MiB [===============     ] 75% 2.1s\u001b[0K\u001b[1G121.6 MiB [===============     ] 75% 2.0s\u001b[0K\u001b[1G121.6 MiB [===============     ] 76% 2.0s\u001b[0K\u001b[1G121.6 MiB [===============     ] 76% 2.1s\u001b[0K\u001b[1G121.6 MiB [===============     ] 76% 2.0s\u001b[0K\u001b[1G121.6 MiB [===============     ] 77% 2.0s\u001b[0K\u001b[1G121.6 MiB [================    ] 77% 1.9s\u001b[0K\u001b[1G121.6 MiB [================    ] 78% 1.9s\u001b[0K\u001b[1G121.6 MiB [================    ] 78% 1.8s\u001b[0K\u001b[1G121.6 MiB [================    ] 79% 1.8s\u001b[0K\u001b[1G121.6 MiB [================    ] 79% 1.7s\u001b[0K\u001b[1G121.6 MiB [================    ] 80% 1.7s\u001b[0K\u001b[1G121.6 MiB [================    ] 80% 1.6s\u001b[0K\u001b[1G121.6 MiB [================    ] 81% 1.6s\u001b[0K\u001b[1G121.6 MiB [================    ] 82% 1.5s\u001b[0K\u001b[1G121.6 MiB [=================   ] 82% 1.5s\u001b[0K\u001b[1G121.6 MiB [=================   ] 83% 1.4s\u001b[0K\u001b[1G121.6 MiB [=================   ] 84% 1.4s\u001b[0K\u001b[1G121.6 MiB [=================   ] 84% 1.3s\u001b[0K\u001b[1G121.6 MiB [=================   ] 85% 1.3s\u001b[0K\u001b[1G121.6 MiB [=================   ] 85% 1.2s\u001b[0K\u001b[1G121.6 MiB [=================   ] 86% 1.2s\u001b[0K\u001b[1G121.6 MiB [=================   ] 86% 1.1s\u001b[0K\u001b[1G121.6 MiB [=================   ] 87% 1.1s\u001b[0K\u001b[1G121.6 MiB [==================  ] 87% 1.0s\u001b[0K\u001b[1G121.6 MiB [==================  ] 88% 1.0s\u001b[0K\u001b[1G121.6 MiB [==================  ] 88% 0.9s\u001b[0K\u001b[1G121.6 MiB [==================  ] 89% 0.9s\u001b[0K\u001b[1G121.6 MiB [==================  ] 90% 0.8s\u001b[0K\u001b[1G121.6 MiB [==================  ] 91% 0.8s\u001b[0K\u001b[1G121.6 MiB [==================  ] 91% 0.7s\u001b[0K\u001b[1G121.6 MiB [=================== ] 92% 0.6s\u001b[0K\u001b[1G121.6 MiB [=================== ] 93% 0.6s\u001b[0K\u001b[1G121.6 MiB [=================== ] 93% 0.5s\u001b[0K\u001b[1G121.6 MiB [=================== ] 94% 0.5s\u001b[0K\u001b[1G121.6 MiB [=================== ] 94% 0.4s\u001b[0K\u001b[1G121.6 MiB [=================== ] 95% 0.4s\u001b[0K\u001b[1G121.6 MiB [=================== ] 95% 0.3s\u001b[0K\u001b[1G121.6 MiB [=================== ] 96% 0.3s\u001b[0K\u001b[1G121.6 MiB [=================== ] 97% 0.2s\u001b[0K\u001b[1G121.6 MiB [====================] 97% 0.2s\u001b[0K\u001b[1G121.6 MiB [====================] 98% 0.2s\u001b[0K\u001b[1G121.6 MiB [====================] 98% 0.1s\u001b[0K\u001b[1G121.6 MiB [====================] 99% 0.1s\u001b[0K\u001b[1G121.6 MiB [====================] 99% 0.0s\u001b[0K\u001b[1G121.6 MiB [====================] 100% 0.0s\u001b[0K\n",
      "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /Users/HAI5RT/Library/Caches/ms-playwright/chromium-1148\n",
      "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-mac-arm64.zip\u001b[22m\n",
      "\u001b[1G77.5 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G77.5 MiB [                    ] 0% 28.4s\u001b[0K\u001b[1G77.5 MiB [                    ] 0% 13.0s\u001b[0K\u001b[1G77.5 MiB [                    ] 0% 9.9s\u001b[0K\u001b[1G77.5 MiB [                    ] 0% 10.5s\u001b[0K\u001b[1G77.5 MiB [                    ] 0% 12.6s\u001b[0K\u001b[1G77.5 MiB [                    ] 1% 7.5s\u001b[0K\u001b[1G77.5 MiB [                    ] 2% 6.9s\u001b[0K\u001b[1G77.5 MiB [                    ] 2% 6.4s\u001b[0K\u001b[1G77.5 MiB [=                   ] 2% 6.1s\u001b[0K\u001b[1G77.5 MiB [=                   ] 3% 5.8s\u001b[0K\u001b[1G77.5 MiB [=                   ] 3% 5.7s\u001b[0K\u001b[1G77.5 MiB [=                   ] 3% 5.6s\u001b[0K\u001b[1G77.5 MiB [=                   ] 4% 5.5s\u001b[0K\u001b[1G77.5 MiB [=                   ] 4% 5.3s\u001b[0K\u001b[1G77.5 MiB [=                   ] 5% 5.2s\u001b[0K\u001b[1G77.5 MiB [=                   ] 5% 5.1s\u001b[0K\u001b[1G77.5 MiB [=                   ] 6% 5.0s\u001b[0K\u001b[1G77.5 MiB [=                   ] 6% 4.9s\u001b[0K\u001b[1G77.5 MiB [=                   ] 7% 6.1s\u001b[0K\u001b[1G77.5 MiB [==                  ] 7% 5.9s\u001b[0K\u001b[1G77.5 MiB [==                  ] 7% 6.0s\u001b[0K\u001b[1G77.5 MiB [==                  ] 8% 6.0s\u001b[0K\u001b[1G77.5 MiB [==                  ] 8% 5.9s\u001b[0K\u001b[1G77.5 MiB [==                  ] 8% 5.7s\u001b[0K\u001b[1G77.5 MiB [==                  ] 9% 5.6s\u001b[0K\u001b[1G77.5 MiB [==                  ] 9% 5.5s\u001b[0K\u001b[1G77.5 MiB [==                  ] 9% 5.4s\u001b[0K\u001b[1G77.5 MiB [==                  ] 10% 5.3s\u001b[0K\u001b[1G77.5 MiB [==                  ] 10% 5.2s\u001b[0K\u001b[1G77.5 MiB [==                  ] 11% 5.2s\u001b[0K\u001b[1G77.5 MiB [==                  ] 11% 5.1s\u001b[0K\u001b[1G77.5 MiB [==                  ] 11% 5.0s\u001b[0K\u001b[1G77.5 MiB [==                  ] 12% 5.0s\u001b[0K\u001b[1G77.5 MiB [===                 ] 12% 5.0s\u001b[0K\u001b[1G77.5 MiB [===                 ] 12% 4.9s\u001b[0K\u001b[1G77.5 MiB [===                 ] 13% 4.9s\u001b[0K\u001b[1G77.5 MiB [===                 ] 14% 4.8s\u001b[0K\u001b[1G77.5 MiB [===                 ] 14% 4.7s\u001b[0K\u001b[1G77.5 MiB [===                 ] 15% 4.7s\u001b[0K\u001b[1G77.5 MiB [===                 ] 15% 4.6s\u001b[0K\u001b[1G77.5 MiB [===                 ] 16% 4.5s\u001b[0K\u001b[1G77.5 MiB [===                 ] 16% 4.9s\u001b[0K\u001b[1G77.5 MiB [===                 ] 17% 4.9s\u001b[0K\u001b[1G77.5 MiB [====                ] 17% 4.9s\u001b[0K\u001b[1G77.5 MiB [====                ] 18% 4.8s\u001b[0K\u001b[1G77.5 MiB [====                ] 18% 4.7s\u001b[0K\u001b[1G77.5 MiB [====                ] 19% 4.7s\u001b[0K\u001b[1G77.5 MiB [====                ] 19% 4.6s\u001b[0K\u001b[1G77.5 MiB [====                ] 20% 4.5s\u001b[0K\u001b[1G77.5 MiB [====                ] 21% 4.4s\u001b[0K\u001b[1G77.5 MiB [====                ] 21% 4.5s\u001b[0K\u001b[1G77.5 MiB [====                ] 21% 4.4s\u001b[0K\u001b[1G77.5 MiB [====                ] 22% 4.4s\u001b[0K\u001b[1G77.5 MiB [=====               ] 22% 4.3s\u001b[0K\u001b[1G77.5 MiB [=====               ] 23% 4.3s\u001b[0K\u001b[1G77.5 MiB [=====               ] 23% 4.2s\u001b[0K\u001b[1G77.5 MiB [=====               ] 24% 4.2s\u001b[0K\u001b[1G77.5 MiB [=====               ] 24% 4.1s\u001b[0K\u001b[1G77.5 MiB [=====               ] 25% 4.1s\u001b[0K\u001b[1G77.5 MiB [=====               ] 25% 4.0s\u001b[0K\u001b[1G77.5 MiB [=====               ] 26% 4.0s\u001b[0K\u001b[1G77.5 MiB [=====               ] 26% 4.2s\u001b[0K\u001b[1G77.5 MiB [=====               ] 27% 4.2s\u001b[0K\u001b[1G77.5 MiB [======              ] 27% 4.1s\u001b[0K\u001b[1G77.5 MiB [======              ] 28% 4.1s\u001b[0K\u001b[1G77.5 MiB [======              ] 28% 4.0s\u001b[0K\u001b[1G77.5 MiB [======              ] 29% 4.0s\u001b[0K\u001b[1G77.5 MiB [======              ] 29% 3.9s\u001b[0K\u001b[1G77.5 MiB [======              ] 30% 3.9s\u001b[0K\u001b[1G77.5 MiB [======              ] 30% 3.8s\u001b[0K\u001b[1G77.5 MiB [======              ] 31% 3.8s\u001b[0K\u001b[1G77.5 MiB [======              ] 31% 3.7s\u001b[0K\u001b[1G77.5 MiB [======              ] 32% 3.7s\u001b[0K\u001b[1G77.5 MiB [=======             ] 32% 3.7s\u001b[0K\u001b[1G77.5 MiB [=======             ] 33% 3.6s\u001b[0K\u001b[1G77.5 MiB [=======             ] 34% 3.6s\u001b[0K\u001b[1G77.5 MiB [=======             ] 34% 3.5s\u001b[0K\u001b[1G77.5 MiB [=======             ] 35% 3.5s\u001b[0K\u001b[1G77.5 MiB [=======             ] 36% 3.4s\u001b[0K\u001b[1G77.5 MiB [=======             ] 36% 3.6s\u001b[0K\u001b[1G77.5 MiB [=======             ] 36% 3.5s\u001b[0K\u001b[1G77.5 MiB [=======             ] 37% 3.5s\u001b[0K\u001b[1G77.5 MiB [========            ] 37% 3.5s\u001b[0K\u001b[1G77.5 MiB [========            ] 38% 3.5s\u001b[0K\u001b[1G77.5 MiB [========            ] 38% 3.4s\u001b[0K\u001b[1G77.5 MiB [========            ] 39% 3.4s\u001b[0K\u001b[1G77.5 MiB [========            ] 39% 3.3s\u001b[0K\u001b[1G77.5 MiB [========            ] 40% 3.3s\u001b[0K\u001b[1G77.5 MiB [========            ] 40% 3.2s\u001b[0K\u001b[1G77.5 MiB [========            ] 41% 3.2s\u001b[0K\u001b[1G77.5 MiB [========            ] 42% 3.1s\u001b[0K\u001b[1G77.5 MiB [=========           ] 42% 3.1s\u001b[0K\u001b[1G77.5 MiB [=========           ] 43% 3.1s\u001b[0K\u001b[1G77.5 MiB [=========           ] 43% 3.0s\u001b[0K\u001b[1G77.5 MiB [=========           ] 44% 3.0s\u001b[0K\u001b[1G77.5 MiB [=========           ] 45% 3.0s\u001b[0K\u001b[1G77.5 MiB [=========           ] 45% 2.9s\u001b[0K\u001b[1G77.5 MiB [=========           ] 46% 2.9s\u001b[0K\u001b[1G77.5 MiB [=========           ] 46% 3.0s\u001b[0K\u001b[1G77.5 MiB [=========           ] 46% 2.9s\u001b[0K\u001b[1G77.5 MiB [=========           ] 47% 2.9s\u001b[0K\u001b[1G77.5 MiB [==========          ] 47% 2.9s\u001b[0K\u001b[1G77.5 MiB [==========          ] 48% 2.9s\u001b[0K\u001b[1G77.5 MiB [==========          ] 48% 2.8s\u001b[0K\u001b[1G77.5 MiB [==========          ] 49% 2.8s\u001b[0K\u001b[1G77.5 MiB [==========          ] 49% 2.7s\u001b[0K\u001b[1G77.5 MiB [==========          ] 50% 2.7s\u001b[0K\u001b[1G77.5 MiB [==========          ] 51% 2.7s\u001b[0K\u001b[1G77.5 MiB [==========          ] 51% 2.6s\u001b[0K\u001b[1G77.5 MiB [==========          ] 52% 2.6s\u001b[0K\u001b[1G77.5 MiB [===========         ] 52% 2.5s\u001b[0K\u001b[1G77.5 MiB [===========         ] 53% 2.5s\u001b[0K\u001b[1G77.5 MiB [===========         ] 54% 2.5s\u001b[0K\u001b[1G77.5 MiB [===========         ] 54% 2.4s\u001b[0K\u001b[1G77.5 MiB [===========         ] 55% 2.4s\u001b[0K\u001b[1G77.5 MiB [===========         ] 55% 2.3s\u001b[0K\u001b[1G77.5 MiB [===========         ] 56% 2.4s\u001b[0K\u001b[1G77.5 MiB [===========         ] 57% 2.4s\u001b[0K\u001b[1G77.5 MiB [============        ] 57% 2.3s\u001b[0K\u001b[1G77.5 MiB [============        ] 58% 2.3s\u001b[0K\u001b[1G77.5 MiB [============        ] 58% 2.2s\u001b[0K\u001b[1G77.5 MiB [============        ] 59% 2.2s\u001b[0K\u001b[1G77.5 MiB [============        ] 60% 2.2s\u001b[0K\u001b[1G77.5 MiB [============        ] 60% 2.1s\u001b[0K\u001b[1G77.5 MiB [============        ] 61% 2.1s\u001b[0K\u001b[1G77.5 MiB [=============       ] 62% 2.0s\u001b[0K\u001b[1G77.5 MiB [=============       ] 63% 2.0s\u001b[0K\u001b[1G77.5 MiB [=============       ] 63% 1.9s\u001b[0K\u001b[1G77.5 MiB [=============       ] 64% 1.9s\u001b[0K\u001b[1G77.5 MiB [=============       ] 65% 1.9s\u001b[0K\u001b[1G77.5 MiB [=============       ] 65% 1.8s\u001b[0K\u001b[1G77.5 MiB [=============       ] 65% 1.9s\u001b[0K\u001b[1G77.5 MiB [=============       ] 66% 1.9s\u001b[0K\u001b[1G77.5 MiB [=============       ] 66% 1.8s\u001b[0K\u001b[1G77.5 MiB [=============       ] 67% 1.8s\u001b[0K\u001b[1G77.5 MiB [==============      ] 67% 1.8s\u001b[0K\u001b[1G77.5 MiB [==============      ] 68% 1.7s\u001b[0K\u001b[1G77.5 MiB [==============      ] 69% 1.7s\u001b[0K\u001b[1G77.5 MiB [==============      ] 70% 1.6s\u001b[0K\u001b[1G77.5 MiB [==============      ] 71% 1.6s\u001b[0K\u001b[1G77.5 MiB [==============      ] 71% 1.7s\u001b[0K\u001b[1G77.5 MiB [==============      ] 72% 1.7s\u001b[0K\u001b[1G77.5 MiB [===============     ] 72% 1.7s\u001b[0K\u001b[1G77.5 MiB [===============     ] 72% 1.6s\u001b[0K\u001b[1G77.5 MiB [===============     ] 73% 1.6s\u001b[0K\u001b[1G77.5 MiB [===============     ] 74% 1.6s\u001b[0K\u001b[1G77.5 MiB [===============     ] 74% 1.5s\u001b[0K\u001b[1G77.5 MiB [===============     ] 75% 1.5s\u001b[0K\u001b[1G77.5 MiB [===============     ] 76% 1.5s\u001b[0K\u001b[1G77.5 MiB [===============     ] 76% 1.4s\u001b[0K\u001b[1G77.5 MiB [===============     ] 77% 1.4s\u001b[0K\u001b[1G77.5 MiB [================    ] 77% 1.4s\u001b[0K\u001b[1G77.5 MiB [================    ] 77% 1.3s\u001b[0K\u001b[1G77.5 MiB [================    ] 78% 1.3s\u001b[0K\u001b[1G77.5 MiB [================    ] 79% 1.3s\u001b[0K\u001b[1G77.5 MiB [================    ] 79% 1.2s\u001b[0K\u001b[1G77.5 MiB [================    ] 80% 1.2s\u001b[0K\u001b[1G77.5 MiB [================    ] 81% 1.1s\u001b[0K\u001b[1G77.5 MiB [================    ] 82% 1.1s\u001b[0K\u001b[1G77.5 MiB [=================   ] 82% 1.0s\u001b[0K\u001b[1G77.5 MiB [=================   ] 83% 1.0s\u001b[0K\u001b[1G77.5 MiB [=================   ] 84% 0.9s\u001b[0K\u001b[1G77.5 MiB [=================   ] 85% 0.9s\u001b[0K\u001b[1G77.5 MiB [=================   ] 86% 0.8s\u001b[0K\u001b[1G77.5 MiB [=================   ] 87% 0.8s\u001b[0K\u001b[1G77.5 MiB [==================  ] 87% 0.8s\u001b[0K\u001b[1G77.5 MiB [==================  ] 87% 0.7s\u001b[0K\u001b[1G77.5 MiB [==================  ] 88% 0.7s\u001b[0K\u001b[1G77.5 MiB [==================  ] 89% 0.6s\u001b[0K\u001b[1G77.5 MiB [==================  ] 90% 0.6s\u001b[0K\u001b[1G77.5 MiB [==================  ] 91% 0.5s\u001b[0K\u001b[1G77.5 MiB [==================  ] 92% 0.5s\u001b[0K\u001b[1G77.5 MiB [=================== ] 92% 0.4s\u001b[0K\u001b[1G77.5 MiB [=================== ] 93% 0.4s\u001b[0K\u001b[1G77.5 MiB [=================== ] 94% 0.3s\u001b[0K\u001b[1G77.5 MiB [=================== ] 95% 0.3s\u001b[0K\u001b[1G77.5 MiB [=================== ] 95% 0.2s\u001b[0K\u001b[1G77.5 MiB [=================== ] 96% 0.2s\u001b[0K\u001b[1G77.5 MiB [=================== ] 97% 0.2s\u001b[0K\u001b[1G77.5 MiB [====================] 97% 0.1s\u001b[0K\u001b[1G77.5 MiB [====================] 98% 0.1s\u001b[0K\u001b[1G77.5 MiB [====================] 99% 0.0s\u001b[0K\u001b[1G77.5 MiB [====================] 100% 0.0s\u001b[0K\n",
      "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /Users/HAI5RT/Library/Caches/ms-playwright/chromium_headless_shell-1148\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 555368 bytes to TRUNCATED_3_Character_Level_GPT__student.pdf\n"
     ]
    }
   ],
   "source": [
    "# This cell truncates long output to a maximum length, then converts the notebook to a PDF\n",
    "# NOTE: You may have to adapt the path or filename to match your local setup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# truncate long cell output to avoid large pdf files\n",
    "from helpers.truncate_output import truncate_long_notebook_output\n",
    "truncated = truncate_long_notebook_output('3_Character_Level_GPT__student.ipynb')\n",
    "\n",
    "# convert to pdf with nbconvert\n",
    "if truncated:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download TRUNCATED_3_Character_Level_GPT__student.ipynb\n",
    "else:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download \"3_Character_Level_GPT__student.ipynb\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
