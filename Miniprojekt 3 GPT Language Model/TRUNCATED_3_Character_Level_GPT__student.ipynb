{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Building a GPT from Scratch\n",
    "---\n",
    "\n",
    "This is an extended version of Andrej Karpathy's notebook in addition to his [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.\n",
    "\n",
    "Adapted by: \n",
    "\n",
    "Prof. Dr.-Ing. Antje Muntzinger, University of Applied Sciences Stuttgart\n",
    "\n",
    "antje.muntzinger@hft-stuttgart.de\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We'll construct a character-level **GPT (Generative Pretrained Transformer)** model from scratch. **Transformer** is the name of the underlying neural net architecture that was introduced in the 2017 groundbreaking paper \"Attention is All You Need\" (Link at the bottom).\n",
    "The model will be trained on different texts, for example Shakespeare, Goethe's \"Faust\", the \"Lord of the Rings\" or books from Jane Austen, and will be able to generate new text based on the text from the book.\n",
    "\n",
    "\n",
    "**NOTE:** You may answer in English or German.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "[1. Loading the Data](#1.-Loading-the-Data)\n",
    "\n",
    "[2. Tokenization](#2.-Tokenization)\n",
    "\n",
    "[3. Making Training Mini-Batches](#3.-Making-Training-Mini-Batches)\n",
    "\n",
    "[4. Defining the Network with PyTorch](#4.-Defining-the-Network-with-PyTorch)\n",
    "\n",
    "[5. Training](#5.-Training)\n",
    "\n",
    "[6. The Mathematical Trick in Self-Attention](#6.-The-Mathematical-Trick-in-Self-Attention)\n",
    "\n",
    "[7. Self-Attention](#7.-Self-Attention)\n",
    "\n",
    "[8. Full GPT Implementation](#9.-Full-GPT-Implementation)\n",
    "\n",
    "[9. Outlook and Next Steps](#9.-Outlook-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x126a1f810>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# select the right file and read it in to inspect it\n",
    "with open('faust.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('austen.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('LOTR.txt', 'r') as f:\n",
    "# with open('LOTR_TVscript.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1a) Find out the length of the dataset and print the first 1000 characters! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  205807\n",
      "﻿Faust:\n",
      "Der Tragödie erster Teil\n",
      "\n",
      "by Johann Wolfgang von Goethe\n",
      "\n",
      "\n",
      "Zueignung\n",
      "\n",
      "\n",
      "Ihr naht euch wieder, schwankende Gestalten,\n",
      "Die früh sich einst dem trüben Blick gezeigt.\n",
      "Versuch ich wohl, euch diesmal festzuhalten?\n",
      "Fühl ich mein Herz noch jenem Wahn geneigt?\n",
      "Ihr drängt euch zu! nun gut, so mögt ihr walten,\n",
      "Wie ihr aus Dunst und Nebel um mich steigt;\n",
      "Mein Busen fühlt sich jugendlich erschüttert\n",
      "Vom Zauberhauch, der euren Zug umwittert.\n",
      "\n",
      "Ihr bringt mit euch die Bilder froher Tage,\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1b) Store all unique characters that occur in this text in `chars` and print them. Store the number of unique characters in `vocab_size` and print the result. **(3 points)**\n",
    "\n",
    "**Hint:** First make a set of all characters to remove duplicates, then make a list out of them to get a unique ordering, and finally sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$%()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÄÖÜßäöü—‘’“”•™﻿\n",
      "vocab_size= 92\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size=', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to **tokenize** the input. This means, we convert the raw text string to some sequence of integers according to some **vocabulary** of possible elements. A **token** can be a character like here, or a piece of a word like in ChatGPT. For a character-level language model, we just translate each character to an integer (**encoding**) and vice-versa (**decoding**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '%', 5: '(', 6: ')', 7: '*', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '?', 25: 'A', 26: 'B', 27: 'C', 28: 'D', 29: 'E', 30: 'F', 31: 'G', 32: 'H', 33: 'I', 34: 'J', 35: 'K', 36: 'L', 37: 'M', 38: 'N', 39: 'O', 40: 'P', 41: 'Q', 42: 'R', 43: 'S', 44: 'T', 45: 'U', 46: 'V', 47: 'W', 48: 'X', 49: 'Y', 50: 'Z', 51: 'a', 52: 'b', 53: 'c', 54: 'd', 55: 'e', 56: 'f', 57: 'g', 58: 'h', 59: 'i', 60: 'j', 61: 'k', 62: 'l', 63: 'm', 64: 'n', 65: 'o', 66: 'p', 67: 'q', 68: 'r', 69: 's', 70: 't', 71: 'u', 72: 'v', 73: 'w', 74: 'x', 75: 'y', 76: 'z', 77: 'Ä', 78: 'Ö', 79: 'Ü', 80: 'ß', 81: 'ä', 82: 'ö', 83: 'ü', 84: '—', 85: '‘', 86: '’', 87: '“', 88: '”', 89: '•', 90: '™', 91: '\\ufeff'}\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2a) Test the code above by encoding some sentence of your choice and decoding it again. Print the encoded and decoded result. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [28, 59, 55, 69, 1, 59, 69, 70, 1, 64, 71, 68, 1, 55, 59, 64, 1, 44, 55, 69, 70, 10, 1, 26, 59, 70, 70, 55, 1, 57, 55, 58, 55, 64, 1, 43, 59, 55, 1, 73, 55, 59, 70, 55, 68, 8, 1, 55, 69, 1, 57, 59, 52, 70, 1, 64, 59, 53, 58, 70, 69, 1, 76, 71, 1, 69, 55, 58, 55, 64, 2]\n",
      "Decoded: Dies ist nur ein Test. Bitte gehen Sie weiter, es gibt nichts zu sehen!\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "sentence = \"Dies ist nur ein Test. Bitte gehen Sie weiter, es gibt nichts zu sehen!\"\n",
    "\n",
    "encoded_test_sentence = encode(sentence)\n",
    "print(\"Encoded:\", encoded_test_sentence)\n",
    "\n",
    "decoded_test_sentence = decode(encoded_test_sentence)\n",
    "print(\"Decoded:\", decoded_test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tokenization is a trade-off between vocabulary size and sequence length: Large vocabularies will lead to shorter encoding sequences and vice versa. For example, encoding each character results in a short vocabulary of 26 tokens for the standard alphabet plus some more for special characters, but each word consists of longer encodings. On the other hand, encoding on word level means each word is encoded as a single token, but the vocabulary will be much larger (up to a whole dictionary of hundreds of thousands of words for one language). In practice, for example in ChatGPT, **sub word encodings** are used, which means not encoding entire words, but also not encoding individual characters. Instead, some intermediate format is used, for example the word 'undefined' could be encoded as three tokens: 'un', 'define', 'd'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2b) Encode the entire text dataset and store it into a `torch.tensor` with `dtype=torch.long`. This will be our input data for the model, and we name it `data`. \n",
    "Print the shape and dtype of `data` and the first 1000 characters of the encoded text for comparison with the text above. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: torch.Size([205807])\n",
      "Dtype of data: torch.int64\n",
      "First 1000 encoded characters: tensor([91, 30, 51, 71, 69, 70, 22,  0, 28, 55, 68,  1, 44, 68, 51, 57, 82, 54,\n",
      "        59, 55,  1, 55, 68, 69, 70, 55, 68,  1, 44, 55, 59, 62,  0,  0, 52, 75,\n",
      "         1, 34, 65, 58, 51, 64, 64,  1, 47, 65, 62, 56, 57, 51, 64, 57,  1, 72,\n",
      "        65, 64,  1, 31, 65, 55, 70, 58, 55,  0,  0,  0, 50, 71, 55, 59, 57, 64,\n",
      "        71, 64, 57,  0,  0,  0, 33, 58, 68,  1, 64, 51, 58, 70,  1, 55, 71, 53,\n",
      "        58,  1, 73, 59, 55, 54, 55, 68,  8,  1, 69, 53, 58, 73, 51, 64, 61, 55,\n",
      "        64, 54, 55,  1, 31, 55, 69, 70, 51, 62, 70, 55, 64,  8,  0, 28, 59, 55,\n",
      "         1, 56, 68, 83, 58,  1, 69, 59, 53, 58,  1, 55, 59, 64, 69, 70,  1, 54,\n",
      "        55, 63,  1, 70, 68, 83, 52, 55, 64,  1, 26, 62, 59, 53, 61,  1, 57, 55,\n",
      "        76, 55, 59, 57, 70, 10,  0, 46, 55, 68, 69, 71, 53, 58,  1, 59, 53, 58,\n",
      "         1, 73, 65, 58, 62,  8,  1, 55, 71, 53, 58,  1, 54, 59, 55, 69, 63, 51,\n",
      "        62,  1, 56, 55, 69, 70, 76, 71, 58, 51, 62, 70, 55, 64, 24,  0, 30, 83,\n",
      "        58, 62,  1, 59, 53, 58,  1, 63, 55, 59, 64,  1, 32, 55, 68, 76,  1, 64,\n",
      "        65, 53, 58,  1, 60, 55, 64, 55, 63,  1, 47, 51, 58, 64,  1, 57, 55, 64,\n",
      "        55, 59, 57, 70, 24,  0, 33, 58, 68,  1, 54, 68, 81, 64, 57, 70,  1, 55,\n",
      "        71, 53, 58,  1, 76, 71,  2,  1, 64, 71, 64,  1, 57, 71, 70,  8,  1, 69,\n",
      "        65,  1, 63, 82, 57, 70,  1, 59, 58, 68,  1, 73, 51, 62, 70, 55, 64,  8,\n",
      "         0, 47, 59, 55,  1, 59, 58, 68,  1, 51, 71, 69,  1, 28, 71, 64, 69, 70,\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Store the encoded text in a torch.tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"Shape of data:\", data.shape)\n",
    "print(\"Dtype of data:\", data.dtype)\n",
    "\n",
    "print(\"First 1000 encoded characters:\", data[:1000])\n",
    "\n",
    "#print(\"First 1000 decoded characters\", decode(data.tolist())[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Making Training Mini-Batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3a) Split the data into 90% training and 10% validation data and store the result in `train_data` and `val_data`, respectively. We keep the validation data to detect overfitting: We don't want just a perfect memorization of this exact input text, we want a neural network that creates new text in a similar style. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training data: 185226\n",
      "Length of the validation data: 20581\n",
      "Check against total length; must be equal 0: (len(data) - len(train_data)-len(val_data)): 0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "train_data = data[:int(len(data)*0.9)]\n",
    "print (\"Length of the training data:\",len(train_data))\n",
    "\n",
    "val_data = data[int(len(data)*0.9):]\n",
    "print (\"Length of the validation data:\",len(val_data))\n",
    "\n",
    "print(\"Check against total length; must be equal 0: (len(data) - len(train_data)-len(val_data)):\", len(data)-len(train_data)-len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only feed in chunks of data of size 8 here: feeding in all text at once is computationally too expensive. This is called the **block size** or **context length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([91, 30, 51, 71, 69, 70, 22,  0, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1] # +1 because the target is the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this `train_data` chunk of 9 characters, 8 training examples are hidden. Let's spell it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([91]) the target is: 30\n",
      "when input is tensor([91, 30]) the target is: 51\n",
      "when input is tensor([91, 30, 51]) the target is: 71\n",
      "when input is tensor([91, 30, 51, 71]) the target is: 69\n",
      "when input is tensor([91, 30, 51, 71, 69]) the target is: 70\n",
      "when input is tensor([91, 30, 51, 71, 69, 70]) the target is: 22\n",
      "when input is tensor([91, 30, 51, 71, 69, 70, 22]) the target is: 0\n",
      "when input is tensor([91, 30, 51, 71, 69, 70, 22,  0]) the target is: 28\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # this will be the input\n",
    "y = train_data[1:block_size+1] # this will be the target\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides efficiency, a second reason to feed in chunks of size `block_size` is to make the Transformer be used to seeing contexts of different lengths, from only 1 token all the way up to `block_size` and every length in between. That is going to be useful later during inference because while we're sampling, we can start the sampling generation with as little as one character of context and the Transformer knows how to predict the next character. Then it can predict everything up to `block_size`. After `block_size`, we have to start truncating because the Transformer will never receive more than block size inputs when it's predicting the next character.\n",
    "\n",
    "Besides the **time dimension** that we have just looked at, there is also the **batch dimension**: We feed in batches of multiple chunks of text that are all stacked up in a single tensor. This is simply done for efficiency, because the GPUs can process these batches in parallel.\n",
    "\n",
    "Now let's create random **batches** of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 4 (=batch_size) random offsets into training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack 4 chunks (4x8 tensor) as rows in a minibatch\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # y is the same but one ahead (shifted 1 position to the right)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3b) Get a batch of training data and store the inputs and targets in `xb` and `yb`, respectively. Print the results and their shapes. **(2 points)** \n",
    "\n",
    "**HINT:** Apply the `get_batch()` function above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eingaben (xb): tensor([[55,  1, 52, 62, 83, 58, 70, 23],\n",
      "        [73, 59, 55, 54, 55, 68, 61, 55],\n",
      "        [56, 51, 62, 70,  8,  1, 54, 51],\n",
      "        [37, 25, 42, 44, 32, 29, 10,  0]])\n",
      "Form von xb: torch.Size([4, 8])\n",
      "Ziele (yb): tensor([[ 1, 52, 62, 83, 58, 70, 23,  0],\n",
      "        [59, 55, 54, 55, 68, 61, 55, 58],\n",
      "        [51, 62, 70,  8,  1, 54, 51, 80],\n",
      "        [25, 42, 44, 32, 29, 10,  0, 32]])\n",
      "Form von yb: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# get a batch from training data\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# Drucken Sie die Ergebnisse und deren Formen\n",
    "print(\"Eingaben (xb):\", xb)\n",
    "print(\"Form von xb:\", xb.shape)\n",
    "print(\"Ziele (yb):\", yb)\n",
    "print(\"Form von yb:\", yb.shape)\n",
    "\n",
    "#decoded_xb = [decode(x.tolist()) for x in xb]\n",
    "#print(\"Decoded xb:\", decoded_xb)\n",
    "#decoded_yb = [decode(y.tolist()) for y in yb]\n",
    "#print(\"Decoded yb:\", decoded_yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3c) How many independent training examples for the transformer does this batch contain? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**ANSWER:** \n",
    "\n",
    "Each batch contains `batch_size` independent sequences, and each sequence has `block_size` positions. Therefore, the total number of independent training examples in the batch is `batch_size * block_size` which is `4 * 8 = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [55] the target is: 1\n",
      "when input is [55, 1] the target is: 52\n",
      "when input is [55, 1, 52] the target is: 62\n",
      "when input is [55, 1, 52, 62] the target is: 83\n",
      "when input is [55, 1, 52, 62, 83] the target is: 58\n",
      "when input is [55, 1, 52, 62, 83, 58] the target is: 70\n",
      "when input is [55, 1, 52, 62, 83, 58, 70] the target is: 23\n",
      "when input is [55, 1, 52, 62, 83, 58, 70, 23] the target is: 0\n",
      "when input is [73] the target is: 59\n",
      "when input is [73, 59] the target is: 55\n",
      "when input is [73, 59, 55] the target is: 54\n",
      "when input is [73, 59, 55, 54] the target is: 55\n",
      "when input is [73, 59, 55, 54, 55] the target is: 68\n",
      "when input is [73, 59, 55, 54, 55, 68] the target is: 61\n",
      "when input is [73, 59, 55, 54, 55, 68, 61] the target is: 55\n",
      "when input is [73, 59, 55, 54, 55, 68, 61, 55] the target is: 58\n",
      "when input is [56] the target is: 51\n",
      "when input is [56, 51] the target is: 62\n",
      "when input is [56, 51, 62] the target is: 70\n",
      "when input is [56, 51, 62, 70] the target is: 8\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3d) Why do the targets look like this, where does the structure come from? What do we input to the transformer? **(2 points)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "The input to the transformer is the input context `xb`. The transformer uses this context to predict the next character in the sequence, which is the target `yb`. During training, the model learns to predict the next character based on the given context `xb`.\n",
    "\n",
    "For each position in the sequence, the input context is a slice of the sequence up to that position. For example, if the sequence is `[64, 1, 69, 59, 55, 52, 55, 64]`, the input contexts for each position is:\n",
    "\n",
    "`[64]`<br>\n",
    "`[64, 1]`<br>\n",
    "`[64, 1, 69]`<br>\n",
    "`[64, 1, 69, 59]`<br>\n",
    "`[64, 1, 69, 59, 55]`<br>\n",
    "`[64, 1, 69, 59, 55, 52]`<br>\n",
    "`[64, 1, 69, 59, 55, 52, 55]`<br>\n",
    "`[64, 1, 69, 59, 55, 52, 55, 64]`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Defining the Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple bigram language model to start with, i.e., the model predicts the next character simply on the last character. This bigram model should look familiar from our first notebook! Only now, we implement a bigram model class inheriting from `nn.Module` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 92])\n",
      "loss= tensor(4.7823, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated text: \n",
      "\n",
      "z8Öü)P*cXTYgd$”JCqÄ™n’-“n Gm—o—wA.X!$;jQwßJZÄb“1”eqöxSGvuqreGLjö1c.*’U9MpS9zGNSA)IL3E” *ä“i*uBüjn•gp\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module): # subclass of nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # e.g. if the input is token 5, the output should be the logits for all tokens at position 6 \n",
    "        # = the 5th row of the embedding table (see makemore video on bigram language model)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None): # targets are optional during inference\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # pluck out the embeddings for the tokens in the input (=the row of the embedding table corresponding to its index) and interpret them as logits=scores\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) batch size=4, time=8, channels=vocab_size because we are predicting the probability of each token (vocab_size C) at each time step (block_size T) in each batch (batch_size B)\n",
    "\n",
    "        # if we have targets, compute the CE loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # need to reshape for CE-loss in PyTorch \n",
    "            # (see https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "            targets = targets.view(B*T) # same shape as logits\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions (ignore the loss because we don't have targets)\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step = prediction for the next token\n",
    "            logits = logits[:, -1, :] # becomes (B, C) instead of (B, T, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) because we sample one token at a time for each batch\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print('loss=', loss) \n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # start with a single token = 0 (idx = current context)\n",
    "\n",
    "print(\"\\nGenerated text: \")\n",
    "# generate operates on batch level -> index into the 0th row = single batch dimension that exists -> one-dimensional array of all the indices (time steps)\n",
    "# afterwards convert to simple python list from tensor for decode function\n",
    "print(decode(model.generate(idx, max_new_tokens=100)[0].tolist())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4a) Go through the class definition above and explain what each function does! (1-2 sentences per function) **(6 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "__init__ Method: \n",
    "- **Purpose**: Initializes the model.\n",
    "- **Parameters**: vocab_size - the size of the vocabulary.\n",
    "- **Functionality**: Creates an embedding table where each token directly reads off the logits for the next token from a lookup table. The embedding table has dimensions (vocab_size, vocab_size).\n",
    "\n",
    "__forward__ Method:\n",
    "- **Purpose**: Defines the forward pass of the model.\n",
    "- **Parameters**:\n",
    "  - idx: Input tensor of shape (B, T) where B is the batch size and T is the sequence length.\n",
    "  - targets: Optional tensor of the same shape as idx, used to compute the loss during training.\n",
    "- **Functionality**:\n",
    "  - Retrieves the logits for the input tokens from the embedding table.\n",
    "  - If targets are provided, computes the cross-entropy loss.\n",
    "  - Returns the logits and the loss.\n",
    "\n",
    "__generate__ Method:\n",
    "- **Purpose**: Generates new text based on the current context.\n",
    "- **Parameters**:\n",
    "  - idx: Input tensor of shape (B, T) representing the current context.\n",
    "  - max_new_tokens: Number of new tokens to generate. \n",
    "- **Functionality**:\n",
    "  - Iteratively generates new tokens by sampling from the probability distribution of the next token.\n",
    "  - Appends the new token to the current context.\n",
    "  - Returns the generated sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4b) How do you interpret the generated text? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "The generated text appears random. This is due to the following reasons:\n",
    "- it's a simple BLM\n",
    "- During generation, the model uses a probabilistic sampling method `(torch.multinomial)`, which introduces randomness. Without strong contextual or linguistic constraints, this randomness leads to nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4c) What loss do you expect for this model? Can you compare the actual loss with your expectation? **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "With a `vocab_size` of `92` we expect the following cross-entropy loss for an untrained BLM:\n",
    "\n",
    "`expected_loss = -log(1/vocab_size) = log(vocab_size) = log(92) = 4,522`\n",
    "\n",
    "The reported loss is slightly higher than the theoretical `4,522`. This might be due to the random initialization of the embedding table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that up until now, the text history is not used, it is a simple bigram model (only the last character is used to predict the next one). Still, we feed in the whole sequence `xb`, `yb` up to `block_size` for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5a) Create a PyTorch Adam optimizer with a learning rate of `1e-3`, pass it the model parameters for optimization (`model.parameters()`) and store it in `optimizer`. Check the documentation if needed! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the training loop now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=4.8324713706970215\n",
      "step=100, loss=4.69413948059082\n",
      "step=200, loss=4.533603668212891\n",
      "step=300, loss=4.386250019073486\n",
      "step=400, loss=4.354820728302002\n",
      "step=500, loss=4.22548770904541\n",
      "step=600, loss=4.2655487060546875\n",
      "step=700, loss=4.136257648468018\n",
      "step=800, loss=3.8658275604248047\n",
      "step=900, loss=3.8911232948303223\n",
      "step=1000, loss=3.7772715091705322\n",
      "step=1100, loss=3.665560483932495\n",
      "step=1200, loss=3.6113686561584473\n",
      "step=1300, loss=3.467334270477295\n",
      "step=1400, loss=3.463602066040039\n",
      "step=1500, loss=3.380408525466919\n",
      "step=1600, loss=3.376497745513916\n",
      "step=1700, loss=3.208296298980713\n",
      "step=1800, loss=3.411639451980591\n",
      "step=1900, loss=3.0954713821411133\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increase batch size for better results\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # logits are not needed here\n",
    "    optimizer.zero_grad(set_to_none=True) # reset the gradients\n",
    "    loss.backward() # compute the gradients\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "    # print the loss every 100 steps\n",
    "    if steps % 100 == 0:\n",
    "        print(f'step={steps}, loss={loss.item()}')\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate new text based on the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "(itchen womm g? Aben bink,\n",
      "Ih dumin.\n",
      "Geht?\n",
      "Sorer debelanier, deng HEn ageserf, gt wolau dichr T.\n",
      "\n",
      "ETr e ELEr,\n",
      "Könich BINämeienschickun An jeih wofelieicken! Nien ge Tr erngechon, Pflt deien!;\n",
      "﻿ber GNST l?\n",
      "EPh h—OPHe, un asen Hochllie\n",
      "Schelänuft.\n",
      "ISched,\n",
      "T fst s deichteinsiebauchrteinen.\n",
      "STenssolechtt Ge kst Sim st,\n",
      "\n",
      "Untenu SClestzundl vestt Den!\n",
      "\n",
      "ODaguner r!—wi er un el Feichalllbeten\n",
      "Unt e HErtr h ot,\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "print(\"Generated text: \")\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5b) How do you interpret the result? What could be a reason that the output is still suboptimal? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: \n",
    "The generated text from your model demonstrates some progress, as it appears to exhibit patterns of capitalization, punctuation, and word-like structures. However, it still lacks semantic coherence and grammatical structure. Here's an analysis:\n",
    "\n",
    "Observations:\n",
    "Patterns and Word-Like Structures:\n",
    "The output contains recognizable patterns of capitalization and punctuation (\"Geht?\", \"Könich\", \"Sorer\").\n",
    "There are word-like tokens such as \"Schelänuft\" and \"Pflt\", though they are nonsensical.\n",
    "Repetition and Noise:\n",
    "Some parts of the text show repetition of letters or tokens (\"GNST\", \"HERESTOPHe\").\n",
    "Random combinations of letters and punctuation suggest that the model is sampling from the learned bigram distributions without deeper contextual understanding.\n",
    "Language Artifacts:\n",
    "Words like \"Könich\" and \"Hochllie\" suggest that the training data might include a mix of natural language resembling German or similarly structured text.\n",
    "Reason for Output Quality:\n",
    "Bigram Limitation:\n",
    "The bigram model only considers one preceding token when predicting the next, leading to limited contextual understanding. Long-term dependencies (e.g., subject-verb agreement) cannot be captured.\n",
    "Training Progress:\n",
    "The loss has decreased significantly, so the model has learned basic bigram relationships, resulting in structured but nonsensical output.\n",
    "However, it hasn’t fully captured the dataset’s language patterns, likely due to insufficient training or model complexity.\n",
    "Random Sampling:\n",
    "The torch.multinomial sampling introduces variability, which can result in less coherent text if the probability distribution is not sharply peaked.\n",
    "Suggestions to Improve Generated Text:\n",
    "Increase Training Steps:\n",
    "Continue training for more steps to allow the model to refine its understanding of bigram relationships.\n",
    "Larger or Higher-Quality Dataset:\n",
    "If the dataset is small or noisy, consider augmenting it with more diverse and high-quality text.\n",
    "Advanced Models:\n",
    "Transition to a trigram model or a transformer-based architecture to handle longer-term dependencies.\n",
    "These architectures can model richer contextual relationships and generate more coherent text.\n",
    "Temperature Sampling:\n",
    "Experiment with temperature scaling when applying F.softmax in the generate method. Lower temperatures produce more deterministic output, while higher temperatures encourage diversity but can lead to noise.\n",
    "Conclusion:\n",
    "The generated text shows that the model is learning, but its simplicity as a bigram model limits the quality of the output. For meaningful and fluent text generation, moving to a more advanced architecture, such as a GPT-style transformer, is highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarized code so far (with some additions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n",
      "step 0: train loss 5.1627, val loss 5.1298\n",
      "step 300: train loss 2.8487, val loss 3.6135\n",
      "step 600: train loss 2.4767, val loss 3.5528\n",
      "step 900: train loss 2.4090, val loss 3.5687\n",
      "step 1200: train loss 2.3954, val loss 3.5871\n",
      "step 1500: train loss 2.3899, val loss 3.6033\n",
      "step 1800: train loss 2.3674, val loss 3.6232\n",
      "step 2100: train loss 2.3642, val loss 3.6271\n",
      "step 2400: train loss 2.3738, val loss 3.6082\n",
      "step 2700: train loss 2.3744, val loss 3.6491\n",
      "\n",
      "ERuiteeis zenels!\n",
      "Dan deinhns wächrtfchür War spüböchrt,\n",
      "(mprt,\n",
      "Sin,\n",
      "Wenn dirdichrschlin,8t icherzwäh ELöckeinek!\n",
      "(Ihero TOräfaff l;\n",
      "S.\n",
      "\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'mps'\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # new: we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # new: average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # new: switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # new: switch back to train mode\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "---\n",
    "## 6. The Mathematical Trick in Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now derive a more complex model that can look at all tokens at once to predict the next one, not just the last token. \n",
    "To use all previous tokens, the simplest idea is to use an average of all previous tokens. \n",
    "For example, the 5th token uses the **channels** (=feature maps, embeddings) of the 1st, 2nd, 3rd, 4th, and 5th token. \n",
    "The average of these is the **feature vector** for the 5th token and summarizes the context / history.\n",
    "Note that we have lost a lot of information, e.g. the order of the tokens, but it's a starting point. Consider the following toy example with batch size 4 , 8 tokens, 2 channels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2 # batch, time, channels. Goal: 8 tokens should talk to each other, but only from previous tokens, not from future tokens\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in each batch in the example vector `x`, we calculate the mean of the tokens that came before it in the time dimension (including itself). \n",
    "The result should be a tensor of shape (B,T,C) where the t-th row of the b-th batch contains the mean of all tokens in this batch that came before this token in the time dimension.\n",
    "We print the original tensor `x` and the resulting tensor `xbow` containing the mean values and make sure the mean values are correct. Here `bow` stands for **bag of words**, which means that each entry is an average of several words (each of the 8 tokens is considered a 'word' here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow = bag of words = simple average of all previous tokens\n",
    "for b in range(B): # iterate over batch dimension\n",
    "    for t in range(T): # iterate over time dimension\n",
    "        xprev = x[b,:t+1] # (t,C) # all previous tokens for this batch and time (slice)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # mean over time dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6644,  0.6945],\n",
       "        [ 1.4708, -0.5753],\n",
       "        [ 0.7407, -0.3684],\n",
       "        [ 0.2492,  1.4284],\n",
       "        [ 0.0192, -0.4692],\n",
       "        [-0.1109, -0.7312],\n",
       "        [-1.2803, -2.2520],\n",
       "        [-0.9338,  0.9527]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # 0th batch element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6644,  0.6945],\n",
       "        [ 1.5676,  0.0596],\n",
       "        [ 1.2920, -0.0830],\n",
       "        [ 1.0313,  0.2948],\n",
       "        [ 0.8289,  0.1420],\n",
       "        [ 0.6722, -0.0035],\n",
       "        [ 0.3933, -0.3247],\n",
       "        [ 0.2274, -0.1651]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] # vertical average of all previous tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using several nested loops like above, we use a trick with matrix multiplication that is mathematically equivalent but more efficient. Here is a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3) \n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, c contains the sum of the column entries of b. Because we only want the \"history\", not the \"future\" tokens to influence the result, we use an upper triangular matrix `a` instead, this is called **masking**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows, \n",
    "# third row is sum of all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to normalize for averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "a = a / torch.sum(a, 1, keepdim=True) # normalize rows to sum to 1\n",
    "b = torch.randint(0,10,(3,2)).float() # some random data\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "# result: first row of b is copied to c, second row is sum of first two rows + normalized, \n",
    "# third row is sum of all rows + normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6a) Now let's go back to our example above and apply the same trick. \n",
    "Define a lower triangular matrix called `wei` (previously `a`) that is normalized to sum to 1 along the rows. Matrix multiply `wei` with `x` to get a new matrix `xbow2`.\n",
    "Make sure that `xbow2` has the same shape as `x` and that it contains the correct values. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix x:\n",
      "tensor([[[ 1.6644,  0.6945],\n",
      "         [ 1.4708, -0.5753],\n",
      "         [ 0.7407, -0.3684],\n",
      "         [ 0.2492,  1.4284],\n",
      "         [ 0.0192, -0.4692],\n",
      "         [-0.1109, -0.7312],\n",
      "         [-1.2803, -2.2520],\n",
      "         [-0.9338,  0.9527]],\n",
      "\n",
      "        [[ 0.1373, -0.9271],\n",
      "         [-0.5926,  0.9964],\n",
      "         [-1.1174,  0.1796],\n",
      "         [-0.0090,  0.0442],\n",
      "         [ 1.7604, -0.7725],\n",
      "         [ 0.3919,  0.1077],\n",
      "         [-0.6756,  1.3004],\n",
      "         [-0.4023,  0.8002]],\n",
      "\n",
      "        [[-0.3328, -0.8564],\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Define the lower triangular weight matrix\n",
    "wei = torch.tril(torch.ones(T, T))  # Lower triangular matrix\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)  # Normalize rows to sum to 1  # what is the diff to: wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "\n",
    "# Matrix multiply wei with x\n",
    "xbow2 = wei @ x  # Shape will be (B, T, C) after broadcasting\n",
    "\n",
    "print(\"Original matrix x:\")\n",
    "print(x)\n",
    "print(\"\\nLower triangular weight matrix wei:\")\n",
    "print(wei)\n",
    "print(\"\\nComputed xbow2 (Bag of Words):\")\n",
    "print(xbow2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "xbow[0], xbow2[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6b) Now we use yet another mathematically equivalent way to compute the bag of words representation using **Softmax** function (this will be needed later for weighted sum instead of average of previous tokens).\n",
    "We start off with a lower triangular matrix where the lower triangle and diagonal is filled with 0, the upper with `-inf`. \n",
    "After applying the softmax function, the result will be again the `wei` matrix from before. Implement this in the following cell, calculate again the matrix multiplication of the new `wei` and `x` and check the result! **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask M:\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Weighing matrix after softmax (wei2):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# We start with a mask that has zeros for positions in the lower triangle (including diagonal)\n",
    "# and -inf in the upper triangle\n",
    "M = torch.zeros((T, T))\n",
    "M[torch.tril(torch.ones(T, T)) == 0] = float('-inf')  # upper triangle to -inf\n",
    "\n",
    "# Apply softmax along the time dimension (dim=1)\n",
    "wei2 = torch.softmax(M, dim=1)\n",
    "\n",
    "# Multiply wei2 by x to get the bag of words representation\n",
    "xbow3 = wei2 @ x\n",
    "\n",
    "# Print matrices to verify\n",
    "print(\"Mask M:\")\n",
    "print(M)\n",
    "print(\"\\nWeighing matrix after softmax (wei2):\")\n",
    "print(wei2)\n",
    "print(\"\\nResulting bag-of-words (xbow3):\")\n",
    "print(xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow[0], xbow2[0], xbow3[0] # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Self-Attention\n",
    "\n",
    "Finally we get to the most important mechanism: **Self-Attention**! This will lead to a weighted average of the tokens (some tokens are more important than others to understand the text) instead of simply using the mean. And here is the idea: Every single token will emit two vectors: A **query** (\"What am I looking for?\") and a **key** (\"What do I contain?\"). The query then dot-products with all the keys to determine the similarity = affinity (stored in `wei`). Instead of the raw input `x`, which is private, a **value** is used (\"What will I communicate?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels (increase channels for more interesting results)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16) # forward pass of x through the key layer\n",
    "q = query(x) # (B, T, 16) # forward pass of x through the query layer\n",
    "# so far, each token has a key and a query vector, no communication yet\n",
    "wei =  q @ k.transpose(-2, -1) # transpose last 2 dimensions (batch remains unchanged): (B, T, 16) @ (B, 16, T) ---> (B, T, T): for each batch, each token talks to all other tokens, so we get an affinity matrix of size (T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T)) # old version -> change to data dependent weights\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # comment to see intermediate results before normalization\n",
    "\n",
    "v = value(x) # we use the aggregated value instead of the raw x \n",
    "# x is private information to this token, v is the public information for communication\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7a) Print `wei` and compare it to the previous values. What is the most important change and why is this important here? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the first weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the final entry 0.2391 is the weight for the 8th token. The 8th token emits a query ( for example \"I am a vowel at position 8, I am looking for consonants at positions up to 4\"). All tokens then emit keys, and maybe a consonant at position 4 will emit a key with high number in this channel, meaning \"I am a consonant at position 4\". The 8th token will therefore have a high weight for the 4th token (0.2297), resulting in a high affinity (dot product) - the 4th and 8th token \"have found each other\". Through the softmax function, a lot of information from the 4th token will be passed to the 8th token (meaning the 8th token will learn a lot from the 4th)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "### Some Notes on Attention\n",
    "- Attention is a **communication mechanism**. It can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. Here we have `block_size = 8` nodes, where the first node is only pointed to by itself, the second by the first and itself, and so on. Attention can be applied to any directed graph, not only language modeling.\n",
    "- Each example across batch dimension is processed completely independently, the examples never \"talk\" to each other across different batches. The batched matrix multiplication above means applying matrix multiplication in parallel in each batch separately. For example here, you can think of 4 different graphs in parallel with 8 noded each, where the 8 nodes only communicate among each other, even though we process 32 nodes at once.\n",
    "- \"Scaled\" attention also divides `wei` by `1/sqrt(head_size)`, in the original paper:\n",
    "\\begin{equation*}\n",
    "   Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation*}\n",
    "This makes it so when input Q,K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much. Without the normalization, using Gaussian input (zero mean and variance 1), the weights will be in the order of `head_size`. Illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size) # k initialized from standard normal distribution (zero mean, unit variance)\n",
    "q = torch.randn(B,T,head_size) # q initialized from standard normal distribution (zero mean, unit variance)\n",
    "wei_unnormalized = q @ k.transpose(-2, -1) # will have variance of head_size roughly\n",
    "wei_normalized = q @ k.transpose(-2, -1)* head_size**-0.5 # normalize by sqrt of head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var() # variance of k: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var() # variance of q: roughly 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.4690)\n",
      "tensor(1.0918)\n"
     ]
    }
   ],
   "source": [
    "print(wei_unnormalized.var()) # variance of the dot product: roughly head_size=16\n",
    "print(wei_normalized.var()) # variance of the dot product: roughly 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7b) Find out why this is important: Apply softmax to a tensor with entries around 0, then to another tensor with more extreme values. What happens? Write in the answer cell why we want to avoid this. **(2 points)**\n",
    "\n",
    "**HINT:** `torch.softmax()` expects an input specifying along which dimension to calculate the normalization (=which dimension should sum to 1), so you can pass `dim=-1` as second input for a 1D tensor. (See https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values around zero: tensor([ 0.1000, -0.0500,  0.0000])\n",
      "Softmax result: tensor([0.3616, 0.3112, 0.3272])\n",
      "Sum of probabilities: 1.0 \n",
      "\n",
      "Large values: tensor([50., 60., 55.])\n",
      "Softmax result: tensor([4.5094e-05, 9.9326e-01, 6.6925e-03])\n",
      "Sum of probabilities: 1.0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Example 1: Values around 0\n",
    "values_small = torch.tensor([0.1, -0.05, 0.0], dtype=torch.float32)\n",
    "prob_small = F.softmax(values_small, dim=-1)\n",
    "print(\"Values around zero:\", values_small)\n",
    "print(\"Softmax result:\", prob_small)\n",
    "print(\"Sum of probabilities:\", prob_small.sum().item(), \"\\n\")\n",
    "\n",
    "# Example 2: Larger values\n",
    "values_large = torch.tensor([50.0, 60.0, 55.0], dtype=torch.float32)\n",
    "prob_large = F.softmax(values_large, dim=-1)\n",
    "print(\"Large values:\", values_large)\n",
    "print(\"Softmax result:\", prob_large)\n",
    "print(\"Sum of probabilities:\", prob_large.sum().item())\n",
    "\n",
    "# Example 3: Larger values &  values around 0\n",
    "values_large = torch.tensor([50.0, 0.1, 55.0], dtype=torch.float32)\n",
    "prob_large = F.softmax(values_large, dim=-1)\n",
    "print(\"Large values:\", values_large)\n",
    "print(\"Softmax result:\", prob_large)\n",
    "print(\"Sum of probabilities:\", prob_large.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Encoding and Positional Encoding\n",
    "\n",
    "We will make one change on the token encoding: Previously, the `token_embedding_table` was of size `(vocab_size, vocab_size)`, which means we directly plucked out the logits from the embedding table. Now we want to introduce an intermediate layer (make the net bigger). Therefore, we introduce a new parameter `n_embd` for the number of embedding dimensions, for example we can choose 32 or 64 for this intermediate representation. So instead of logits, the `token_embedding_table` will give us **token embeddings**. These will be fed to a linear layer afterwards to get the logits:\n",
    "```\n",
    "self.lm_head = nn.Linear(n_embd, vocab_size) # linear layer to decode into the vocabulary   \n",
    "```\n",
    "\n",
    "In the attention mechanism derived so far, there is no notion of space. Attention simply acts over a set of vectors. Remember that we can think of attention as a directed graph, where the nodes have no idea where they are positioned in a space. But space matters in text: For example, \"people love animals\" has a significantly different meaning than \"animals love people\", so the ordering of the words is very important. This is why we need to **positionally encode** tokens:\n",
    "So far, we have only encoded each token according to its identity `idx`. But we now also encode its position in a second embedding table: Each position from `0` to `block_size-1` will get its own embedding vector. This is the code snippet from the init function that we will implement below: \n",
    "```\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # token embedding according to identity (e.g., first character in vocabulary)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # positional encoding according to position in text (e.g., first character in text)\n",
    "```\n",
    "And here is a code snippet from the forward function, showing how integers from 0 to `block_size` are positionally encoded: \n",
    "```\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - integers from 0 to T-1\n",
    "        x = tok_emb + pos_emb # (B,T,C) via broadcasting (pos_emb gets right-aligned, new dimension of 1 gets added, broadcasted across batch)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "```\n",
    "Right now, this is not useful yet, because we only use the last token in the Bigram model, so the position does not matter. But using attention, it will matter!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Single Self-Attention Head\n",
    "\n",
    "Now let's summarize the code so far and add a single self-attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n",
      "step 0: train loss 4.5645, val loss 4.5525\n",
      "step 500: train loss 2.5698, val loss 3.5501\n",
      "step 1000: train loss 2.4292, val loss 3.7169\n",
      "step 1500: train loss 2.3676, val loss 3.7612\n",
      "step 2000: train loss 2.3415, val loss 3.8056\n",
      "step 2500: train loss 2.3190, val loss 3.8577\n",
      "step 3000: train loss 2.2995, val loss 3.9520\n",
      "step 3500: train loss 2.2939, val loss 3.9182\n",
      "step 4000: train loss 2.2832, val loss 3.9271\n",
      "step 4500: train loss 2.2718, val loss 3.9604\n",
      "\n",
      "Ichät, dustrerndenndie St\n",
      "CHin;\n",
      "KWrlem.\n",
      "Eimend zerum den,\n",
      "Sichr ffen Scht.\n",
      "Leiseberechmeunfr’st win diem rch adr mist iel ochol Hurn, Dlseigescheust zuerr werisen.\n",
      "Den Gei Baufr dan Zurgegen!\n",
      "\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # new: increase number of iterations due to lower learning rate\n",
    "eval_interval = 500 \n",
    "learning_rate = 1e-3 # new: lower learning rate (self-attention is more complex)\n",
    "device = 'mps'\n",
    "print('Running on device:',device)\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # we don't need gradients for this function (more efficient)\n",
    "def estimate_loss(): # average loss over eval_iters iterations\n",
    "    out = {}\n",
    "    model.eval() # switch to eval mode (not relevant here because no dropout etc., but good practice)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # switch back to train mode\n",
    "    return out\n",
    "\n",
    "# new: single self-attention head\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for key. Typically no bias is used in self-attention\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for query\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # define the linear layer for value\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # buffer = not a parameter; masking with lower triangular matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # batch, time, channels\n",
    "        k = self.key(x)   # (B,T,C) - apply the key linear layer\n",
    "        q = self.query(x) # (B,T,C) - apply the query linear layer\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) - scale by head_size**-0.5 (normalization from original paper, see above)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - mask out the future\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) - apply softmax to get the weights\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C) - apply the value linear layer\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C) - weighted aggregation = self-attention\n",
    "        return out \n",
    "    \n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # new: dimensionality of embeddings changed to n_embd as intermediate layer\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # new: position embeddings\n",
    "        self.sa_head = Head(n_embd) # new: self-attention head\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # new: linear layer for prediction\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to the last block_size tokens (because we now use position embeddings, which only contain the last block_size tokens)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device) # move the model to the GPU if available\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create context on the GPU if available\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss decreased a bit, but the result is still not great. We will introduce some more changes following the transformer paper for further improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "---\n",
    "## 8. Full GPT Implementation\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "First, we add **multi-head attention**, which is simply several attention heads running in parallel, then concatenating the result over the channel dimension. A **projection layer** combines the concatenated outputs from all heads into a single unified representation and projects back to the original pathway. Note that \"projection\" in the context of Transformer models refers to a linear transformation that can either maintain, reduce, or even increase the dimensionality of the data. \n",
    "\n",
    "Intuitive Explanation: It helps to have multiple communication channels because these tokens have a lot to talk about - they want to find the consonants, the vowels, the vowels just from certain positions etc. and so it helps to create multiple independent channels of communication to gather lots of different types of data and then decode the output.\n",
    "\n",
    "<img src=\"img/multi-head-attention.jpg\" width=\"200\">\n",
    "\n",
    "### Transformer Block\n",
    "\n",
    "So far, we directly calculated the logits after the attention block, but this was way too fast - intuitively \"the tokens looked at each other, but didn't really have time to think on what they found from the other tokens\". Therefore, we add a feedforward single layer followed by a ReLU nonlinearity. Both layers together are called the **Transformer Block**, where we combine **communication** (self-attention) with **computation** (feedforward layer). This is on a per token level: Each token independently looks at the other tokens, and once it has gathered all the data, it thinks on that data individually. We implement this in the `Block` class below. The transformer block gets repeated over and over again.\n",
    "\n",
    "<img src=\"img/transformer.jpg\" width=\"300\">\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "Also note that the transformer architecture above contains **skip connections (residual connections)**: The network contains parallel paths (one with some computations, one with the identity as \"shortcut\") that are combined via additions. Additions are great for backpropagation because they distribute gradients equally to both branches, so there is a \"shortcut\" for the gradients to directly propagate from the output to the input of the network. This avoids the vanishing gradient problem especially in the beginning - the transformer blocks only get more influence over time.\n",
    "\n",
    "\n",
    "<img src=\"img/skip-connection.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm\n",
    "\n",
    "The transformer architecture uses **layer norm** (called \"Norm\" in the architecture image above), which is very similar to **batch norm**: Batch norm makes sure that across the batch dimension, any individual neuron has unit gaussian distribution (zero mean, unit standard deviation). In layer norm, we don't normalize the columns, but the rows, which normalizes over layers instead of over batches:\n",
    "\n",
    "\\begin{equation*}\n",
    "y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\varepsilon}}\\cdot \\gamma + \\beta,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (copied from BatchNorm1d in makemore series)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # previous batch mean -> index changed from 0 to 1\n",
    "    xvar = x.var(1, keepdim=True) # previous batch variance -> index changed from 0 to 1\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # no running mean and variance buffers needed like in batch norm\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 8a) Check if mean and standard deviation of rows and/or columns are normalized now! Write the result in the answer cell. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row means: tensor([ 2.3842e-09,  2.1458e-08,  7.1526e-09,  8.9407e-09,  0.0000e+00,\n",
      "        -7.1526e-09,  4.7684e-09,  1.7881e-08,  4.7684e-09, -1.4305e-08,\n",
      "        -4.7684e-09,  9.5367e-09, -4.7684e-09, -4.7684e-09, -1.9073e-08,\n",
      "         9.2387e-09, -1.4305e-08,  4.7684e-09, -4.7684e-09, -1.4305e-08,\n",
      "         2.9802e-09,  1.6689e-08, -1.1921e-08, -1.1921e-08,  3.0994e-08,\n",
      "         1.1921e-08,  2.8610e-08,  1.1921e-08, -1.4305e-08,  1.7881e-09,\n",
      "         2.1458e-08, -9.5367e-09])\n",
      "Row stds: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "Column means: tensor([ 0.1469, -0.5910, -0.3974,  0.0468, -0.1431,  0.0138, -0.2664,  0.4181,\n",
      "         0.1426,  0.2191,  0.2554, -0.2625, -0.0543, -0.1050,  0.1541,  0.2492,\n",
      "         0.2498,  0.1354, -0.2027, -0.3772,  0.2920,  0.1959, -0.2249, -0.0574,\n",
      "         0.1293, -0.1413,  0.1445, -0.2509,  0.1434,  0.0128,  0.0631, -0.2482,\n",
      "        -0.0977,  0.0945,  0.1880,  0.0951,  0.0047,  0.2833,  0.1154, -0.3063,\n",
      "         0.0510,  0.1602,  0.0598,  0.1157,  0.0083, -0.2541, -0.0447, -0.0921,\n",
      "         0.1891, -0.0150, -0.1857, -0.4513, -0.1106,  0.0320,  0.0417,  0.1272,\n",
      "        -0.3022, -0.2864,  0.2507, -0.1101,  0.0402,  0.2277,  0.2753,  0.2577,\n",
      "        -0.1698,  0.2775, -0.1854,  0.0767, -0.2023,  0.2106,  0.1443,  0.1391,\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Means along dimension 1 (per sample)\n",
    "print(\"Row means:\", x.mean(dim=1))  # should be close to 0\n",
    "print(\"Row stds:\", x.std(dim=1))    # should be close to 1\n",
    "\n",
    "# Means along dimension 0 (per feature across samples)\n",
    "print(\"Column means:\", x.mean(dim=0)) # no guarantee to be 0\n",
    "print(\"Column stds:\", x.std(dim=0))   # no guarantee to be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that layer norm is usually applied before the self-attention and linear layer nowadays (unlike the original paper) - one of the very few changes of the transformer architecture during the last years, otherwise mostly the architecture remained unchanged. This is called the **pre-norm formulation**. So here is a code snippet used below showing the two layer norms we will implement, one before the self-attention and one before the linear layer:\n",
    "\n",
    "```\n",
    "        x = x + self.sa(self.ln1(x)) # layer norm directly applied to x before self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # layer norm applied before linear layer\n",
    "```\n",
    "\n",
    "Finally, another layer norm is typically applied at the end of the Transformer and right before the final linear layer that decodes into vocabulary. \n",
    "\n",
    "The size of the layer norm is `n_embds=32` here, so this is a per token transformation, it just normalizes the features and makes them unit Gaussian at initialization. Because these layer norms contain gamma and beta as trainable parameters, the layer norm may eventually create outputs that are not unit Gaussian depending on the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up the Model\n",
    "\n",
    "We now have all components together so that we can scale up the model and make it bigger. Therefore, we add a parameter `n_layer=4` to specify that we want 4 transformer blocks.\n",
    "\n",
    "We also add **dropout** to prevent overfitting: with 4 transformer blocks, the network is getting quite large now. Therefore, we randomly deactivate some connections to prevent them from becoming too dominant. Because the mask of what's being dropped out has changed every single forward backward pass, effectively we end up training an ensemble of sub-networks. At test time, everything is fully enabled and all of those sub-networks are merged into a single ensemble, making it more robust.\n",
    "\n",
    "\n",
    "<img src=\"img/dropout.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full GPT with Multi-Head Attention and Transformer Block\n",
    "\n",
    "We finally get to the full GPT code, adding all the components explained above!\n",
    "\n",
    "**TODO:** 8b) In the summarized code below, comment each line to make sure you have understood all GPT components! You may use support from ChatGPT or GitHub Copilot, but double-check the results and be able to explain it yourself. (Yes, this is tedious, but it will help you get an in-depth understanding of the full GPT architecture) **(10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n",
      "10.809692 M parameters\n",
      "\n",
      "==============\n",
      "step 0: train loss 4.5842, val loss 4.5758\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "CLZQ(v2r‘SmÜpCpi“ydCr:1TTJ%‘O﻿(txjvFN,(o‘9O“J(T”ÄWOüYQPXzäVAjg3e.tRl46:ÖAb”E?IB\n",
      "u?üm(”Y8D3-OCqD.AERp“7ÖhXtQWötiWßB,YfVOdfYdotQvgWW0BM*aKIkutQ.y%Xi”™L$“‘r8E:(DxYz o”$PxtU-hmn:üQ7T/GmöHZfW‘s/ZIYfBCÜWss*\n",
      "\n",
      "==============\n",
      "step 500: train loss 1.7839, val loss 3.7779\n",
      "==============\n",
      "\n",
      "Sample:\n",
      "\n",
      "Arge Brel).\n",
      "Umme! ksetehrst unt’glauf der Hauf.\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# YOUR COMMENTS HERE\n",
    "batch_size = 16 # \n",
    "block_size = 32 # \n",
    "max_iters = 5000 # \n",
    "eval_interval = 500 # \n",
    "learning_rate = 1e-3 # \n",
    "eval_iters = 200 # \n",
    "n_embd = 64 # \n",
    "n_head = 4 # \n",
    "n_layer = 4 # \n",
    "dropout = 0.2 # \n",
    "\n",
    "# hyperparameters version 2 - only uncomment when training on GPU (no comments needed here)\n",
    "#\"\"\"\n",
    "batch_size = 64 \n",
    "block_size = 256 \n",
    "max_iters = 5000 \n",
    "eval_interval = 500 \n",
    "learning_rate = 3e-4 \n",
    "eval_iters = 200 \n",
    "n_embd = 384 \n",
    "n_head = 6 \n",
    "n_layer = 6 \n",
    "dropout = 0.2  \n",
    "#\"\"\"\n",
    "\n",
    "# ------------\n",
    "\n",
    "device = 'mps'\n",
    "print('Running on device:',device) # \n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data # \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # \n",
    "    x, y = x.to(device), y.to(device) # \n",
    "    return x, y #\n",
    "\n",
    "@torch.no_grad() # \n",
    "def estimate_loss(): # \n",
    "    out = {} # \n",
    "    model.eval() # \n",
    "    for split in ['train', 'val']: # \n",
    "        losses = torch.zeros(eval_iters) # \n",
    "        for k in range(eval_iters): # \n",
    "            X, Y = get_batch(split) # \n",
    "            logits, loss = model(X, Y) #  \n",
    "            losses[k] = loss.item() # \n",
    "        out[split] = losses.mean() # \n",
    "    model.train() # \n",
    "    return out # \n",
    "\n",
    "class Head(nn.Module): ### nochmal ckecken\n",
    "\n",
    "    def __init__(self, head_size): # \n",
    "        super().__init__() # \n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # \n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # \n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) # \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # \n",
    "\n",
    "    def forward(self, x): # \n",
    "        B,T,C = x.shape # \n",
    "        k = self.key(x)   # \n",
    "        q = self.query(x) # \n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # \n",
    "        wei = F.softmax(wei, dim=-1) # \n",
    "        wei = self.dropout(wei) # \n",
    "        \n",
    "        v = self.value(x) # \n",
    "        out = wei @ v # \n",
    "        \n",
    "        return out # \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size): \n",
    "        super().__init__() # \n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # \n",
    "        self.proj = nn.Linear(n_embd, n_embd) #\n",
    "        self.dropout = nn.Dropout(dropout) # \n",
    "        \n",
    "    def forward(self, x): # \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # \n",
    "        out = self.dropout(self.proj(out)) # \n",
    "        return out # \n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd): #\n",
    "        super().__init__() # \n",
    "        self.net = nn.Sequential( # \n",
    "            nn.Linear(n_embd, 4 * n_embd), # \n",
    "            nn.ReLU(), # \n",
    "            nn.Linear(4 * n_embd, n_embd), # \n",
    "            nn.Dropout(dropout), # \n",
    "        )\n",
    "\n",
    "    def forward(self, x): # \n",
    "        return self.net(x) # \n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head): # \n",
    "        super().__init__() # \n",
    "        head_size = n_embd // n_head # \n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # \n",
    "        self.ffwd = FeedFoward(n_embd) # \n",
    "        self.ln1 = nn.LayerNorm(n_embd) # \n",
    "        self.ln2 = nn.LayerNorm(n_embd) # \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # \n",
    "        x = x + self.ffwd(self.ln2(x)) #\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() # \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # \n",
    "        self.ln_f = nn.LayerNorm(n_embd) # \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # \n",
    "\n",
    "    def forward(self, idx, targets=None): # \n",
    "        B, T = idx.shape # \n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # \n",
    "        x = tok_emb + pos_emb # \n",
    "        x = self.blocks(x) # \n",
    "        x = self.ln_f(x) # \n",
    "        logits = self.lm_head(x) # \n",
    "\n",
    "        if targets is None: # \n",
    "            loss = None # \n",
    "        else:\n",
    "            B, T, C = logits.shape # \n",
    "            logits = logits.view(B*T, C) # \n",
    "            targets = targets.view(B*T) # \n",
    "            loss = F.cross_entropy(logits, targets) # \n",
    "\n",
    "        return logits, loss # \n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # \n",
    "        for _ in range(max_new_tokens): # \n",
    "            idx_cond = idx[:, -block_size:] # \n",
    "            logits, loss = self(idx_cond)  # \n",
    "            logits = logits[:, -1, :] # \n",
    "            probs = F.softmax(logits, dim=-1) # \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # \n",
    "        return idx\n",
    "\n",
    "\n",
    "model = GPTLanguageModel() # \n",
    "model = model.to(device) # \n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') # \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # \n",
    "\n",
    "for iter in range(max_iters): # \n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1: #\n",
    "        losses = estimate_loss() # \n",
    "        print(\"\\n==============\")\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # \n",
    "        print(\"==============\")\n",
    "        \n",
    "        print(\"\\nSample:\")\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=device) # \n",
    "        print(decode(model.generate(context, max_new_tokens=200)[0].tolist())) # \n",
    "\n",
    "    xb, yb = get_batch('train') #\n",
    "\n",
    "    logits, loss = model(xb, yb) # \n",
    "    optimizer.zero_grad(set_to_none=True) # \n",
    "    loss.backward() # \n",
    "    optimizer.step() # \n",
    "    \n",
    "print(\"\\nFinal sample:\") # \n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) #\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist())) # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have trained a more powerful GPT model using self-attention. Let's generate a longer text and see how the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seck, der dumpfe Nase\n",
      "\n",
      "Wie HOR.\n",
      "Spind ihn gleich im Flach geleiten!\n",
      "Ich hab ihn nicht, wie sich eine Stunde gehn?\n",
      "Wenn ihr den dir frechen Geist! Mir wachsten muß!\n",
      "Die köcht nur die Bilder säuselt!\n",
      "\n",
      "MEPHISTOPHELES.\n",
      "Es wird sich gleich von neuem Spiegel.\n",
      "Mich dank in die Wälder wohl behagen,\n",
      "Ich führe mich wenig tanzt, mir und höret,\n",
      "Weiß wo er noch verschwarze Zeug ds Nacht belieben.\n",
      "Auch, Fauste, die ehre\n",
      "Tatenzpier und quickung sterbt,\n",
      "Mein läßt’s nun ebendich so macher strebt.\n",
      "\n",
      "WAGNER.\n",
      "Allein der Fülle!\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# generate a longer sample\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "new_text = decode(model.generate(context, max_new_tokens=10000)[0].tolist())\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to a text file\n",
    "f =  open(\"GPT_generated_text.txt\",\"w\")\n",
    "f.write(new_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (optional):** Apply the code to a different text of your choice! What loss do you achieve? What parameters did you change and why? How do you interpret the output compared to the Shakespeare output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlook and Next Steps\n",
    "### Andrej's Suggested Further Experiments\n",
    "\n",
    "- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
    "- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
    "- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n",
    "- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder and Encoder\n",
    "\n",
    "Text generation as above only uses the **decoder** part of the transformer architecture. The **decoder attention block** implemented above has **triangular masking**, and is usually used in autoregressive settings, like language modeling. \n",
    "\n",
    "In other settings, we do want \"future\" tokens to influence the prediction, so we do not use triangular masking. For example, in sentiment analysis, we look at a whole sentence at once, then predict the sentiment \"happy\" or \"sad\" of the speaker. This can be realized using an **encoder** attention block. To implement an encoder attention block, we can simply delete the single line that does masking with `tril`, allowing all tokens to communicate. Attention does not care whether tokens from the future contribute or not, it supports arbitrary connectivity between nodes.\n",
    "\n",
    "### From Self-Attention to Cross-Attention\n",
    "\n",
    "**Self-attention** means that the keys and values are produced from the same source as queries. In **cross-attention**, the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module). For example, when translating from French to English, we condition the decoding on the past decoding *and* the fully encoded french prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From GPT to ChatGPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a long way to go from our toy GPT example to ChatGPT. \n",
    "First of all, ChatGPT's **pre-training** was done on a large chunk of internet, resulting in a decoder-only transformer for text generation. \n",
    "So the pretraining is quite similar to our toy example training, except that we used roughly 10 million parameters and the largest transformer for ChatGPT uses 175 billion (!) parameters. Also it was trained on 300 billion tokens (our training set would be 300.000 tokens roughly when not using character-level tokens, but sub-word chunks). This is about a million fold increase in number of tokens - and today, even bigger datasets are used with trillions of tokens for training on thousands of GPUs!\n",
    "\n",
    "See the following table for the number of parameters, number of layers, n_embd, number of heads, head size, batch size and learning rate in **GPT-3**:\n",
    "\n",
    "\n",
    "<img src=\"img/GPT3_params_table.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pre-training, the model will be a document completer, it will not give answers but produce more questions or result in some undefined behavior. For becoming an assistant, further **fine-tuning** is needed using **Reinforcement Learning from Human Feedback (RLHF)**. Here is an overview of manual fine-tuning with human AI trainers (see the OpenAI ChatGPT blog for details, link below):\n",
    "\n",
    "<img src=\"img/chatgpt_diagram_light.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "To sum it up, we trained a decoder only Transformer following the famous paper 'Attention is All You Need' from 2017, which is basically a GPT. We saw how using self-attention, we can calculate a weighted average of past tokens to predict the next token. We trained it on different texts (Shakespeare, Faust, Jane Austen etc.) and produced new texts in the same writing style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- Attention is All You Need paper: https://arxiv.org/abs/1706.03762\n",
    "- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 \n",
    "- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output in 3_Character_Level_GPT__student.ipynb above threshold seen and so a NEW version has been made: `TRUNCATED_3_Character_Level_GPT__student.ipynb`.\n",
      "[NbConvertApp] WARNING | pattern 'TRUNCATED_3_Character_Level_GPT__solution.ipynb' matched no files\n",
      "This application is used to convert notebook files (*.ipynb)\n",
      "        to various other formats.\n",
      "\n",
      "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "=======\n",
      "The options below are convenience aliases to configurable class-options,\n",
      "as listed in the \"Equivalent to\" description-line of the aliases.\n",
      "To see all configurable class-options for some <cmd>, use:\n",
      "    <cmd> --help-all\n",
      "\n",
      "--debug\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "    Equivalent to: [--Application.log_level=10]\n",
      "--show-config\n",
      "    Show the application's configuration (human-readable format)\n",
      "    Equivalent to: [--Application.show_config=True]\n",
      "...truncated..."
     ]
    }
   ],
   "source": [
    "# This cell truncates long output to a maximum length, then converts the notebook to a PDF\n",
    "# NOTE: You may have to adapt the path or filename to match your local setup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# truncate long cell output to avoid large pdf files\n",
    "from helpers.truncate_output import truncate_long_notebook_output\n",
    "truncated = truncate_long_notebook_output('3_Character_Level_GPT__student.ipynb')\n",
    "\n",
    "# convert to pdf with nbconvert\n",
    "if truncated:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download TRUNCATED_3_Character_Level_GPT__student.ipynb\n",
    "else:\n",
    "    !jupyter nbconvert --to webpdf --allow-chromium-download 3_Character_Level_GPT__student.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
