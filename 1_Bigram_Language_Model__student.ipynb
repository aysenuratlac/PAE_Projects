{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Bigram Language Model from Scratch\n",
    "---\n",
    "\n",
    "This is an extended version of Andrej Karpathy's notebook in addition to his [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on bigram language models.\n",
    "\n",
    "Adapted by: \n",
    "\n",
    "Prof. Dr.-Ing. Antje Muntzinger, University of Applied Sciences Stuttgart\n",
    "\n",
    "antje.muntzinger@hft-stuttgart.de\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We'll construct a bigram language model from scratch. \n",
    "The model will be trained on a text file containing names and will be able to generate new names based on what it has learned. So if you are expecting a baby and looking for some extraordinary new name, you might get some inspiration here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[1. Loading the Data](#1.-loading-the-data)\n",
    "\n",
    "[2. Preprocessing the Data](#2.-preprocessing-the-data)\n",
    "\n",
    "[3. Bigram Counts as PyTorch Tensor](#3.-Bigram-Counts-as-PyTorch-Tensor)\n",
    "\n",
    "[4. Sampling New Characters](#4.-sampling-new-characters)\n",
    "\n",
    "[5. Broadcasting](#5.-broadcasting)\n",
    "\n",
    "[6. Generating New Names](#6.-generating-new-names)\n",
    "\n",
    "[7. Evaluating the Model](#7.-evaluating-the-model)\n",
    "\n",
    "[8. The Neural Network Approach](#8.-the-neural-network-approach)\n",
    "\n",
    "[9. The First Neuron](#9.-The-First-Neuron)\n",
    "\n",
    "[10. Creating 27 Neurons](#10.-Creating-27-Neurons)\n",
    "\n",
    "[11. Summary (so far)](#11.-Summary-(so-far))\n",
    "\n",
    "[12. Optimization](#12.-Optimization)\n",
    "\n",
    "[13. Optimizing over the Whole Dataset](#13.-Optimizing-over-the-Whole-Dataset)\n",
    "\n",
    "[14. Sampling from the Neural Net](#14.-Sampling-from-the-Neural-Net)\n",
    "\n",
    "[15. Conclusion](#15.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preprocessing the Data\n",
    "\n",
    "First, we read the names from the text file and save them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 1) Print the first 10 names. Find out the number of names contained in the dataset as well as the shortest and longest name. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "print(\"First 10 names in the dataset: \", words[:10])\n",
    "print(\"Number of names contained in the dataset: \", len(words))\n",
    "print(\"Longest name in the dataset: \", max(words, key=len))\n",
    "print(\"Shortest name in the dataset: \",min(words, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in one example name like \"isabella\", a lot of information is contained. For example:\n",
    "- \"i\" is likely to be the first character of a name\n",
    "- \"s\" is likely to follow after an \"i\"\n",
    "- \"a\" is likely to follow after \"is\"\n",
    "- ...\n",
    "- after \"isabella\", the name is likely to end.\n",
    "\n",
    "Here we are going to create a **bigram** language model, which means we only use the single previous character to predict the next. For example, we use \"s\" (not \"is\") to predict \"a\" in \"isabella\" and forget that we have a lot more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Data\n",
    "\n",
    "First, we introduce two special characters, `<S>` for \"start\" and `<E>` for \"end\". Then we zip two consecutive characters and print them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {} # dictionary to store bigram counts\n",
    "for w in words: # print bigrams\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # zip(['a', 'b', 'c'], ['b', 'c', 'd']) -> [('a', 'b'), ('b', 'c'), ('c', 'd')]\n",
    "        bigram = (ch1, ch2)\n",
    "        # count how many times bigram appears\n",
    "        b[bigram] = b.get(bigram, 0) + 1 # b.get(bigram, 0) returns b[bigram] if bigram is in b, otherwise 0\n",
    "        print(ch1, ch2, b[bigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that already in the first three names, the bigram `a <E>` occurs three times - which makes sense because many names end with an \"a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all bigrams\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by the count (by default it sorts by the first element of the tuple, but we want the second element)\n",
    "sorted(b.items(), key = lambda kv: kv[1], reverse=True) # kv is a tuple, kv[0] is the key, kv[1] is the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 2) What is the most common bigram in the dataset? What is the least common bigram? **(2 points)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "print(\"most common bigram:\", max(b, key=b.get))\n",
    "print(\"least common bigram:\", min(b, key=b.get))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigram Counts as PyTorch Tensor\n",
    "\n",
    "Next, we store the bigrams in a matrix instead of a python dictionary: The rows are going to be the first character of the bigram and the columns are going to be the second character. Each entry in this matrix will tell us how often that first character is followed by the second character in the dataset.\n",
    "\n",
    "To create this matrix (also called **2D array** or **2D tensor**), we use PyTorch, which allows us to create multi-dimensional arrays and manipulate them very efficiently. We will also use a special character `.` for word boundaries instead of `<A>` and `<E>` because we don't need to distinguish between these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get all characters and map them to integers in a mapping called \"s to i\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words)))) # get all unique characters in words\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # create a dictionary mapping characters to integers\n",
    "stoi['.'] = 0 # add a special character for word boundaries instead of <A> and <E>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create the inverse mapping \"i to s\" that assigns a character to each integer. We can use `items()` to get a list of a dictionary's tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stoi)\n",
    "print(stoi.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:s for s,i in stoi.items()} # reverse mapping from integers to characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3a) We now have a vocabulary of 27 characters. \n",
    "Instanciate a torch tensor called `N` of size 27x27 with zero values of dtype int32. Check the PyTorch documentation to see how to create a tensor if needed. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "N = torch.zeros((len(stoi), len(stoi)), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the matrix `N` with the counts of the bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the counts of bigrams in the matrix\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j] # character string of bigram, e.g. 'ab'\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray') # upper text: bigram\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray') # lower text: count\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3b) Why do we use `.item()` in the code cell above? Check out the PyTorch documentation if needed! **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "The .item() method is used to extract the value of a single element from a PyTorch tensor and convert it from the tensor N[i, j] (which is a 0-dimensional tensor) to an integer before being passed to plt.text(). This is necessary because plt.text() expects standard Python data types, not PyTorch tensor objects.\n",
    "\n",
    "Without .item(), PyTorch would return a tensor object (like tensor(5)), which might cause issues or unexpected behavior when trying to display the value using matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 3c) Assume our first character is an 'f'. Looking at the matrix plot above, which is the most likely next character? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER GOES HERE\n",
    "print(itos[torch.argmax(N[stoi['f']]).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N[1,2])\n",
    "print(N[1,2].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visual summary so far - this is how we can get the most likely next character for input 'e' by plucking out the 6th row:\n",
    "\n",
    "<img src=\"img/bigram1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling New Characters\n",
    "\n",
    "Now we want to sample new characters. We start with the first character `.`, \n",
    "and then sample the next character based on the count of bigrams.\n",
    "We have the raw counts stored in the matrix N, but we need to convert them to \n",
    "probabilities in order to sample from the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4a) Pluck out the first row of the matrix, convert the entries to floats, and normalize them so that the row sums to 1. **(3 points)** \n",
    "\n",
    "**HINT:** You can simply cast a PyTorch tensor to float using `.float()`, see for example here: https://www.datascienceweekly.org/tutorials/pytorch-change-tensor-type-cast-a-pytorch-tensor-to-another-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "p = N[0].float() # convert first row of N to float\n",
    "p /= p.sum() # normalize values\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4b) How can you interpret this row now? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "\"All values are between 0 and 1 and can be interpreted as a nominal probability\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from this distribution, we use `torch.multinomial`, which samples from the multinomial probability distribution (in simple words: \"you give me probabilities and I will give you integers sampled according to the probability distribution\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # makes the random numbers reproducible\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # sample from the distribution\n",
    "# replacement=True means that we are sampling with replacement (i.e. we can sample the same index multiple times)\n",
    "print(\"sampled index:\", ix) # print the sampled index\n",
    "itos[ix] # convert the sampled index to a character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4c) Sample 100 characters according to your probability distribution for word beginnings. Print the sampled indices and characters. Can you relate the output to your probability distribution? Did you expect this output, or can you see unexpected output characters? **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix_100 = torch.multinomial(first_row, num_samples=100, replacement=True, generator=g) # use torch.multinominal with batch size of 100\n",
    "for i in ix_100:\n",
    "    print(i.item(),\":\", itos[i.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "Yes, i can relate the output to probability distribution. I expect this output because the letters with highest probabilities are : a,b,c,d,e,j,k,l,m,n and they are found in output more than others. Also the probability of '..' was 0 and i have never seen it on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the updated visualization including normalization for probabilities as outputs:\n",
    "\n",
    "<img src=\"img/bigram2.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Broadcasting\n",
    "We store all probabilities in a matrix `P`, so that each row is normalized to 1 and contains the probabilities for the next character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float() \n",
    "P /= P.sum(1, keepdims=True) # normalize the rows of P (we use keepdims=True to keep the dimensions of P, see below)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 5) Can you think of a reason why we calculate the probabilities based on `N+1` instead of `N`? **(1 point)**\n",
    "\n",
    "**HINT:** What happens for `N=0`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "It is a so called Laplace-smoothing which makes sure that no entry is 0. This is needed not have any 0% probability for the entries without occurrence in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need `keepdims=True` in order to sum each line, the result should be a 27x1 column vector containing the row sums - otherwise all entries get collapsed to a 1D instead of 2D array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P.sum(1, keepdims=True).shape) # shape collapes to 27x1 due to summation along axis 1 (column vector)\n",
    "print(P.sum(1, keepdims=False).shape) # 1D tensor of size 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why does the division above `P /= P.sum(1, keepdims=True)` even work? We divide the 27x27 matrix `P` by a 27x1 vector. This only works because PyTorch automatically applies **broadcasting**, a type of tensor manipulation: From https://pytorch.org/docs/stable/notes/broadcasting.html we get the information that two tensors are **broadcastable** if the following rules hold:\n",
    "\n",
    "    Each tensor has at least one dimension.\n",
    "\n",
    "    When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "If two tensors x, y are “broadcastable”, the resulting tensor size is calculated as follows:\n",
    "\n",
    "    If the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n",
    "\n",
    "    Then, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension.\n",
    "    \n",
    "In our case, broadcasting takes the 27x1 column vector and copies it 27 times to get a 27x27 matrix, then it makes an element-wise division. We can check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the first row sums to 1\n",
    "print(P[0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (optional):** What happens if you remove `keepdims=True`? Will the division still result in the same matrix - why or why not? \n",
    "\n",
    " **HINT:** Don't forget to revert your changes for later cells to work right - or copy the code cell and use different variable names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "If we remove it, it will return 1D array(each element is sum of one row),then when we divide the tensor(27,27) with sum(27). As it didnt protect the dimension features, the division would be wrong(maybe column wise). In the code below, as there is a mistake in calculation, sum of first row is not 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "P_v2 = (N+1).float()\n",
    "P_v2 /= P_v2.sum(1)\n",
    "print(\"If it would normalized correctly sum supposed to be 1 but : \",P_v2[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "# we are actually normalizing the columns of P, not the rows:\n",
    "print('row sums:', P1[0].sum(), P2[0].sum())\n",
    "print('column sums:', P1[:,0].sum(), P2[:,0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating New Names\n",
    "\n",
    "With this, we can sample new names by starting with the start character `.` and sampling the next character, then the next next character and so on, until we sample the end character `.`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20): # generate 20 names\n",
    "  \n",
    "    out = []\n",
    "    ix = 0 # start with the start-of-word character\n",
    "    while True:\n",
    "        p = P[ix] # get the probabilities of the next character by plucking the row corresponding to the current character\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # sample the next character (=column index)\n",
    "        out.append(itos[ix]) # convert the sampled index to a character and append it to the output\n",
    "        if ix == 0: # if we sampled the end-of-word character: break\n",
    "            break\n",
    "    print(''.join(out)) # print the generated name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look right... but it is! The bigram language model is simply not  powerful enough to create more reasonable names. For example, it generated a name \"a\" twice, but it doesn't know that \"a\" is the first character here. All it knows is that \"a\" is a character in the name and how likely it is that the name ends here, without looking at previous characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 6) In the code cell above, experiment with replacing `p` with the uniform distribution, making everything equally likely. Can you see that the bigram model works better than pure chance? **(2 points)**\n",
    "\n",
    "**HINT:** Don't forget to revert your changes for later cells to work right - or copy the code cell and use different variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):  \n",
    "    out = []\n",
    "    ix = 0  # Start mit dem Startzeichen '.'\n",
    "    \n",
    "    while True:\n",
    "        # Ersetzen von `p` mit einer Gleichverteilung\n",
    "        p = torch.ones_like(P[ix])  # Erstellen eines Vektors gleicher Wahrscheinlichkeiten\n",
    "        p /= p.sum()  # Normieren, sodass sich die Summe auf 1 beläuft (Gleichverteilung)\n",
    "        \n",
    "        # Ziehe das nächste Zeichen\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])  # Füge das Zeichen zur Ausgabe hinzu\n",
    "        \n",
    "        # Schleife abbrechen, wenn das Endzeichen `.` gesampelt wird\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(out))  # Gib den generierten Namen aus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating the Model\n",
    "\n",
    "We want to evaluate the quality of the model using a single number, the **training loss**. But which single number to use? Let's print the probabilities of the first bigrams to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:3]: # print bigram probabilities for the first 3 words\n",
    "    chs = ['.'] + list(w) + ['.'] # add start and end of word characters\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # iterate over bigrams\n",
    "        ix1 = stoi[ch1] # get the integer representation of the characters\n",
    "        ix2 = stoi[ch2] # get the integer representation of the characters\n",
    "        prob = P[ix1, ix2] # get the probability of the bigram\n",
    "        print(f'{ch1}{ch2}: {prob:.4f}') # print the bigram and its probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7a) What probabilities would we expect for an untrained model? How do you interpret these outputs compared to the untrained model? **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "For an untrained model, we would expect all bigram probabilities to be equal. This is because, in an untrained model, we have no prior information about the likelihood of different character transitions.\n",
    "\n",
    "If there are 27 possible characters (including the start and end markers), each bigram probability in an untrained model would be close to 1/27 ≈ 0.037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we summarize these probabilities in a single number? First of all, we can use the **likelihood**, which is the product of all these probabilities. The likelihood tells us which probability our model assigns to the whole dataset. So we want the likelihood to be as big as possible for a good model (**maximum likelihood estimation**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 7b) Can you think of a problem when calculating the likelihood as defined above? **(1 point)**\n",
    "\n",
    "**HINT**: What happens numerically if we multiply many tiny numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "Since the likelihood is the product of probabilities (which are typically small), it can quickly approach very small values, close to zero, as the dataset grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the likelihood, we use the **log likelihood**: Applying the logarithm transforms the likelihood to a value in $[-\\infty, 0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input values between 0 and 1\n",
    "x = torch.linspace(0.001, 1, 100)  # Avoid log(0) by starting from 0.001\n",
    "\n",
    "# Compute the logarithm of the input values\n",
    "y = torch.log(x)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(x, y, label='log(x)')\n",
    "plt.title('Logarithm of Input Values Between 0 and 1')\n",
    "plt.xlabel('Input Value')\n",
    "plt.ylabel('Logarithm')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7c) Why is maximizing the likelihood equivalent to maximizing the log likelihood? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "Because the logarithm is a monotonically increasing function. This means that if you increase the likelihood, its logarithm also increases, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of maximizing the log likelihood, in practice we minimize the negative log likelihood, because a loss usually is a number in $[0,\\infty]$, where small numbers are good. Finally, the negative log likelihood is simply the sum of the single logarithms due to\n",
    "\n",
    "$$\\log(a*b*c) = \\log(a) + \\log(b) + \\log(c)$$\n",
    "\n",
    "This is great because a single probability close to zero will not cause the whole loss to be tiny any more, we now use addition instead of multiplication!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 7d) Calculate the **negative log likelihood loss**, store it in `nll` and print it. Sometimes, we also use the **average negative log likelihood loss**, so average `nll` by the number of bigrams evaluated and print the average as well. **(3 points)**\n",
    "\n",
    "**HINT:** You can start by copying the second last code cell above, where we calculated the probabilities of bigrams. Then apply `torch.log` to these probabilities and sum the negative logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "nll = 0  # initialize negative log likelihood loss\n",
    "num_bigrams = 0  # counter for the total number of bigrams\n",
    "\n",
    "for w in words:  # loop over each word in the dataset\n",
    "    chs = ['.'] + list(w) + ['.']  # add start and end markers to each word\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):  # iterate through each bigram in the word\n",
    "        ix1 = stoi[ch1]  # index for first character\n",
    "        ix2 = stoi[ch2]  # index for second character\n",
    "        prob = P[ix1, ix2]  # get the probability of the bigram\n",
    "        nll += -torch.log(prob)  # add the negative log probability to nll\n",
    "        num_bigrams += 1  # increment the bigram count\n",
    "\n",
    "# Average negative log likelihood\n",
    "avg_nll = nll / num_bigrams\n",
    "\n",
    "# Print results\n",
    "print(\"Negative Log Likelihood Loss:\", nll.item())\n",
    "print(\"Average Negative Log Likelihood Loss:\", avg_nll.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summarizes our first bigram model, which simply counts occurences of bigrams and generates new names based on the corresponding probability distribution. The average log likelihood loss of this model is around 2.45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualization includes the whole bigram model with negative log likelihood loss:\n",
    "\n",
    "<img src=\"img/bigram3.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. The Neural Network Approach\n",
    "\n",
    "Let's try another approach: a neural network! Our neural network will still be a bigram language model: It receives a single character as an input, processes it using some weights or some parameters w and outputs the probability distribution over the next character. We will evaluate the parameters of the neural net using the negative log likelihood loss, which means comparing the output probability distribution and the label (the identity of the next character). We are going to use gradient-based optimization to tune the parameters of this network to minimize the loss with the goal to better predict the next character. In the end, the result will look quite similar to the intuitive approach counting occurances of bigrams above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the training set for the neural network made of two lists, the inputs and the targets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]: # only use the first word for now, which contains 5 examples\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1) # input is the first character of the bigram\n",
    "        ys.append(ix2) # target is the second character of the bigram\n",
    "    \n",
    "xs = torch.tensor(xs) # convert the list to a tensor\n",
    "ys = torch.tensor(ys) # convert the list to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the 5 resulting inputs and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 8) The following code cell applies **one-hot-encoding**. Shortly explain what this is and why it is needed! **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "One-hot encoding allows us to convert categorical data (like characters) into a numerical format that can be used as input for neural networks. Neural networks expect numerical input, and one-hot vectors ensure each character has a unique, distinguishable vector representation without introducing any inherent ordering or numerical relationships between characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply one-hot encoding to the input\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "print(xenc)\n",
    "print(xenc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also visualize the one-hot encoding like this:\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of the one-hot encoding\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. The First Neuron\n",
    "\n",
    "A neuron consists of a simple dot product $x\\cdot W + b$. We don't use a **bias** $b$ here, so let's first initialize the **weights** $W$ randomly sampled from a normal distribution, i.e., most weights will be around 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 1))\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 9) Apply the matrix multiplication (denoted by `@` in PyTorch) to get the **logits**. Store the result in a variable called `logits`. Which shape will the result have and why? Check your predicted shape by printing the result! **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "# Perform matrix multiplication to get logits\n",
    "logits = xenc @ W\n",
    "\n",
    "# Print the shape of logits to verify\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE\n",
    "\n",
    "When we multiply xenc (of shape (num_bigrams, 27)) by W (of shape (27, 1)), the resulting shape will be (num_bigrams, 1). Each row in logits will represent the output (or logit) for one bigram in the dataset.\n",
    "\n",
    "Since our num_bigrams size is 5, the result is of shape (5,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating 27 Neurons\n",
    "\n",
    "Now we want 27 neurons instead of just one, because we eventually want to output the 27 probabilities for each character to be the next. So we want the weight matrix to be 27x27 instead of 27x1, and we will in parallel evaluate the 5 inputs on 27 neurons, so the output will be 5x27 (matrix multiplication of a 5x27 input with 27x27 weight matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. Each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers tell us the firing rate of each of the 27 neurons on the 5 inputs. For example, the result at row 3 and column 13 is giving us the firing rate of the 13th neuron looking at the third input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc @ W)[3,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is equivalent to a dot product between the third input and the 13th column of $W$, so using matrix multiplication we can very efficiently evaluate the dot product between lots of input examples in a batch and lots of neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc[3] * W[:,13]).sum() # element-wise multiplication and sum is equivalent to matrix multiplication, but less efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the steps so far, including one-hot-encoding, weights and logit calculation for input 'e':\n",
    "\n",
    "<img src=\"img/neuron1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the image we only plot a single input 'e', whereas in the code we process 5 inputs in parallel.\n",
    "\n",
    "We see that the logits are positive and negative. They can be interpreted as **log counts** (=logarithms of the counts of each bigram). We can elementwise exponentiate to transform them into numbers in $(0, \\infty)$, where negative numbers go to $(0,1)$ and positive numbers go to $(1,\\infty)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input values between -2 and 2\n",
    "x = torch.linspace(-2, 2, 100)  # \n",
    "\n",
    "# Compute the exponential of the input values\n",
    "y = torch.exp(x)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(x, y, label='log(x)')\n",
    "plt.title('Exponential of Positive and Negative Values')\n",
    "plt.xlabel('Input Value')\n",
    "plt.ylabel('Exponential')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 10a) Calculate the counts from the log counts by applying exponential function. Store the result in `counts` and print it. Normalize the counts like above, store the result in `probs` and print it. **(2 points)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "# Calculate counts from log counts\n",
    "counts = torch.exp(logits)  # Apply exponential function to each element in logits\n",
    "print(\"Counts:\\n\", counts)\n",
    "\n",
    "# Normalize counts to get probabilities\n",
    "probs = counts / counts.sum(dim=1, keepdims=True)  # Sum across rows and divide\n",
    "print(\"Probabilities:\\n\", probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the visualized steps including exponentiation and normalization:\n",
    "\n",
    "<img src=\"img/neuron2.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 10b) Do the last two maths operations in the cell above look familiar? How is this transformation called? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE \n",
    "\n",
    "Yes, the last tow math operations combined are known as the softmax transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the resulting \"probabilities\" for the first character (of course they are meaningless because the network is still untrained, the weights are randomly initialized):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary (so far)\n",
    "\n",
    "Let's summarize the code so far to get an overview of the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs # input to the network ('.emma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys # target for the network ('emma.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape # output probabilities of shape 5x27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the negative log likelihood loss for the first 5 bigrams\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram:\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indices {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualization of the first feedforward pass of input 'e' through the neuron, including negative log likelihood loss calculation. You can see in the output above that the loss is roughly 4 (see 'bigram example 2' output).\n",
    "\n",
    "<img src=\"img/neuron3.jpg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the output above that the average loss is 3.7, which is quite high. We can change the random seed for sampling $W$ for randomly changing the loss. But of course, we can do better: The loss calculation is only made up of differentiable operations (multiplication, addition, exponential, division...) We can minimize the loss by computing the gradients of the loss with respect to $W$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the loss, we basically need to pluck out the predicted \n",
    "probabilities at the indices of the target characters. For example, for the first bigram,\n",
    "the input character is '.' (index 0) and the target character is 'e' (index 5), and the predicted probability is stored in `probs[0,5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the following probabilities for calculating the loss:\n",
    "probs[0,5], probs[1,13], probs[2,13], probs[3,1], probs[4,0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this in the loss calculation at the end of the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # we need to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean() # average negative log likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.item()) # print the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the loss is the same as above. Finally we can do the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None # set to zero the gradient (None is a special value that PyTorch recognizes as \"set to zero\")\n",
    "loss.backward() # compute the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something magical happened when `loss.backward()` was run: PyTorch keeps track of all the operations in the forward pass, under the hood it builds a full computational graph, so it knows all the dependencies and all the mathematical operations inside the neural network. When we calculate the loss and call a `.backward()` on it, PyTorch fills in the partial derivatives of all the intermediate steps. So now we can look at the gradient and see that it is not zero anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape, W.grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `W` and `W.grad` are of shape 27x27, and each entry of `W.grad` is telling us the influence of that weight on the loss function. For example, since `W.grad[0,0]` is positive, slightly increasing `W[0,0]` will lead to a slightly bigger loss. Therefore we use the *negative* gradient for updating the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad # update the weights with learning rate 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the loss has really decreased using this one **gradient descent** step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Optimizing over the Whole Dataset\n",
    "\n",
    "Now let's apply gradient descent to the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement() # number of elements in the tensor xs (5 in '.emma.', here more)\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "for k in range(500):\n",
    "  \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts - one-hot input will pluck the correct row from W, like in the previous bigram model\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad # even a large learning rate is fine here, because the bigram model is very simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss is converging roughly to 2.48, which is similar to the loss we achieved all the way up in the original bigram model without a neural network (2.45). Back then, we achieved this loss by counting, now by optimizing weights. That makes sense because fundamentally we're not using any additional information, we're still just taking in the previous character and trying to predict the next one, but instead of doing it explicitly by counting and normalizing we are doing it with gradient-based learning. The explicit approach happens to very well optimize the loss function without any need for a gradient based optimization because the setup for bigram language models is so straightforward, we can just afford to estimate those probabilities directly and maintain them in a table. But the gradient-based approach is significantly more flexible, so we've actually gained a lot because we can expand this approach and complexify the neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (optional):** Remember the model smoothing used in the first bigram model by adding a number to the counts to avoid zero probabilities. Can you think of a smoothing equivalent in gradient descent based bigram model?\n",
    "\n",
    "**HINT:** Think of the weight matrix $W$ and its influence on the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** YOUR ANSWER GOES HERE \n",
    "\n",
    "The code above already includes a smoothing:\n",
    "\n",
    "`loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()`\n",
    "\n",
    "While the last part `+ 0.01*(W**2).mean()` is a regularization term which is proportional to `W**2` which penalizes large values in `W` and encourages a smoother and less extreme distribution of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the weights and the probabilities have changed after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the negative log likelihood loss for the first 5 bigrams\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram:\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indices {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 13) Here is the final visualization after training. Can you interpret the weights, have they improved? Can you compare the trained weights to the weights from our first approach using simple counting instead of a neuron? **(2 points)** \n",
    "\n",
    "\n",
    "<img src=\"img/neuron4.jpg\" width=\"900\">\n",
    "\n",
    "\n",
    "**ANSWER:** YOUR ANSWER GOES HERE \n",
    "\n",
    "The weights from the first approach using simple counting can be thought of as counts of occurrences normalized by the total occurrences of the first character in each bigram. However, the weights of the trained neuron are an improvement in the following way:\n",
    "\n",
    "The weights do not just reflect direct counts but learned patterns that capture dependencies between characters more effectively than raw counts. Means, the weight is higher if a dependency occurred often. Also the trained neuron reflects a smoothing effect: even if a certain bigram wasn't seen during the training, the model still assign a nonzero probability to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Sampling from the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "  \n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        \n",
    "        # ----------\n",
    "        # BEFORE:\n",
    "        #p = P[ix] # bigram probabilities by counting\n",
    "        # ----------\n",
    "        # NOW:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, comparable to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        # ----------\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get nearly identical samples as in the first model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Conclusion\n",
    "\n",
    "To summarize, we introduced the bigram character level language model, we saw how we can train the model, how we can sample from the model and how we can evaluate the quality of the model using the negative log likelihood loss. We trained the model in two completely different ways that actually get the same result and the same model: First, we just counted the frequency of all the bigrams and normalized. Second, we used the negative log likelihood loss as a guide to optimizing the counts matrix so that the loss was minimized in a gradient-based framework. Both approaches gave the same result, but the gradient-based framework is much more flexible and we can extend it to more complex settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
